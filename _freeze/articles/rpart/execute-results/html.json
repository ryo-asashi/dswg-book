{
  "hash": "93988c4613cf55dce1210d89591a7672",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"rpart\"\nauthor: \"データサイエンス関連基礎調査WG\"\ndate: \"2025-08-01\"\noutput:   \n  word_document:\n    toc: yes\n    toc_depth: '3'\n    fig_width: 6\n    fig_height: 3\n---\n\n\n\n\n\n\n## パッケージの概要\n\nrpartは再帰的分割による回帰木・分類木の実装を与えます。また、rpart.plotで決定木の可視化が可能です。 \n\n\n## 使用例：irisデータの分類\n\nirisデータを用いて、がく弁・花弁の長さ・幅の情報からアヤメの種類を特定する分類モデルを作成します。\n\n### irisデータセットを読み込む\n\nirisデータを読み込み、データの先頭を表示します。\n\n-   Sepal.Length：がく弁の長さ\n-   Sepal.Width：がく弁の幅\n-   Petal.Length：花弁の長さ\n-   Petal.Width：花弁の幅\n\nアヤメの種類はsetosa(1)、versicolor(2)、virginica(3)の3種類です。\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(iris)\nhead(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n\n\n:::\n:::\n\n\n\n\n### irisデータの構造\n\nirisデータの各種構造を確認します。\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n:::\n\n\n\n\nまた、データを散布図にプロットして確認します。\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(iris, col=c(2, 3, 4)[iris$Species])\n```\n\n::: {.cell-output-display}\n![](rpart_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n### モデル構築1（全体データ）\n\nまずは全てのデータを使って分類木モデルを構築してみます。\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart)\nlibrary(rpart.plot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'rpart.plot' was built under R version 4.5.1\n```\n\n\n:::\n\n```{.r .cell-code}\n# シードを設定\nset.seed(123) \n(iris.rp <- rpart(Species ~ ., data = iris))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nn= 150 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)  \n  2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *\n  3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)  \n    6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *\n    7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *\n```\n\n\n:::\n:::\n\n\n\n\n#### 可視化\n\nモデルを可視化します。plotで木の構造（分岐）を表示し、textで各ノードの分岐の基準や分類ラベルを表示します。デフォルトの設定だと図が見切れてしまうことがあります。\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(iris.rp)\ntext(iris.rp)\n```\n\n::: {.cell-output-display}\n![](rpart_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nrpart.plotを用いると、分類木をより分かりやすく表示させることができます。\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrpart.plot(iris.rp)\n```\n\n::: {.cell-output-display}\n![](rpart_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### モデル構築2（訓練データとテストデータに分割）\n\nirisデータをモデル生成のための訓練データと、モデル評価のためのテストデータに分割します。データ割合は訓練データを7割、テストデータを3割とします。確認のため、データサイズを出力します。\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# シードを設定\nset.seed(123) \n\n# データの分割\nsample_indices <- sample(1:nrow(iris), 0.7 * nrow(iris))  \ndf.train <- iris[sample_indices, ]\ndf.test <- iris[-sample_indices, ]\n\n# データサイズの確認\nc(nrow(iris), nrow(df.train), nrow(df.test))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 150 105  45\n```\n\n\n:::\n:::\n\n\n\n\n### モデル生成\n\n訓練データを用いて分類木モデルを生成します。\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# シードを設定\nset.seed(123)\n(model.rp <- rpart(Species ~ ., data = df.train))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nn= 105 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 105 68 virginica (0.34285714 0.30476190 0.35238095)  \n  2) Petal.Length< 2.45 36  0 setosa (1.00000000 0.00000000 0.00000000) *\n  3) Petal.Length>=2.45 69 32 virginica (0.00000000 0.46376812 0.53623188)  \n    6) Petal.Width< 1.75 35  4 versicolor (0.00000000 0.88571429 0.11428571) *\n    7) Petal.Width>=1.75 34  1 virginica (0.00000000 0.02941176 0.97058824) *\n```\n\n\n:::\n:::\n\n\n\n\n#### モデル評価\n\nテストデータを使ってモデル評価を行います。まずはテストデータを元に生成したモデルを用いて予測結果を算出します。\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction <- predict(model.rp, df.test, type = \"class\")\n```\n:::\n\n\n\n\n予測結果とテストデータのもともとのアヤメの分類とを比較します。おおむね正しく分類できていることが分かります。\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(result <- table(prediction, df.test$Species))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            \nprediction   setosa versicolor virginica\n  setosa         14          0         0\n  versicolor      0         18         1\n  virginica       0          0        12\n```\n\n\n:::\n\n```{.r .cell-code}\n(accuracy_prediction <- sum(diag(result)) / sum(result))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9777778\n```\n\n\n:::\n:::\n\n\n\n\n### ハイパーパラメーターのチューニング\n\nrpartの主なハイパーパラメーターは以下の通りです。\n\n- 木の複雑度に関するパラメータ(cp)\n- ノード分割の最小サンプル数(minsplit)\n- 木の最大の深さ(maxdepth)\n\nこれらのハイパーパラメータの最適な設定を探す作業がハイパーパラメーターのチューニングとなります。\n\nまずはcpの最適な設定を確認します。これはprintcpを用いることができます。\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprintcp(model.rp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nClassification tree:\nrpart(formula = Species ~ ., data = df.train)\n\nVariables actually used in tree construction:\n[1] Petal.Length Petal.Width \n\nRoot node error: 68/105 = 0.64762\n\nn= 105 \n\n       CP nsplit rel error  xerror     xstd\n1 0.52941      0  1.000000 1.10294 0.068075\n2 0.39706      1  0.470588 0.47059 0.069364\n3 0.01000      2  0.073529 0.11765 0.039979\n```\n\n\n:::\n:::\n\n\n\n\nxerror（交差検証誤差）が最も低くなるcpは0.01でした。これはデフォルトの設定と一致します。\n\n\n次に、minsplitのチューニングを行います。簡便的にテストデータでの設定の差を確認します。なお、デフォルトの設定は20です。\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# シードを設定\nset.seed(123)\n\n# 候補となる minsplit の値\nminsplit_values <- c(5, 20, 40)\n\n# minsplit ごとの精度を格納するデータフレーム\nresults <- data.frame(minsplit = minsplit_values, Accuracy = NA)\n\n# 各 minsplit のモデルを作成し、精度を測定\nfor (i in seq_along(minsplit_values)) {\n  control <- rpart.control(minsplit = minsplit_values[i])\n  model <- rpart(Species ~ ., data = df.train, method = \"class\", control = control)\n  \n  # 予測\n  predictions <- predict(model, df.test, type = \"class\")\n  accuracy <- mean(predictions == df.test$Species)\n  \n  # 結果を保存\n  results$Accuracy[i] <- accuracy\n}\n\n# 結果の確認\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  minsplit  Accuracy\n1        5 0.9777778\n2       20 0.9777778\n3       40 0.9777778\n```\n\n\n:::\n:::\n\n\n\nirisデータだと特段変化がないようです。\n\n\n最後にmaxdepthについても同様に試してみます。デフォルトの設定は5です。\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# シードを設定\nset.seed(123)\n\n# 候補となる maxdepth の値\nmaxdepth_values <- c(3, 5, 7)\n\n# maxdepth ごとの精度を格納するデータフレーム\nresults <- data.frame(maxdepth = maxdepth_values, Accuracy = NA)\n\n# 各 minsplit のモデルを作成し、精度を測定\nfor (i in seq_along(minsplit_values)) {\n  control <- rpart.control(maxdepth = maxdepth_values[i])\n  model <- rpart(Species ~ ., data = df.train, method = \"class\", control = control)\n  \n  # 予測\n  predictions <- predict(model, df.test, type = \"class\")\n  accuracy <- mean(predictions == df.test$Species)\n  \n  # 結果を保存\n  results$Accuracy[i] <- accuracy\n}\n\n# 結果の確認\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  maxdepth  Accuracy\n1        3 0.9777778\n2        5 0.9777778\n3        7 0.9777778\n```\n\n\n:::\n:::\n\n\n\nこちらもirisデータだとと特段変化がないようです。\n",
    "supporting": [
      "rpart_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
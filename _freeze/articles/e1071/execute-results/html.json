{
  "hash": "be233ae2fb963b8cab2f6b3c7bc993ff",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"e1071\"\nauthor: \"データサイエンス関連基礎調査WG\"\ndate: \"2025-08-01\"\noutput:\n  word_document:\n    toc: yes\n    toc_depth: '3'\n    fig_width: 6\n    fig_height: 3\n---\n\n\n\n\n\n\n## パッケージの概要\n\nRのパッケージe1071は、サポートベクトルマシン（Support Vector Machines）やカーネル法に関連する機械学習アルゴリズムやツールを提供するパッケージです。サポートベクトルマシンを実装するためのsvm()関数は、サポートベクトルマシンに特化したC++ライブラリであるlibsvmの機能を利用しています。\n\n本コード活用例では、iris（アイリスの花の種類ごとに測定された花弁などの長さ・幅のデータ）を用いて、サポートベクトルマシンを実装します。\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(e1071)\ndata(iris)\nattach(iris)\n```\n:::\n\n\n\n\n## サポートベクトルマシンの実装例\n\ne1071パッケージでサポートベクトルマシンを実装する際は、svm関数を利用します。\n\n以下のコードで、iris（アヤメの花ごとの種類、花弁などの長さ・幅のデータ）のSpecies（種類）を目的変数、Species以外を説明変数として、サポートベクトルマシンを作成し、print(model)で、モデルのParametersなどを出力します。\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- subset(iris, select = -Species)\ny <- Species\nmodel <- svm(x, y)\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nsvm.default(x = x, y = y)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  51\n```\n\n\n:::\n:::\n\n\n\n\nprint(model)の出力結果のうち、Parametersに記載されている「SVM-Type」は、サポートベクトルマシンの種類を示しており、「SVM-Kernel」はカーネル関数の種類を示しています。\n\nsvm関数で利用可能なサポートベクトルマシンの種類およびカーネル関数の種類は以下の通りです。詳細については、リンク先を参照してください。\n\nなお、e1071パッケージで利用可能なカーネル関数は下表の4種類のみです。kernlabパッケージでは、カーネル関数を自作するなど、e1071パッケージよりも柔軟にカーネル関数を設定可能です。\n\n<https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf>\n\n＜サポートベクトルマシンの種類＞\n\n| 区分 | 種類               | パラメータ    |\n|------|--------------------|---------------|\n| 分類 | C-classification   | cost          |\n| 分類 | nu-classification  | nu            |\n| 分類 | one-classification | nu            |\n| 回帰 | eps-regression     | cost, epsilon |\n| 回帰 | nu-regression      | cost, nu      |\n\n＜カーネル関数の種類＞\n\n| 種類         | パラメータ           |\n|--------------|----------------------|\n| linear       | なし                 |\n| polynomial   | gamma, degree, coef0 |\n| radial       | gamma                |\n| sigmoid      | coef0                |\n\n### テストデータと訓練データの分割\n\n本項では、irisをテストデータと訓練データに分割し、訓練データを用いて、先ほどと同じようにサポートベクトルマシンを作成します。\n\nprint(model)の出力結果の通り、サポートベクトルマシンの種類は\"C-classification\",カーネル関数は\"radial\"です。\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindex<-1:nrow(iris)\nN<-trunc(length(index)/3)\ntestindex<-sample(index, N)\ntestset<-iris[testindex,]\ntrainset<-iris[-testindex,]\nx <- subset(trainset, select = -Species)\ny <- trainset$Species\nmodel <- svm(x, y)\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nsvm.default(x = x, y = y)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  40\n```\n\n\n:::\n:::\n\n\n\n\n#### 結果の確認\nテストデータを対象に、モデルによって予測した結果と正解データを比較すると、ほとんどのデータで品種を正しく予測していることが確認できます。\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvm.pred <- predict(model, testset[, -5])\ntable(pred = svm.pred, true = testset[, 5] )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            true\npred         setosa versicolor virginica\n  setosa         20          0         0\n  versicolor      0         10         2\n  virginica       0          2        16\n```\n\n\n:::\n:::\n\n\n\n\n#### パラメータのチューニング\n最後に、パラメータのチューニングに利用するtune関数を紹介します。先ほど作成したmodelでチューニング可能なパラメータは\"cost\"(サポートベクトルマシン\"C-classification\"のパラメータ)と\"gamma\"（カーネル関数\"radial\"のパラメータ）の2つです。\n\n以下のコードでは、rangesで指定したパラメータの組み合わせを対象にモデルのチューニングを行います。\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvm.tune <- tune(svm, Species~., data = iris,\n            ranges = list(gamma = 2^(-6:2), cost = 2^(2:10)),\n            tunecontrol = tune.control(sampling = \"fix\")\n)\n```\n:::\n\n\n\n\nsummary関数を利用することで、最も良いパフォーマンスとなったパラメータの組み合わせや、パラメータの各組み合わせのパフォーマンスを確認することができます。\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(svm.tune)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nParameter tuning of 'svm':\n\n- sampling method: fixed training/validation set \n\n- best parameters:\n gamma cost\n     1    4\n\n- best performance: 0.02 \n\n- Detailed performance results:\n      gamma cost error dispersion\n1  0.015625    4  0.04         NA\n2  0.031250    4  0.06         NA\n3  0.062500    4  0.06         NA\n4  0.125000    4  0.04         NA\n5  0.250000    4  0.06         NA\n6  0.500000    4  0.04         NA\n7  1.000000    4  0.02         NA\n8  2.000000    4  0.06         NA\n9  4.000000    4  0.08         NA\n10 0.015625    8  0.06         NA\n11 0.031250    8  0.06         NA\n12 0.062500    8  0.06         NA\n13 0.125000    8  0.06         NA\n14 0.250000    8  0.06         NA\n15 0.500000    8  0.04         NA\n16 1.000000    8  0.02         NA\n17 2.000000    8  0.06         NA\n18 4.000000    8  0.08         NA\n19 0.015625   16  0.06         NA\n20 0.031250   16  0.04         NA\n21 0.062500   16  0.06         NA\n22 0.125000   16  0.04         NA\n23 0.250000   16  0.04         NA\n24 0.500000   16  0.06         NA\n25 1.000000   16  0.04         NA\n26 2.000000   16  0.06         NA\n27 4.000000   16  0.08         NA\n28 0.015625   32  0.06         NA\n29 0.031250   32  0.06         NA\n30 0.062500   32  0.04         NA\n31 0.125000   32  0.04         NA\n32 0.250000   32  0.06         NA\n33 0.500000   32  0.04         NA\n34 1.000000   32  0.04         NA\n35 2.000000   32  0.06         NA\n36 4.000000   32  0.08         NA\n37 0.015625   64  0.06         NA\n38 0.031250   64  0.04         NA\n39 0.062500   64  0.04         NA\n40 0.125000   64  0.04         NA\n41 0.250000   64  0.06         NA\n42 0.500000   64  0.04         NA\n43 1.000000   64  0.04         NA\n44 2.000000   64  0.06         NA\n45 4.000000   64  0.08         NA\n46 0.015625  128  0.04         NA\n47 0.031250  128  0.04         NA\n48 0.062500  128  0.04         NA\n49 0.125000  128  0.06         NA\n50 0.250000  128  0.06         NA\n51 0.500000  128  0.04         NA\n52 1.000000  128  0.04         NA\n53 2.000000  128  0.06         NA\n54 4.000000  128  0.08         NA\n55 0.015625  256  0.04         NA\n56 0.031250  256  0.04         NA\n57 0.062500  256  0.04         NA\n58 0.125000  256  0.06         NA\n59 0.250000  256  0.06         NA\n60 0.500000  256  0.04         NA\n61 1.000000  256  0.04         NA\n62 2.000000  256  0.06         NA\n63 4.000000  256  0.08         NA\n64 0.015625  512  0.02         NA\n65 0.031250  512  0.04         NA\n66 0.062500  512  0.04         NA\n67 0.125000  512  0.06         NA\n68 0.250000  512  0.06         NA\n69 0.500000  512  0.04         NA\n70 1.000000  512  0.04         NA\n71 2.000000  512  0.06         NA\n72 4.000000  512  0.08         NA\n73 0.015625 1024  0.04         NA\n74 0.031250 1024  0.04         NA\n75 0.062500 1024  0.04         NA\n76 0.125000 1024  0.06         NA\n77 0.250000 1024  0.06         NA\n78 0.500000 1024  0.04         NA\n79 1.000000 1024  0.04         NA\n80 2.000000 1024  0.06         NA\n81 4.000000 1024  0.08         NA\n```\n\n\n:::\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
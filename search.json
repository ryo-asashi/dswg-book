[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rパッケージ活用事例集",
    "section": "",
    "text": "はじめに\nこのウェブサイトは、アクチュアリーの業務や研究に役立つRパッケージの情報を日本アクチュアリー会の会員に向けて広く提供することを目的として、データサイエンス関連基礎調査部会の情報共有チームが作成したものです。\n\n\n\n\n\n\nGitHub リポジトリ dswg-book は情報共有チーム内部で出力結果を事前に参照するために用意されたものであり、会員に直接参照されることを目的としたものではありません。\n\n\n\n\n\nWarning: package 'DT' was built under R version 4.5.1",
    "crumbs": [
      "はじめに"
    ]
  },
  {
    "objectID": "articles/actuar.html",
    "href": "articles/actuar.html",
    "title": "actuar",
    "section": "",
    "text": "パッケージの概要\nactuar パッケージは、（主に損害保険分野の）保険数理に関する機能を実装したパッケージです。actuar パッケージの特徴の一つは様々な確率分布を扱うための関数を実装していることで、パレート分布、Burr分布、対数ガンマ分布、対数ロジスティック分布など、裾の厚い分布が数多く収録されています。\nstats パッケージと同様、actuar パッケージにおいても、それぞれの分布に対して、その名称に接頭辞 “d”、“p”、“r”、“q” を付けた関数が用意されています。接頭辞ごとに関数の機能が異なり、d****(x, ...) が密度関数、p****(x, ...) が累積分布関数、r****(n, ...) が乱数を生成する関数、q(p, ...) が分位点を返す関数です。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>actuar</span>"
    ]
  },
  {
    "objectID": "articles/actuar.html#確率密度関数をプロットする",
    "href": "articles/actuar.html#確率密度関数をプロットする",
    "title": "actuar",
    "section": "確率密度関数をプロットする",
    "text": "確率密度関数をプロットする\n\nパレート分布（Pareto Distribution）\n\n\\texttt{dpareto}(x,{\\alpha},{\\theta})={{\\alpha}{\\theta}^{\\alpha}}/{(x+\\theta)^{\\alpha+1}}\n\nパレート分布 pareto は「全体の数値の大部分は、全体を構成するうちの一部の要素が生み出している」というパレートの法則を表現する確率分布で、裾の重い分布の代表例として知られています。アクチュアリーサイエンスの分野では、損害保険における高額な保険金支払いのモデリングで頻繁に用いられます。ほとんどの請求は少額である一方、ごく一部の請求が非常に高額になるような事象をうまく表現できるため、巨大な損失額が発生するリスクの分析に役立ちます。\n\nlibrary(actuar)\n\nWarning: package 'actuar' was built under R version 4.5.1\n\nxlim &lt;- c(0, 3)\nylim &lt;- c(0, 2)\nmain &lt;- \"Pareto Distribution\"\n\ncurve(dpareto(x, 1, 1), type = \"l\", lty = 1, col = \"darkgray\",\n      xlim = xlim, ylim = ylim, ylab = \"\", main = main)\ncurve(dpareto(x, 2, 1), type = \"l\", lty = 2, col = \"darkcyan\", add = TRUE)\ncurve(dpareto(x, 1, 2), type = \"l\", lty = 3, col = \"darkred\", add = TRUE)\ncurve(dpareto(x, 2, 2), type = \"l\", lty = 4, col = \"darkblue\", add = TRUE)\n\n\n\n\n\n\n\n\n\n\nバー分布（Burr Distribution）\n\n\\texttt{dburr}(x,{\\alpha},{\\gamma},{\\theta})={{\\alpha}{\\gamma}(x/{\\theta})^{\\gamma}}/{x[1+(x/{\\theta)^{\\gamma}}]^{\\alpha + 1}}\n\nバー分布 burr は非常に柔軟な確率分布で、多くの有名な分布（ガンマ分布、対数正規分布、パレート分布など）を特別な場合として含んだり、近似したりすることができます。裾の重さを調整できるため、パレート分布と同様に極端な損失のモデリングに役立ちますが、柔軟性のために、より多様なデータに対して適合が良い場合があります。そのため、損害額のモデリングにおいて強力な選択肢の一つとなります。\n\nxlim &lt;- c(0, 3)\nylim &lt;- c(0, 2)\nmain &lt;- \"Burr Distribution\"\n\ncurve(dburr(x, 1, 1, 1), type = \"l\", lty = 1, col = \"darkgray\",\n      xlim = xlim, ylim = ylim, ylab = \"\", main = main)\ncurve(dburr(x, 2, 1, 1), type = \"l\", lty = 2, col = \"darkcyan\", add = TRUE)\ncurve(dburr(x, 1, 2, 2), type = \"l\", lty = 3, col = \"darkred\", add = TRUE)\ncurve(dburr(x, 2, 2, 2), type = \"l\", lty = 4, col = \"darkblue\", add = TRUE)\n\n\n\n\n\n\n\n\n\n\n対数ガンマ分布（Log-gamma Distribution）\n\n\\texttt{dlgamma}(x,{\\alpha},{\\lambda})={{\\lambda}^{\\alpha}{\\Gamma}({\\alpha})}/{[(\\log{x})^{\\alpha-1}x^{\\lambda+1}]}\n\nactuar における対数ガンマ分布 lgamma は、対数をとるとガンマ分布に従う確率変数が従う分布、すなわち、\\mathrm{X}がガンマ分布に従うときにe^{\\mathrm{X}}が従う分布のことを指します。ガンマ分布が持つ柔軟な形状の特性を対数スケールで持つため、極端に大きな値が発生するような事象、たとえば巨額の保険金支払いや大規模な自然災害による損害額などのモデリングに利用できることがあります。\n\nxlim &lt;- c(1, 4)\nylim &lt;- c(0, 2)\nmain &lt;- \"Log-gamma Distribution\"\n\ncurve(dlgamma(x, 1, 1), type = \"l\", lty = 1, col = \"darkgray\",\n      xlim = xlim, ylim = ylim, ylab = \"\", main = main)\ncurve(dlgamma(x, 2, 1), type = \"l\", lty = 2, col = \"darkcyan\", add = TRUE)\ncurve(dlgamma(x, 1, 2), type = \"l\", lty = 3, col = \"darkred\", add = TRUE)\ncurve(dlgamma(x, 2, 2), type = \"l\", lty = 4, col = \"darkblue\", add = TRUE)\n\n\n\n\n\n\n\n\n\n\n対数ロジスティック分布（Log-logistic Distribution）\n\n\\texttt{dllogis}(x,{\\gamma},{\\theta})={{\\gamma}(x/{\\theta})^{\\gamma}}/{x[1+(x/{\\theta)}]^2}\n\n対数ロジスティック分布 llogis は、対数をとるとロジスティック分布（累積分布関数がロジスティック関数である連続分布）に従う確率変数の分布です。経済学の分野ではフィスク分布（Fisk Distribution）とも呼ばれます。この分布は対数正規分布と似たような山形の形状を取りますが、それよりも裾が重いという特徴を持ちます。この性質から、損害保険における損失額のモデリング、特に甚大な損害が発生する可能性を無視できない場合に優れた選択肢となりえます。\n\nxlim &lt;- c(0, 3)\nylim &lt;- c(0, 2)\nmain &lt;- \"Log-logistic Distribution\"\n\ncurve(dllogis(x, 1, 1), type = \"l\", lty = 1, col = \"darkgray\",\n      xlim = xlim, ylim = ylim, ylab = \"\", main = main)\ncurve(dllogis(x, 2, 1), type = \"l\", lty = 2, col = \"darkcyan\", add = TRUE)\ncurve(dllogis(x, 1, 2), type = \"l\", lty = 3, col = \"darkred\", add = TRUE)\ncurve(dllogis(x, 2, 2), type = \"l\", lty = 4, col = \"darkblue\", add = TRUE)\n\n\n\n\n\n\n\n\n\n\nその他の分布\nactuar パッケージには、上記のほかにも以下のような確率分布が実装されています。\n\n\n\n逆ガンマ分布\ninvgamma\n\n\n逆パレート分布\ninvpareto\n\n\n逆バー分布\ninvburr\n\n\n逆ワイブル分布\ninvweibull\n\n\n逆指数分布\ninvexp\n\n\n一般化ベータ分布\ngenbeta\n\n\n一般化パレート分布\ngenpareto",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>actuar</span>"
    ]
  },
  {
    "objectID": "articles/actuar.html#参考資料",
    "href": "articles/actuar.html#参考資料",
    "title": "actuar",
    "section": "参考資料",
    "text": "参考資料\n[1] Dutang, C., Goulet, V. and Pigeon, M. (2008). actuar: An R Package for Actuarial Science",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>actuar</span>"
    ]
  },
  {
    "objectID": "articles/aglm.html",
    "href": "articles/aglm.html",
    "title": "aglm",
    "section": "",
    "text": "パッケージの概要\naglm パッケージは、藤田ほか (2019) によって提唱された Accurate GLM (AGLM) を実装したパッケージです。AGLM は、目的変数の値を特徴量ことの非線形な効果の加法的な組み合わせとして表現するモデルであり、高い解釈性を保ちながら柔軟な予測を可能にします。\nAGLM は、以下の２つの手法を組み合わせることで、目的変数と予測変数の非線形な関係を効果的に捉えます。\nlibrary(ggplot2)\nlibrary(MASS) # Bostonデータセットを利用します\n\nset.seed(42)\n# 学習用データと評価用データを分割します。\nsplit_data &lt;- rsample::initial_split(Boston)\n\n# 学習用データを説明変数（デザイン行列）と目的変数に分割します。\ntrain &lt;- rsample::training(split_data)\ntrain_X &lt;- dplyr::select(train, -medv)\ntrain_y &lt;- train$medv\n\n# 評価データを説明変数と目的変数に分割します。\ntest &lt;- rsample::testing(split_data)\ntest_X &lt;- dplyr::select(test, -medv)\ntest_y &lt;- test$medv",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>aglm</span>"
    ]
  },
  {
    "objectID": "articles/aglm.html#パッケージの概要",
    "href": "articles/aglm.html#パッケージの概要",
    "title": "aglm",
    "section": "",
    "text": "連続変数の離散化：連続変数である特徴量が取りうる値の範囲を適切な区間に分割し、その特徴量の効果を区分的関数としてモデル化することによって非線形な効果を表現します。\nフューズドラッソ（Fused Lasso）による正則化：AGLM では、連続変数を単純に区間に対応させてダミー変数化する代わりに、「順序ダミー（O-ダミー）変数化」と呼ばれる方法を利用します。これにより、モデルの学習時に推定される各パラメーターは、隣接する区間における効果水準の差に対応することになります。そのようにモデル化したうえでラッソ回帰を適用すると、隣接区間の効果の差が重要でないような区切りを消失させて、区間を自動的に再統合することができます。\n\n\n\n\n\n\n\nNote\n\n\n\nAGLM モデルの学習における最小化問題の正則化項は、おおむね次の形式で与えられます。\n\nR{ \\left(\\{\\beta_{j,k}\\}_{j,k}\\;\\middle|\\;\\lambda, \\alpha \\right)}\n=\\lambda\\left\\{(1-\\alpha)\\sum_j\\sum_k{\\left|\\beta_{j,k}\\right|}^2\n+\\alpha\\sum_j\\sum_k{\\left|\\beta_{j,k}\\right|}\\right\\}\n\nここで、罰則の対象となるパラメータ \\beta_{j,k} は、連続変数である特徴量 x_j が取りうる値の範囲を離散化した区間ごとの効果の水準ではなく、隣接する区間の水準の差に対応します。このため、\\beta_{j,k} が小さくなるほど特徴量 x_j の効果がなめらかになります。また、\\beta_{j,k} がゼロのときは隣り合う区間における効果の水準が等しくなります。これは、離散化した区間の再統合としてとらえることができます。\n\\lambda は正則化項の影響度を決めるパラメーターで、この値が大きいほど、各パラメーター \\beta_{j,k} の値が小さく抑えられるようになります。また、\\alpha は正則化項の第1項と第2項のバランスを決めるパラメーターで、\\alpha=1 の場合をラッソ正則化、\\alpha=0 の場合をリッジ正則化、0&lt;\\alpha&lt;1 の場合をエラスティックネットと呼びます。なお、\\lambda や \\alpha の値を決めるときは、意味のある \\lambda の値の範囲が \\alpha の値によって大きく異なることに注意が必要です。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>aglm</span>"
    ]
  },
  {
    "objectID": "articles/aglm.html#aglm-モデルを構築する",
    "href": "articles/aglm.html#aglm-モデルを構築する",
    "title": "aglm",
    "section": "AGLM モデルを構築する",
    "text": "AGLM モデルを構築する\n\nラッソ正則化\naglm() 関数の第1引数 x に説明変数（デザイン行列）を渡し、第2引数 y に目的変数を渡すことで、AGLM モデルを学習させることができます。\nまた、predict() 関数に学習済みの AGLM モデルと新たなデータを渡せば、そのデータに対する予測値を出力することができます。なお、aglm は glmnet パッケージを活用して実装されており、出力される “AccurateGLM” オブジェクトには多数の\\lambda の値に対応する結果が保存されています。“AccurateGLM” オブジェクトに対する predict() 関数のメソッドの引数 s の値を指定することで、正則化の強さが異なるさまざまなモデルによる予測値を得ることが可能です。\n\nlibrary(aglm)\nlibrary(patchwork)\n\nmodel_lasso &lt;- aglm(\n  x = train_X,\n  y = train_y,\n  alpha = 1, # 正則化項のαパラメーターを指定（1：ラッソ、0：リッジ）\n  add_linear_columns = FALSE\n)\n\npreds &lt;- predict(\n  object = model_lasso,\n  newx = test_X,\n  s = 0.062 # 正則化項のλパラメーターを数値またはベクトルで指定\n) |&gt; c() # 行列形式の出力をベクトルに変換\n\nset_title &lt;- function(label, rmse)\n  ggtitle(sprintf(\"%s | RMSE: %.3f\", label, rmse))\n\np &lt;- ggplot(mapping = aes(x = test_y, y = preds)) +\n  geom_point() +\n  set_title(\"Lasso s0.062\", yardstick::rmse_vec(test_y, preds))\n\npreds &lt;- predict(model_lasso, test_X, s = 0.62) |&gt; c()\nq &lt;- ggplot(mapping = aes(x = test_y, y = preds)) +\n  geom_point() +\n  set_title(\"Lasso s0.62\", yardstick::rmse_vec(test_y, preds))\n\np + q\n\n\n\n\n\n\n\n\n構築した AGLM モデルを plot() 関数に渡すことで、特徴量ごとの予測値への影響を可視化することができます。正則化項としてラッソを用いる場合、\\lambda を大きくするほど、離散化した区間ごとに当てはめられた効果水準の差が消失して意味のある区間の数が減少していき、（add_linear_columns = FALSE としている場合）最後には特徴量の効果は消失します。\n\npar(mar = c(4, 4, 1, 1)) # プロット領域中の余白を調整\nvars = c(\"lstat\", \"rm\", \"dis\", \"ptratio\")\nplot(model_lasso, verbose = FALSE, vars = vars, s = 0.062)\n\n\n\n\n\n\n\nplot(model_lasso, verbose = FALSE, vars = vars, s = 0.16)\n\n\n\n\n\n\n\nplot(model_lasso, verbose = FALSE, vars = vars, s = 0.62)\n\n\n\n\n\n\n\nplot(model_lasso, verbose = FALSE, vars = vars, s = 6.3)\n\n\n\n\n\n\n\n\n\n\nリッジ正則化\naglm() 関数の引数 alpha を 0 にすると、パラメータの推定がリッジ正則化によって行われます。ここでは、\\lambda の値を明示的に与えてみます。\n\nmodel_ridge &lt;- aglm(\n  x = train_X,\n  y = train_y,\n  alpha = 0, # 正則化項のαパラメーターを指定（1：ラッソ、0：リッジ）\n  add_linear_columns = FALSE,\n  lambda = c(100, 1) # 正則化項のλを指定\n)\n\npreds &lt;- predict(model_ridge, test_X, s = 1) |&gt; c()\np &lt;- ggplot(mapping = aes(x = test_y, y = preds)) +\n  geom_point() +\n  set_title(\"Ridge s1\", yardstick::rmse_vec(test_y, preds))\n\npreds &lt;- predict(model_ridge, test_X, s = 100) |&gt; c()\nq &lt;- ggplot(mapping = aes(x = test_y, y = preds)) +\n  geom_point() +\n  set_title(\"Ridge s100\", yardstick::rmse_vec(test_y, preds))\n\np + q\n\n\n\n\n\n\n\n\nRidge正則化を用いている場合、\\lambda の値を大きくしたときに、離散化した区間ごとに当てはめられたパラメータの差は小さくなるもののゼロにはならず、特徴量ごとの影響をあらわす関数がなめらかになっていきます。\n\npar(mar = c(4, 4, 1, 1)) # プロット領域中の余白を調整\nplot(model_ridge, verbose = FALSE, vars = vars, s = 0.1)\n\n\n\n\n\n\n\nplot(model_ridge, verbose = FALSE, vars = vars, s = 1)\nplot(model_ridge, verbose = FALSE, vars = vars, s = 10)\n\n\n\n\n\n\n\nplot(model_ridge, verbose = FALSE, vars = vars, s = 100)\n\n\n\n\n\n\n\n\n\n\nElastic-Net\naglm() 関数の引数 alpha に 0&lt;\\alpha&lt;1 の値を渡すことで、エラスティックネットによる正則化を利用することができます。エラスティックネットでは、ラッソとリッジの中間的な性質を持ったモデルが出力されます。\n\nmodel_EN &lt;- aglm(\n  x = train_X,\n  y = train_y,\n  alpha = 0.5, # 正則化項のαパラメーターを指定（エラスティックネット）\n  add_linear_columns = FALSE,\n  lambda = c(10, 1, 0.1, 0.01) # 正則化項のλを指定\n)\n\npreds &lt;- predict(model_EN, test_X, s = 0.1) |&gt; c()\np &lt;- ggplot(mapping = aes(x = test_y, y = preds)) +\n  geom_point() +\n  set_title(\"ElasticNet α0.5 s0.1\", yardstick::rmse_vec(test_y, preds))\n\npreds &lt;- predict(model_EN, test_X, s = 1) |&gt; c()\nq &lt;- ggplot(mapping = aes(x = test_y, y = preds)) +\n  geom_point() +\n  set_title(\"ElasticNet α0.5 s1\", yardstick::rmse_vec(test_y, preds))\n\np + q\n\n\n\n\n\n\n\n\n\npar(mar = c(4, 4, 1, 1)) # プロット領域中の余白を調整\nplot(model_EN, verbose = FALSE, vars = vars, s = 0.01)\n\n\n\n\n\n\n\nplot(model_EN, verbose = FALSE, vars = vars, s = 0.1)\n\n\n\n\n\n\n\nplot(model_EN, verbose = FALSE, vars = vars, s = 1)\n\n\n\n\n\n\n\nplot(model_EN, verbose = FALSE, vars = vars, s = 10)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>aglm</span>"
    ]
  },
  {
    "objectID": "articles/ALEPlot.html",
    "href": "articles/ALEPlot.html",
    "title": "ALEPlot",
    "section": "",
    "text": "パッケージの概要\nALEPlot パッケージは、予測モデルとデータをもとに、ALE (Accumulated Local Effects、累積局所効果) を可視化する機能を実装したパッケージです。\nなお、ALE は、関心のある説明変数 X について、それが取りうる値の分割 [x_{[0]}, x_{[1]}), [x_{[1]}, x_{[2]}), ... を考え、各区間について、そこに含まれるデータの X の値を区間の両端の値に置き換えたときの予測値の増分の平均値をその区間の局所効果とみなし、区間ごとの局所効果を累積させることで、変数 X の影響を X に関する1変数関数として表現する手法です。\n手法の詳細については、たとえば、「解釈可能な機械学習」に関するウェブ版書籍（Molnar著、株式会社HACARUS訳）の解説 https://hacarus.github.io/interpretable-ml-book-ja/ale.html などをご参照ください。\ndata(Boston, package = \"MASS\")\nlibrary(parsnip) # 予測モデルの構築\nlibrary(dplyr) # データフレームの操作\n\n# ランダムシードを固定する\nset.seed(42)\n\n# 予測モデルを構築する\nmodel &lt;- boost_tree() %&gt;%\n  set_mode('regression') %&gt;%\n  fit(medv~., data = Boston)\n\nsummary(model)\n\n             Length Class       Mode   \nlvl          0      -none-      NULL   \nordered      1      -none-      logical\nspec         7      boost_tree  list   \nfit          9      xgb.Booster list   \npreproc      4      -none-      list   \nelapsed      2      -none-      list   \ncensor_probs 0      -none-      list",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ALEPlot</span>"
    ]
  },
  {
    "objectID": "articles/ALEPlot.html#aleプロットを作成するaleplot",
    "href": "articles/ALEPlot.html#aleプロットを作成するaleplot",
    "title": "ALEPlot",
    "section": "ALEプロットを作成する（ALEPlot）",
    "text": "ALEプロットを作成する（ALEPlot）\nALEPlot() 関数を使えば、ALE プロットを描画することができます。\nここで、pred.fun に渡す予測用の関数は、X.model と newdata を入力すると予測値を数値ベクトルとして出力するようなものであることが必要です。lm を含む多くのモデルでは、モデルに対応する predict 関数のメソッドが数値ベクトルを出力するため、pred.fun = predict としておけば動作します。predict の返り値がベクトルでないようなクラスのモデルでは、以下のように適切に定義することが必要です。\n\nlibrary(ALEPlot)\n\n# ALEPlot関数の `X` に渡す説明変数Xを用意する\nX &lt;- select(Boston, -medv)\n\n# ALEPlot関数の `pred.fun` に渡す予測のための関数を定義する\npred_parsnip = function(X.model, newdata){\n  predict(X.model, new_data = newdata)$.pred\n  # 予測値をベクトルとして抽出\n}\n\n# ALEPlotを描画する\nale &lt;- ALEPlot(X, model, pred_parsnip, J = \"rm\", K = 50)\n\n\n\n\n\n\n\n\nDALEXパッケージがインストール済みの場合は、DALEX::yhat() 関数を利用することもできます。この関数は、ALEPlot() 関数が要求するのと同じ引数名で定義されており、しかも、かなり広範な種類の予測モデルに対応しています。\n\n# 2次元ALEPlotを描画する\nale.2d &lt;- ALEPlot(X, model, DALEX::yhat, c(\"rm\", \"dis\"))\n\n\n\n\n\n\n\n\n標準のプロットは R のグラフィックス関数で作成されますが、返り値をデータフレーム化することで、ggplot2 パッケージで ALE を可視化することもできます。\n\nlibrary(ggplot2)\n\nggplot(data = as.data.frame(ale[2:3]),\n       aes(x = x.values, y = f.values)) +\n  geom_line() + geom_point()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ALEPlot</span>"
    ]
  },
  {
    "objectID": "articles/ALEPlot.html#pdプロットを作成するpdplot",
    "href": "articles/ALEPlot.html#pdプロットを作成するpdplot",
    "title": "ALEPlot",
    "section": "PDプロットを作成する（PDPlot）",
    "text": "PDプロットを作成する（PDPlot）\nPD プロットを作成するための関数 PDPlot() を用いれば、PD を描画することもできます。基本的な使い方は ALEPlot() 関数と同じです。\nなお、PD は、ALE と同様に、予測モデルにおける特定の特徴量の効果を解釈するための手法です。詳細については、前掲のウェブ書籍の解説 https://hacarus.github.io/interpretable-ml-book-ja/pdp.html などをご参照ください。\n\n# PDPlotを描画する\npdp &lt;- PDPlot(X, model, pred_parsnip, \"rm\", 20)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ALEPlot</span>"
    ]
  },
  {
    "objectID": "articles/C50.html",
    "href": "articles/C50.html",
    "title": "C50",
    "section": "",
    "text": "パッケージの概要\nC50パッケージではC5.0アルゴリズムを用いた決定木モデルを使用できます。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>C50</span>"
    ]
  },
  {
    "objectID": "articles/C50.html#使用例pimaindiansdiabetesデータの分類",
    "href": "articles/C50.html#使用例pimaindiansdiabetesデータの分類",
    "title": "C50",
    "section": "使用例：PimaIndiansDiabetesデータの分類",
    "text": "使用例：PimaIndiansDiabetesデータの分類\nPimaIndiansDiabetesデータを用いて、種々の項目からその患者が糖尿病患者であるかを判定する決定木モデルをC50パッケージを用いて構築します。\n\nlibrary(mlbench)\ndata(PimaIndiansDiabetes)\ndf &lt;- PimaIndiansDiabetes\ntar.idx &lt;- 9\n\nデータの中身の確認です。（head）\n\nhead(df)\n\n  pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1        6     148       72      35       0 33.6    0.627  50      pos\n2        1      85       66      29       0 26.6    0.351  31      neg\n3        8     183       64       0       0 23.3    0.672  32      pos\n4        1      89       66      23      94 28.1    0.167  21      neg\n5        0     137       40      35     168 43.1    2.288  33      pos\n6        5     116       74       0       0 25.6    0.201  30      neg\n\n\nデータの中身の確認です。（str）\n\nstr(df)\n\n'data.frame':   768 obs. of  9 variables:\n $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...\n $ glucose : num  148 85 183 89 137 116 78 115 197 125 ...\n $ pressure: num  72 66 64 66 40 74 50 0 70 96 ...\n $ triceps : num  35 29 0 23 35 0 32 0 45 0 ...\n $ insulin : num  0 0 0 94 168 0 88 0 543 0 ...\n $ mass    : num  33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ...\n $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...\n $ age     : num  50 31 32 21 33 30 26 29 53 54 ...\n $ diabetes: Factor w/ 2 levels \"neg\",\"pos\": 2 1 2 1 2 1 2 1 2 2 ...\n\n\n各変数の意味は以下の通りです。\n\n\n\n変数名\nデータ型\n概要\n\n\n\n\npregnant 　　\nnum\n妊娠回数 　　　\n\n\nglucose\nnum\n血漿グルコース濃度（負荷試験）\n\n\npressure 　　　\nnum\n拡張期血圧 [mm Hg] 　 　\n\n\ntriceps 　　　　\nnum\n上腕三頭筋皮膚襞の厚さ [mm]\n\n\ninsulin 　　　　\nnum\n2時間血清インスリン [mu U/ml]\n\n\nmass\nnum\nBMI　　　\n\n\npedigree\nnum\n糖尿病血統機能\n\n\nage\nnum\n年齢\n\n\ndiabetes\nFactor\nクラス変数（糖尿病の検査）\n\n\n\nsummaryを表示します。\n\nsummary(df)\n\n    pregnant         glucose         pressure         triceps     \n Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  \n 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  \n Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  \n Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  \n 3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  \n Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  \n    insulin           mass          pedigree           age        diabetes \n Min.   :  0.0   Min.   : 0.00   Min.   :0.0780   Min.   :21.00   neg:500  \n 1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437   1st Qu.:24.00   pos:268  \n Median : 30.5   Median :32.00   Median :0.3725   Median :29.00            \n Mean   : 79.8   Mean   :31.99   Mean   :0.4719   Mean   :33.24            \n 3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262   3rd Qu.:41.00            \n Max.   :846.0   Max.   :67.10   Max.   :2.4200   Max.   :81.00            \n\n\npsychパッケージのpairs.panelsを用いて、散布図の一覧を表示します。\n\nlibrary(psych)\n\n\ndf.x &lt;- df[-tar.idx]\ndf.y &lt;- df[tar.idx]\ndf.y &lt;- ifelse(df.y == \"pos\", 1, 2)\npairs.panels(df.x, bg=c(\"skyblue\", \"salmon\")[df.y], pch=21)\n\n\n\n\n\n\n\n\nggplot2パッケージを用いて、ヒストグラムを表示します。変数ごとにpos/negの分布の傾向を把握することができます。\n\nlibrary(ggplot2)\n\n\nAttaching package: 'ggplot2'\n\n\nThe following objects are masked from 'package:psych':\n\n    %+%, alpha\n\nlibrary(tidyr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n# 数値変数だけを抽出（diabetes以外）\nnum_vars &lt;- names(df)[sapply(df, is.numeric)]\n\n# 長い形式に変換\ndf_long &lt;- df %&gt;%\n  pivot_longer(cols = all_of(num_vars), names_to = \"variable\", values_to = \"value\")\n\n# ヒストグラムを1つの図にまとめて作成\nggplot(df_long, aes(x = value, fill = diabetes)) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 30) +\n  facet_wrap(~variable, scales = \"free\") +\n  labs(title = \"Histograms of Variables by Diabetes\",\n       x = \"Value\", y = \"Count\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>C50</span>"
    ]
  },
  {
    "objectID": "articles/C50.html#モデル構築",
    "href": "articles/C50.html#モデル構築",
    "title": "C50",
    "section": "モデル構築",
    "text": "モデル構築\nモデル生成のための訓練データと、モデル評価のためのテストデータに分割します。データ割合は訓練データを7割、テストデータを3割とします。\n\n# 再現性のためにシードを設定\nset.seed(1234)\n\n# データの分割\nn &lt;- nrow(df)\ntrain.rate &lt;- 0.7\n\n# データの抽出\ntrain.set &lt;- sample(n, n * train.rate)\ntrain.df &lt;- df[train.set, ]\n\ntest.set &lt;- setdiff(1:n, train.set)\ntest.df &lt;- df[test.set, ]\n\nモデルを生成します。\n\nlibrary(C50)\n\nWarning: package 'C50' was built under R version 4.5.1\n\n\n\nmodel &lt;- C5.0(\n  x = train.df[-tar.idx], \n  y = train.df[[tar.idx]]\n)\n\nsummaryを用いると、モデルのサマリが表示されます。\n\nsummary(model)\n\n\nCall:\nC5.0.default(x = train.df[-tar.idx], y = train.df[[tar.idx]])\n\n\nC5.0 [Release 2.07 GPL Edition]     Fri Aug  1 17:34:30 2025\n-------------------------------\n\nClass specified by attribute `outcome'\n\nRead 537 cases (9 attributes) from undefined.data\n\nDecision tree:\n\nglucose &lt;= 127:\n:...age &lt;= 28:\n:   :...mass &lt;= 32.2: neg (113/1)\n:   :   mass &gt; 32.2:\n:   :   :...triceps &gt; 8: neg (64/8)\n:   :       triceps &lt;= 8:\n:   :       :...glucose &lt;= 110: neg (7)\n:   :           glucose &gt; 110: pos (4)\n:   age &gt; 28:\n:   :...mass &lt;= 26.3: neg (33/1)\n:       mass &gt; 26.3:\n:       :...pedigree &lt;= 0.845: neg (100/30)\n:           pedigree &gt; 0.845:\n:           :...pregnant &lt;= 1: neg (5/1)\n:               pregnant &gt; 1: pos (10)\nglucose &gt; 127:\n:...glucose &lt;= 165:\n    :...mass &lt;= 29.9: neg (55/15)\n    :   mass &gt; 29.9:\n    :   :...insulin &lt;= 196: pos (70/20)\n    :       insulin &gt; 196:\n    :       :...age &lt;= 32: neg (13/1)\n    :           age &gt; 32: pos (8/1)\n    glucose &gt; 165:\n    :...pedigree &gt; 1.114: neg (5/2)\n        pedigree &lt;= 1.114:\n        :...age &gt; 27: pos (41/1)\n            age &lt;= 27:\n            :...triceps &lt;= 26: neg (3)\n                triceps &gt; 26: pos (6)\n\n\nEvaluation on training data (537 cases):\n\n        Decision Tree   \n      ----------------  \n      Size      Errors  \n\n        16   81(15.1%)   &lt;&lt;\n\n\n       (a)   (b)    &lt;-classified as\n      ----  ----\n       339    22    (a): class neg\n        59   117    (b): class pos\n\n\n    Attribute usage:\n\n    100.00% glucose\n     89.76% mass\n     75.79% age\n     31.66% pedigree\n     16.95% insulin\n     15.64% triceps\n      2.79% pregnant\n\n\nTime: 0.0 secs\n\n\nplotを用いると、決定木の様子が可視化されます。(図がつぶれてしまうことがありますので、参考程度です)\n\nplot(model)\n\n\n\n\n\n\n\n\n構築したモデルを使用して予測を行います。\n\nres_pred &lt;- predict(model, test.df[-tar.idx])\n\n予測結果から得られる混同行列を確認します。\n(Summaryにて訓練データでの混同行列が確認できるので、以下テストデータでの混同行列の表示を省略します)\n\nres_act &lt;- test.df[[tar.idx]]\nconf_mat &lt;- table(res_act, res_pred)\nprint(conf_mat)\n\n       res_pred\nres_act neg pos\n    neg 116  23\n    pos  47  45\n\n\n各種評価指標を確認します。計算用の関数の定義です。\n\ncreate_df_res &lt;- function(cmat){\n  TN &lt;- cmat[1,1]\n  FP &lt;- cmat[1,2]\n  FN &lt;- cmat[2,1]\n  TP &lt;- cmat[2,2]\n  \n  # 正解率：どれだけ正しく分類できたかの割合\n  acc &lt;- round((TP + TN) / (TP + TN + FN + FP), 3)\n  \n  # 適合率：陽性と判定されたものがどれだけ正しく陽性であるかの割合\n  prec &lt;- round(TP / (TP + FP), 3)\n  \n  # 再現率（真陽性率）：実際に陽性のものをどれだけ正しく陽性と判定できたかの割合\n  rec &lt;- round(TP / (TP + FN), 3)\n  \n  # F値：適合率と再現率の調和平均（両者はトレードオフの関係）\n  Fval &lt;- round(2 * rec * prec / (rec + prec), 3)\n  \n  # 真陰性率：実際に陰性のものをどれだけ正しく陰性と判定できたかの割合\n  TNRat &lt;- round(TN / (TN + FP), 3)\n  \n  df_rat &lt;- data.frame(\n    Item = c(\"正解率\", \"適合率\", \"再現率（真陽性率）\", \"F値\", \"真陰性率\"), \n    Rate = c(acc, prec, rec, Fval, TNRat)\n  )\n  \n  df_rat\n}\n\ndata.frameの出力整形用です。\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nshow_df &lt;- function(df, cap){\n  kable(df, format = \"pandoc\", caption = cap) %&gt;%\n    kable_styling(full_width = FALSE, position = \"left\")\n}\n\n各種指標を確認します。\n\ndf_res &lt;- create_df_res (conf_mat)\nshow_df(df_res, \"結果\")\n\n\n結果\n\n\nItem\nRate\n\n\n\n\n正解率\n0.697\n\n\n適合率\n0.662\n\n\n再現率（真陽性率）\n0.489\n\n\nF値\n0.562\n\n\n真陰性率\n0.835",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>C50</span>"
    ]
  },
  {
    "objectID": "articles/C50.html#c5.0のハイパーパラメーター",
    "href": "articles/C50.html#c5.0のハイパーパラメーター",
    "title": "C50",
    "section": "C5.0のハイパーパラメーター",
    "text": "C5.0のハイパーパラメーター\n以下、C5.0モデルのパラメータをいくつか変更し、モデル性能の差を簡易的に見てゆきます。\n\ntrials\ntrials:C5.0モデルはブースティングを実装しており、ブースティングの繰り返しの数を設定できます。\n\nmodel_trials &lt;- C5.0(\n  x = train.df[-tar.idx], \n  y = train.df[[tar.idx]],\n  trials = 3\n)\n\nsummary(model_trials)\n\n\nCall:\nC5.0.default(x = train.df[-tar.idx], y = train.df[[tar.idx]], trials = 3)\n\n\nC5.0 [Release 2.07 GPL Edition]     Fri Aug  1 17:34:31 2025\n-------------------------------\n\nClass specified by attribute `outcome'\n\nRead 537 cases (9 attributes) from undefined.data\n\n-----  Trial 0:  -----\n\nDecision tree:\n\nglucose &lt;= 127:\n:...age &lt;= 28:\n:   :...mass &lt;= 32.2: neg (113/1)\n:   :   mass &gt; 32.2:\n:   :   :...triceps &gt; 8: neg (64/8)\n:   :       triceps &lt;= 8:\n:   :       :...glucose &lt;= 110: neg (7)\n:   :           glucose &gt; 110: pos (4)\n:   age &gt; 28:\n:   :...mass &lt;= 26.3: neg (33/1)\n:       mass &gt; 26.3:\n:       :...pedigree &lt;= 0.845: neg (100/30)\n:           pedigree &gt; 0.845:\n:           :...pregnant &lt;= 1: neg (5/1)\n:               pregnant &gt; 1: pos (10)\nglucose &gt; 127:\n:...glucose &lt;= 165:\n    :...mass &lt;= 29.9: neg (55/15)\n    :   mass &gt; 29.9:\n    :   :...insulin &lt;= 196: pos (70/20)\n    :       insulin &gt; 196:\n    :       :...age &lt;= 32: neg (13/1)\n    :           age &gt; 32: pos (8/1)\n    glucose &gt; 165:\n    :...pedigree &gt; 1.114: neg (5/2)\n        pedigree &lt;= 1.114:\n        :...age &gt; 27: pos (41/1)\n            age &lt;= 27:\n            :...triceps &lt;= 26: neg (3)\n                triceps &gt; 26: pos (6)\n\n-----  Trial 1:  -----\n\nDecision tree:\n\nglucose &gt; 154: pos (75.3/15.4)\nglucose &lt;= 154:\n:...mass &lt;= 26.3: neg (96.8/8.6)\n    mass &gt; 26.3:\n    :...age &lt;= 28:\n        :...pressure &lt;= 44: pos (9.1/2.4)\n        :   pressure &gt; 44: neg (150.8/33.5)\n        age &gt; 28:\n        :...insulin &lt;= 142: neg (163.6/79.5)\n            insulin &gt; 142: pos (41.4/9.3)\n\n-----  Trial 2:  -----\n\nDecision tree:\n\nglucose &lt;= 120: neg (213.2/29.8)\nglucose &gt; 120:\n:...mass &lt;= 29.9:\n    :...age &lt;= 25: neg (19.5)\n    :   age &gt; 25:\n    :   :...glucose &lt;= 164: neg (47.7/15.6)\n    :       glucose &gt; 164: pos (6.8)\n    mass &gt; 29.9:\n    :...pedigree &lt;= 0.315:\n        :...age &lt;= 30: neg (22.9/2.1)\n        :   age &gt; 30: pos (36.5/13.9)\n        pedigree &gt; 0.315:\n        :...pressure &lt;= 66: pos (29.6/3.3)\n            pressure &gt; 66:\n            :...age &gt; 49: pos (19.1/2.3)\n                age &lt;= 49:\n                :...insulin &gt; 465: neg (5.4)\n                    insulin &lt;= 465:\n                    :...mass &gt; 46.8: pos (10.3)\n                        mass &lt;= 46.8:\n                        :...glucose &lt;= 123: neg (5.8)\n                            glucose &gt; 123:\n                            :...pressure &lt;= 70: pos (11.2/0.7)\n                                pressure &gt; 70:\n                                :...glucose &gt; 156: pos (14.7/2.3)\n                                    glucose &lt;= 156:\n                                    :...glucose &lt;= 132: pos (16.6/3.5)\n                                        glucose &gt; 132: neg (32.9/11.4)\n\n\nEvaluation on training data (537 cases):\n\nTrial       Decision Tree   \n-----     ----------------  \n      Size      Errors  \n\n   0        16   81(15.1%)\n   1         6  110(20.5%)\n   2        15   94(17.5%)\nboost            79(14.7%)   &lt;&lt;\n\n\n       (a)   (b)    &lt;-classified as\n      ----  ----\n       349    12    (a): class neg\n        67   109    (b): class pos\n\n\n    Attribute usage:\n\n    100.00% glucose\n    100.00% mass\n     99.81% age\n     51.40% pedigree\n     46.55% pressure\n     45.81% insulin\n     15.64% triceps\n      2.79% pregnant\n\n\nTime: 0.0 secs\n\n\n結果を確認します。各種数値が向上していることが分かります。\n\nres_trials_pred &lt;- predict(model_trials, test.df[-tar.idx])\nconf_mat_trials &lt;- table(res_act, res_trials_pred)\ndf_res_trials &lt;- create_df_res(conf_mat_trials)\nshow_df(df_res_trials, \"結果（trials）\")\n\n\n結果（trials）\n\n\nItem\nRate\n\n\n\n\n正解率\n0.740\n\n\n適合率\n0.750\n\n\n再現率（真陽性率）\n0.522\n\n\nF値\n0.616\n\n\n真陰性率\n0.885\n\n\n\n\n\n\n\nwinnow\nwinnow：デフォルトはFALSEです。TRUEにすると、重要でない特徴量を削減してくれます。\n\nmodel_winnow &lt;- C5.0(\n  x = train.df[-tar.idx], \n  y = train.df[[tar.idx]],\n　control = C5.0Control(winnow = TRUE)\n)\n\nsummary(model_winnow)\n\n\nCall:\nC5.0.default(x = train.df[-tar.idx], y = train.df[[tar.idx]], control\n = C5.0Control(winnow = TRUE))\n\n\nC5.0 [Release 2.07 GPL Edition]     Fri Aug  1 17:34:32 2025\n-------------------------------\n\nClass specified by attribute `outcome'\n\nRead 537 cases (9 attributes) from undefined.data\n\n4 attributes winnowed\nEstimated importance of remaining attributes:\n\n     29%  glucose\n      5%  age\n      1%  insulin\n     &lt;1%  pedigree\n\nDecision tree:\n\nglucose &lt;= 127: neg (336/55)\nglucose &gt; 127:\n:...glucose &gt; 165: pos (55/7)\n    glucose &lt;= 165:\n    :...age &lt;= 23: neg (25/5)\n        age &gt; 23:\n        :...pedigree &gt; 0.722: pos (21/3)\n            pedigree &lt;= 0.722:\n            :...pedigree &gt; 0.661: neg (6)\n                pedigree &lt;= 0.661:\n                :...pedigree &lt;= 0.305: neg (48/19)\n                    pedigree &gt; 0.305: pos (46/15)\n\n\nEvaluation on training data (537 cases):\n\n        Decision Tree   \n      ----------------  \n      Size      Errors  \n\n         7  104(19.4%)   &lt;&lt;\n\n\n       (a)   (b)    &lt;-classified as\n      ----  ----\n       336    25    (a): class neg\n        79    97    (b): class pos\n\n\n    Attribute usage:\n\n    100.00% glucose\n     27.19% age\n     22.53% pedigree\n\n\nTime: 0.0 secs\n\n\n結果の確認です。最初のモデルよりも正解率は高いですが、F値は下がりました。\n\nres_winnow_pred &lt;- predict(model_winnow, test.df[-tar.idx])\nconf_mat_winnow &lt;- table(res_act, res_winnow_pred)\ndf_res_winnow &lt;- create_df_res(conf_mat_winnow)\nshow_df(df_res_winnow, \"結果（winnow）\")\n\n\n結果（winnow）\n\n\nItem\nRate\n\n\n\n\n正解率\n0.710\n\n\n適合率\n0.736\n\n\n再現率（真陽性率）\n0.424\n\n\nF値\n0.538\n\n\n真陰性率\n0.899\n\n\n\n\n\n\n\nCF\nCF：信頼係数です。デフォルトは0.25です。小さくすると過学習を防いでくれます。\n\nmodel_CF &lt;- C5.0(\n  x = train.df[-tar.idx], \n  y = train.df[[tar.idx]],\n  control = C5.0Control(CF = 0.1)\n)\n\nsummary(model_CF)\n\n\nCall:\nC5.0.default(x = train.df[-tar.idx], y = train.df[[tar.idx]], control\n = C5.0Control(CF = 0.1))\n\n\nC5.0 [Release 2.07 GPL Edition]     Fri Aug  1 17:34:32 2025\n-------------------------------\n\nClass specified by attribute `outcome'\n\nRead 537 cases (9 attributes) from undefined.data\n\nDecision tree:\n\nglucose &lt;= 127:\n:...age &lt;= 28: neg (188/13)\n:   age &gt; 28:\n:   :...mass &lt;= 26.3: neg (33/1)\n:       mass &gt; 26.3:\n:       :...pedigree &lt;= 0.845: neg (100/30)\n:           pedigree &gt; 0.845: pos (15/4)\nglucose &gt; 127:\n:...glucose &gt; 165: pos (55/7)\n    glucose &lt;= 165:\n    :...mass &lt;= 29.9: neg (55/15)\n        mass &gt; 29.9:\n        :...insulin &lt;= 196: pos (70/20)\n            insulin &gt; 196:\n            :...age &lt;= 32: neg (13/1)\n                age &gt; 32: pos (8/1)\n\n\nEvaluation on training data (537 cases):\n\n        Decision Tree   \n      ----------------  \n      Size      Errors  \n\n         9   92(17.1%)   &lt;&lt;\n\n\n       (a)   (b)    &lt;-classified as\n      ----  ----\n       329    32    (a): class neg\n        60   116    (b): class pos\n\n\n    Attribute usage:\n\n    100.00% glucose\n     66.48% age\n     54.75% mass\n     21.42% pedigree\n     16.95% insulin\n\n\nTime: 0.0 secs\n\n\n結果の確認です。最初のモデルよりも性能が向上しています。\n\nres_CF_pred &lt;- predict(model_CF, test.df[-tar.idx])\nconf_mat_CF &lt;- table(res_act, res_CF_pred)\ndf_res_CF &lt;- create_df_res(conf_mat_CF)\nshow_df(df_res_CF, \"結果（CF）\")\n\n\n結果（CF）\n\n\nItem\nRate\n\n\n\n\n正解率\n0.723\n\n\n適合率\n0.684\n\n\n再現率（真陽性率）\n0.565\n\n\nF値\n0.619\n\n\n真陰性率\n0.827\n\n\n\n\n\n\n\nminCases\nminCases：葉ノードに含まれる必要サンプル数です。デフォルトは2です。大きくすると過学習を防いでくれます。\n\nmodel_minCases &lt;- C5.0(\n  x = train.df[-tar.idx], \n  y = train.df[[tar.idx]],\n  control = C5.0Control(minCases = 15)\n)\n\nsummary(model_minCases)\n\n\nCall:\nC5.0.default(x = train.df[-tar.idx], y = train.df[[tar.idx]], control\n = C5.0Control(minCases = 15))\n\n\nC5.0 [Release 2.07 GPL Edition]     Fri Aug  1 17:34:32 2025\n-------------------------------\n\nClass specified by attribute `outcome'\n\nRead 537 cases (9 attributes) from undefined.data\n\nDecision tree:\n\nglucose &lt;= 127:\n:...age &lt;= 28: neg (188/13)\n:   age &gt; 28:\n:   :...mass &lt;= 26.3: neg (33/1)\n:       mass &gt; 26.3:\n:       :...pedigree &lt;= 0.845: neg (100/30)\n:           pedigree &gt; 0.845: pos (15/4)\nglucose &gt; 127:\n:...glucose &gt; 165: pos (55/7)\n    glucose &lt;= 165:\n    :...mass &lt;= 29.9: neg (55/15)\n        mass &gt; 29.9:\n        :...pregnant &gt; 7: pos (18/2)\n            pregnant &lt;= 7:\n            :...insulin &lt;= 196: pos (56/18)\n                insulin &gt; 196: neg (17/4)\n\n\nEvaluation on training data (537 cases):\n\n        Decision Tree   \n      ----------------  \n      Size      Errors  \n\n         9   94(17.5%)   &lt;&lt;\n\n\n       (a)   (b)    &lt;-classified as\n      ----  ----\n       330    31    (a): class neg\n        63   113    (b): class pos\n\n\n    Attribute usage:\n\n    100.00% glucose\n     62.57% age\n     54.75% mass\n     21.42% pedigree\n     16.95% pregnant\n     13.59% insulin\n\n\nTime: 0.0 secs\n\n\n結果の確認です。最初のモデルよりも性能が向上しています。\n\nres_minCases_pred &lt;- predict(model_minCases, test.df[-tar.idx])\nconf_mat_minCases &lt;- table(res_act, res_minCases_pred)\ndf_res_minCases &lt;- create_df_res(conf_mat_minCases)\nshow_df(df_res_minCases, \"結果（minCases）\")\n\n\n結果（minCases）\n\n\nItem\nRate\n\n\n\n\n正解率\n0.727\n\n\n適合率\n0.699\n\n\n再現率（真陽性率）\n0.554\n\n\nF値\n0.618\n\n\n真陰性率\n0.842\n\n\n\n\n\n各モデルの結果を一覧にします。\n\ndf_res_all &lt;- data.frame(\n  Item = df_res[,1],\n  default = df_res[, 2], \n  trials = df_res_trials[,2], \n  winnow = df_res_winnow[,2], \n  CF = df_res_CF[,2],\n  minCases = df_res_minCases[,2])\nshow_df(df_res_all, \"各モデルの結果一覧\")\n\n\n各モデルの結果一覧\n\n\nItem\ndefault\ntrials\nwinnow\nCF\nminCases\n\n\n\n\n正解率\n0.697\n0.740\n0.710\n0.723\n0.727\n\n\n適合率\n0.662\n0.750\n0.736\n0.684\n0.699\n\n\n再現率（真陽性率）\n0.489\n0.522\n0.424\n0.565\n0.554\n\n\nF値\n0.562\n0.616\n0.538\n0.619\n0.618\n\n\n真陰性率\n0.835\n0.885\n0.899\n0.827\n0.842",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>C50</span>"
    ]
  },
  {
    "objectID": "articles/class.html",
    "href": "articles/class.html",
    "title": "class",
    "section": "",
    "text": "パッケージの概要\nclassパッケージは分類に関する様々な関数を提供します。本ドキュメントではknn関数を用いたk最近傍法の使用方法を確認します。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>class</span>"
    ]
  },
  {
    "objectID": "articles/class.html#使用例irisデータの分類",
    "href": "articles/class.html#使用例irisデータの分類",
    "title": "class",
    "section": "使用例：irisデータの分類",
    "text": "使用例：irisデータの分類\nirisデータを用いて、がく弁・花弁の長さ・幅の情報からアヤメを分類するモデルをclassパッケージを用いて構築します。\n\nirisデータセットを読み込む\nirisデータを読み込み、データの先頭を表示します。\n\nSepal.Length：がく弁の長さ\nSepal.Width：がく弁の幅\nPetal.Length：花弁の長さ\nPetal.Width：花弁の幅\n\nアヤメの種類はsetosa(1)、versicolor(2)、virginica(3)の3種類です。\n\ndata(iris)\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\n\nirisデータの構造\nirisデータの各種構造を確認します。\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nまた、データを散布図にプロットして確認します。\n\nplot(iris, col=c(2, 3, 4)[iris$Species])\n\n\n\n\n\n\n\n\n\n\nモデル構築\nirisデータをモデル生成のための訓練データと、モデル評価のためのテストデータに分割します。データ割合は訓練データを7割、テストデータを3割とします。確認のため、データサイズを出力します。\n\n# 再現性のためにシードを設定\nset.seed(123)\n\n# データの分割\nsample_indices &lt;- sample(1:nrow(iris), 0.7 * nrow(iris))  \ntrain_data &lt;- iris[sample_indices, ]\ntest_data &lt;- iris[-sample_indices, ]\n\n# データサイズの確認\nc(nrow(iris), nrow(train_data), nrow(test_data))\n\n[1] 150 105  45\n\n\nknn関数を使用し、k最近傍法を実行します。\n\nres &lt;- knn(train_data[-5], test_data[-5], train_data$Species, k = 3)\n\n分類結果とテストデータを比べてみます。概ね合っていることが分かります。\n\n(table(res, test_data$Species))\n\n            \nres          setosa versicolor virginica\n  setosa         14          0         0\n  versicolor      0         17         0\n  virginica       0          1        13",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>class</span>"
    ]
  },
  {
    "objectID": "articles/corrplot.html",
    "href": "articles/corrplot.html",
    "title": "corrplot",
    "section": "",
    "text": "パッケージの概要\ncorrplotは、相関行列の可視化に特化したパッケージです。 デフォルトの設定でも相関行列を見やすい形で表示できるほか、 50を超えるパラメータにより見た目を自由にカスタマイズできます。\nlibrary(corrplot) \nlibrary(Lahman) #今回使用するデータセット \nlibrary(data.table) #データの成形に使用\nlibrary(lsr) #相関行列以外の例に使用",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>corrplot</span>"
    ]
  },
  {
    "objectID": "articles/corrplot.html#データセットの準備",
    "href": "articles/corrplot.html#データセットの準備",
    "title": "corrplot",
    "section": "データセットの準備",
    "text": "データセットの準備\n今回は公開されているデータセットでもカラム数の多い、’Sean Lahman’s Baseball Database’というデータセットを使用します。\nこれはMLBの選手別・チーム別の成績等を蓄積したデータベースで、 今回は選手情報が格納されたPeople、各年度の打撃成績が格納されたBatting、 年俸が格納されたSalariesの3つのテーブルを結合して使用します。\nデータセットの詳細についてはパッケージ付属のDocumentationを参照してください。\nまず、上記テーブルの読み込み・結合と、最低限の前処理を行います。\n\n#大規模データの処理に向くdata.tableとしてデータを保持\ndt_mlb_people &lt;- as.data.table(People)\ndt_mlb_batting &lt;- as.data.table(Batting)\ndt_mlb_salaries &lt;- as.data.table(Salaries)\n#People(選手情報)からは一部の列のみを取得\ndt_mlb_people &lt;- dt_mlb_people[,.(playerID, birthCountry, birthState, weight, height, bats, throws, birthDate)]\n#Salaries(年俸)テーブルとBatting(打撃成績)をLeft join\ndt_mlb &lt;- dt_mlb_batting[dt_mlb_salaries, on = .(yearID, teamID, lgID, playerID)]\n#上記テーブルとPeopleをLeft join\ndt_mlb &lt;- dt_mlb_people[dt_mlb, on = .(playerID)]\n#文字列型となっている列などをfactor型に変換\ndt_mlb$playerID &lt;- as.factor(dt_mlb$playerID)\ndt_mlb$birthCountry &lt;- as.factor(dt_mlb$birthCountry)\ndt_mlb$birthState &lt;- as.factor(dt_mlb$birthState)\ndt_mlb$stint &lt;- as.factor(dt_mlb$stint)\n#日付型となっている列を数値型に変換\ndt_mlb$birthDate &lt;- as.numeric(dt_mlb$birthDate)\n\n出来上がったテーブルの概要は以下のとおり。\n\n#レコード数\nnrow(dt_mlb)\n## [1] 26437\n#列名リスト\nnames(dt_mlb)\n##  [1] \"playerID\"     \"birthCountry\" \"birthState\"   \"weight\"       \"height\"      \n##  [6] \"bats\"         \"throws\"       \"birthDate\"    \"yearID\"       \"stint\"       \n## [11] \"teamID\"       \"lgID\"         \"G\"            \"AB\"           \"R\"           \n## [16] \"H\"            \"X2B\"          \"X3B\"          \"HR\"           \"RBI\"         \n## [21] \"SB\"           \"CS\"           \"BB\"           \"SO\"           \"IBB\"         \n## [26] \"HBP\"          \"SH\"           \"SF\"           \"GIDP\"         \"salary\"\n#テーブル表示\ndt_mlb\n##         playerID birthCountry birthState weight height   bats throws birthDate\n##           &lt;fctr&gt;       &lt;fctr&gt;     &lt;fctr&gt;  &lt;int&gt;  &lt;int&gt; &lt;fctr&gt; &lt;fctr&gt;     &lt;num&gt;\n##     1: barkele01          USA         KY    225     77      R      R     -5292\n##     2: bedrost01          USA         MA    200     75      R      R     -4409\n##     3: benedbr01          USA         AL    175     73      R      R     -5250\n##     4:  campri01          USA         GA    195     73      R      R     -6049\n##     5: ceronri01          USA         NJ    192     71      R      R     -5706\n##    ---                                                                        \n## 26433: strasst01          USA         CA    239     77      R      R      6775\n## 26434: taylomi02          USA         FL    215     76      R      R      7754\n## 26435: treinbl01          USA         KS    225     77      R      R      6755\n## 26436: werthja01          USA         IL    235     77      R      R      3426\n## 26437: zimmery01          USA         NC    215     75      R      R      5384\n##        yearID  stint teamID   lgID     G    AB     R     H   X2B   X3B    HR\n##         &lt;int&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n##     1:   1985      1    ATL     NL    20    17     0     0     0     0     0\n##     2:   1985      1    ATL     NL    37    64     3     5     0     0     0\n##     3:   1985      1    ATL     NL    70   208    12    42     6     0     0\n##     4:   1985      1    ATL     NL    66    13     1     3     0     0     1\n##     5:   1985      1    ATL     NL    96   282    15    61     9     0     3\n##    ---                                                                      \n## 26433:   2016      1    WAS     NL    24    48     3    10     1     0     0\n## 26434:   2016      1    WAS     NL    76   221    28    51    11     0     7\n## 26435:   2016      1    WAS     NL    73     0     0     0     0     0     0\n## 26436:   2016      1    WAS     NL   143   525    84   128    28     0    21\n## 26437:   2016      1    WAS     NL   115   427    60    93    18     1    15\n##          RBI    SB    CS    BB    SO   IBB   HBP    SH    SF  GIDP   salary\n##        &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n##     1:     0     0     1     0     7     0     0     0     0     0   870000\n##     2:     1     0     0     1    22     0     0     6     0     0   550000\n##     3:    20     0     1    22    12     1     1     4     2     8   545000\n##     4:     2     0     0     1     5     0     0     1     0     0   633333\n##     5:    25     0     3    29    25     1     1     0     4    15   625000\n##    ---                                                                     \n## 26433:     2     0     0     2    11     0     0     7     0     2 10400000\n## 26434:    16    14     3    14    77     0     1     0     1     2   524000\n## 26435:     0     0     0     0     0     0     0     0     0     0   524900\n## 26436:    69     5     1    71   139     0     4     0     6    17 21733615\n## 26437:    46     4     1    29   104     1     5     0     6    12 14000000\n#サマリを表示\nsummary(dt_mlb)\n##       playerID        birthCountry     birthState        weight     \n##  moyerja01:   25   USA      :20505   CA     : 4872   Min.   :140.0  \n##  vizquom01:   24   D.R.     : 2140   TX     : 1551   1st Qu.:185.0  \n##  glavito02:   23   Venezuela: 1107   FL     : 1490   Median :195.0  \n##  bondsba01:   22   P.R.     :  892   NY     :  984   Mean   :199.2  \n##  griffke02:   22   Mexico   :  333   IL     :  963   3rd Qu.:215.0  \n##  rodrial01:   22   CAN      :  304   (Other):15534   Max.   :315.0  \n##  (Other)  :26299   (Other)  : 1156   NA's   : 1043                  \n##      height     bats      throws      birthDate            yearID    \n##  Min.   :66.0   B: 2579   B:    0   Min.   :-16407.0   Min.   :1985  \n##  1st Qu.:72.0   L: 7488   L: 5660   1st Qu.: -1918.0   1st Qu.:1994  \n##  Median :74.0   R:16370   R:20777   Median :   575.0   Median :2001  \n##  Mean   :73.5             S:    0   Mean   :   693.7   Mean   :2001  \n##  3rd Qu.:75.0                       3rd Qu.:  3426.0   3rd Qu.:2009  \n##  Max.   :83.0                       Max.   :  9168.0   Max.   :2016  \n##                                                                      \n##   stint           teamID      lgID             G                AB       \n##  1   :25263   LAN    :  958   AL:12962   Min.   :  1.00   Min.   :  0.0  \n##  2   :  163   CLE    :  950   NL:13475   1st Qu.: 29.00   1st Qu.:  1.0  \n##  3   :   14   PHI    :  949              Median : 56.00   Median : 63.0  \n##  4   :    1   BOS    :  945              Mean   : 67.84   Mean   :173.1  \n##  NA's:  996   SLN    :  943              3rd Qu.:107.00   3rd Qu.:331.0  \n##               BAL    :  940              Max.   :163.00   Max.   :716.0  \n##               (Other):20752              NA's   :996      NA's   :996    \n##        R                H               X2B              X3B         \n##  Min.   :  0.00   Min.   :  0.00   Min.   : 0.000   Min.   : 0.0000  \n##  1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.: 0.000   1st Qu.: 0.0000  \n##  Median :  5.00   Median : 11.00   Median : 2.000   Median : 0.0000  \n##  Mean   : 23.35   Mean   : 45.64   Mean   : 8.816   Mean   : 0.9713  \n##  3rd Qu.: 41.00   3rd Qu.: 85.00   3rd Qu.:16.000   3rd Qu.: 1.0000  \n##  Max.   :152.00   Max.   :262.00   Max.   :59.000   Max.   :23.0000  \n##  NA's   :996      NA's   :996      NA's   :996      NA's   :996      \n##        HR              RBI               SB                CS        \n##  Min.   : 0.000   Min.   :  0.00   Min.   :  0.000   Min.   : 0.000  \n##  1st Qu.: 0.000   1st Qu.:  0.00   1st Qu.:  0.000   1st Qu.: 0.000  \n##  Median : 0.000   Median :  5.00   Median :  0.000   Median : 0.000  \n##  Mean   : 5.133   Mean   : 22.28   Mean   :  3.323   Mean   : 1.419  \n##  3rd Qu.: 7.000   3rd Qu.: 38.00   3rd Qu.:  3.000   3rd Qu.: 2.000  \n##  Max.   :73.000   Max.   :165.00   Max.   :110.000   Max.   :29.000  \n##  NA's   :996      NA's   :996      NA's   :996       NA's   :996     \n##        BB               SO             IBB               HBP        \n##  Min.   :  0.00   Min.   :  0.0   Min.   :  0.000   Min.   : 0.000  \n##  1st Qu.:  0.00   1st Qu.:  0.0   1st Qu.:  0.000   1st Qu.: 0.000  \n##  Median :  4.00   Median : 17.0   Median :  0.000   Median : 0.000  \n##  Mean   : 16.93   Mean   : 32.4   Mean   :  1.434   Mean   : 1.554  \n##  3rd Qu.: 28.00   3rd Qu.: 55.0   3rd Qu.:  1.000   3rd Qu.: 2.000  \n##  Max.   :232.00   Max.   :223.0   Max.   :120.000   Max.   :35.000  \n##  NA's   :996      NA's   :996     NA's   :996       NA's   :996     \n##        SH               SF              GIDP            salary        \n##  Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Min.   :       0  \n##  1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.:  293750  \n##  Median : 0.000   Median : 0.000   Median : 1.000   Median :  550000  \n##  Mean   : 1.637   Mean   : 1.463   Mean   : 3.915   Mean   : 2085160  \n##  3rd Qu.: 2.000   3rd Qu.: 2.000   3rd Qu.: 7.000   3rd Qu.: 2350000  \n##  Max.   :39.000   Max.   :17.000   Max.   :35.000   Max.   :33000000  \n##  NA's   :996      NA's   :996      NA's   :996",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>corrplot</span>"
    ]
  },
  {
    "objectID": "articles/corrplot.html#使用例",
    "href": "articles/corrplot.html#使用例",
    "title": "corrplot",
    "section": "使用例",
    "text": "使用例\n相関行列を計算するには数値型である必要があるため、factor型となっている特徴量は除いておきます。\n\ncols_factor &lt;- names(dt_mlb)[sapply(dt_mlb, is.factor)]\ncols_nonfactor &lt;- setdiff(names(dt_mlb), cols_factor)\ndt &lt;- dt_mlb[,.SD,.SDcol = c(cols_nonfactor)]\n\ncor関数で相関行列を計算し、corrplot関数を用いて相関行列を可視化します。\nsalaryとの相関を確認してみると、打撃成績の中ではHR(ホームラン)との相関が相対的に強いように見えます。\n\nmat_cor &lt;- cor(dt, \n               method = 'pearson', #ピアソンの積率相関係数。順位相関係数(ケンドール、スピアマン)も使用できる。\n               use = 'pairwise.complete.obs')\n#NAとなっているデータの取り扱い。\n#デフォルトはNAが混ざっている列の相関係数をNAとするものであったので、\n#NAとなっているデータを無視して計算するように設定。\ncorrplot(mat_cor)\n\n\n\n\n\n\n\n\n以下、いくつか見た目のカスタマイズ例を紹介します。\n\ncorrplot(mat_cor[c(1:12,22), c(1:12,22)],\n         method = 'color', #単純な色分けのみにする\n         tl.cex = 0.75, #特徴量名のフォント大きさ調節(小さめ)\n         addCoef.col = 'black', #相関係数を指定した色で表示\n         number.cex = 0.65, #相関係数のフォント大きさ調節(小さめ)\n         order = 'AOE', #相関の高いもの同士が近くになるように表示順序を変更\n         col = adjustcolor(COL2('BrBG', 200),#色合いを変更 200は色の段階数\n                           offset = c(1/5, 1/5, 1/5, 0), #相関係数が見えやすいよう少し薄く色を調節\n                           transform = diag(c(4/5, 4/5, 4/5, 1))) \n         )\n\n\n\n\n\n\n\n\n\nmat_p &lt;- cor.mtest(dt, conf.level = 0.95)$p #p値の計算\ncorrplot(mat_cor[c(1:12,22), c(1:12,22)],\n         method = 'square', #正方形で表示\n         diag = FALSE, #対角線は表示しない\n         tl.pos = 'd', #特徴量名を対角線に表示\n         tl.cex = 0.6, #特徴量名のフォント大きさ調節(小さめ)\n         order = 'hclust', #相関の高いもの同士が近くになるように表示順序を変更\n         addrect = 3, #hclustを選択した時のみ指定可能、近いもの同士を3つに分類して正方形の枠で囲む\n         rect.col = 'gray40', #前述の枠の色\n         p.mat = mat_p[c(1:12,22), c(1:12,22)], #p値行列を与える\n         sig.level = 0.10, #p値が0.1以上であれば×を表示\n         )\n\n\n\n\n\n\n\n\n\nmat_p = cor.mtest(dt, conf.level = 0.95)$p #p値の計算\ncorrplot(mat_cor[c(1:12,22), c(1:12,22)],\n         type = 'lower', #下半分のみ表示\n         method = 'shade', #単純な色分けだが、指定した条件を満たすの場合は斜め線を入れる\n         addshade = 'negative', #斜め線を入れる条件\n         tl.cex = 0.6, #特徴量名のフォント大きさ調節(小さめ)\n         order = 'alphabet', #名前順\n         p.mat = mat_p[c(1:12,22), c(1:12,22)], #p値行列を与える\n         insig ='label_sig', #p値に応じて*を入れる\n         sig.level = c(0.001, 0.01, 0.05), #*の数のしきい値\n         pch.cex = 0.9, #*の大きさ\n         )\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(mat_cor[c(1:12,22), c(1:12,22)], #corrplot.mixedとすると二つの表示形式をミックスできる\n         lower = 'number', #下半分は相関係数を表示する\n         upper = 'ellipse', #上半分は楕円で表示する\n         tl.cex = 0.6, #特徴量名のフォント大きさ調節(小さめ)\n         number.cex = 0.55, #相関係数のフォント大きさ調節(小さめ)\n         order = 'AOE', #相関の高いもの同士が近くになるように表示順序を変更\n         )\n\n\n\n\n\n\n\n\n相関行列ではないものを表示することも可能です。\n例えば、質的変数と量的変数の組に対して分散分析のイプシロンを表示してみます。\n\nmat_nf &lt;- matrix(nrow = length(cols_nonfactor), ncol = length(cols_factor))\nrownames(mat_nf) &lt;- cols_nonfactor\ncolnames(mat_nf) &lt;- cols_factor\n\ndt &lt;- dt_mlb\nfor(c_r in cols_nonfactor){\n  for(c_c in cols_factor){\n    dt_tmp &lt;- dt[!is.na(dt[[c_r]]) & !is.na(dt[[c_c]]),.SD, .SDcols = c(c_r, c_c)]\n    mu &lt;- mean(dt_tmp[[c_r]])\n    ss_t &lt;- sum((dt_tmp[[c_r]]-mu)^2)\n    ss_m &lt;- sum(dt_tmp[, .(ss = .N * (mean(.SD[[1]]) - ..mu)^2), by = c_c, .SDcols = c_r]$ss)\n    ss_e &lt;- ss_t - ss_m\n    dig_free &lt;- length(unique(dt_tmp[[c_c]]))-1\n    ms_e &lt;- ss_e/(nrow(dt_tmp)-dig_free-1)\n    mat_nf[c_r, c_c] &lt;- sqrt(max((ss_m - dig_free*ms_e)/ss_t, 0))\n  }\n}\ncorrplot(mat_nf[,setdiff(cols_factor, c('playerID'))],\n         is.corr = FALSE, #相関行列ではない場合 (数値の範囲が[-1, 1]固定ではなくなる)\n         col.lim = c(0, 1), #数値の範囲指定 \n         method = 'color', tl.cex = 0.8,\n         col = COL1('YlGn'),\n         cl.ratio = 0.4 #凡例の幅調節　そのままだと細すぎたので太目に変更\n         )\n\nWarning in corrplot(mat_nf[, setdiff(cols_factor, c(\"playerID\"))], is.corr =\nFALSE, : col.lim interval too wide, please set a suitable value\n\n\n\n\n\n\n\n\n\nまた別の例として、質的変数同士の組に対してはクラメールの連関係数を表示してみます。\n\nmat_ff &lt;- matrix(nrow = length(cols_factor), ncol = length(cols_factor))\nrownames(mat_ff) &lt;- cols_factor\ncolnames(mat_ff) &lt;- cols_factor\n\ndt &lt;- dt_mlb\nfor(c_r in cols_factor){\n  for(c_c in cols_factor){\n    #非常に小さいカテゴリがある場合は検定の仮定となる近似が満たされなくなるため、警告が表示される\n    suppressWarnings(mat_ff[c_r, c_c] &lt;- lsr::cramersV(dt[[c_r]], dt[[c_c]]))\n  }\n}\ncorrplot(mat_ff[setdiff(cols_factor, c('playerID')), setdiff(cols_factor, c('playerID'))],\n         is.corr = FALSE, #相関行列ではない場合 (数値の範囲が[-1, 1]固定ではなくなる)\n         col.lim = c(0, 1), #数値の範囲指定 \n         method = 'color',\n         addgrid.col = 'white', #罫線を白色で表示する\n         tl.cex = 0.8, addCoef.col = 'black', number.cex = 0.65, \n         col = COL1('Blues'),\n         )",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>corrplot</span>"
    ]
  },
  {
    "objectID": "articles/corrplot.html#参考",
    "href": "articles/corrplot.html#参考",
    "title": "corrplot",
    "section": "参考",
    "text": "参考\n\nAn Introduction to corrplot Package",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>corrplot</span>"
    ]
  },
  {
    "objectID": "articles/crosstable.html",
    "href": "articles/crosstable.html",
    "title": "crosstable",
    "section": "",
    "text": "パッケージの概要\ncrosstableは、クロス集計表を作成するためのツールです。複数の変数に基づいてデータを集計し、頻度等を表示します。このパッケージを使用すると、変数間の関係を簡単に分析でき、データの要約を視覚化することができます。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>crosstable</span>"
    ]
  },
  {
    "objectID": "articles/crosstable.html#使用するデータ",
    "href": "articles/crosstable.html#使用するデータ",
    "title": "crosstable",
    "section": "使用するデータ",
    "text": "使用するデータ\nクロス集計表を作成するにあたり、ここでは当パッケージに含まれるデータセットのmtcars2を使用するものとします。mtcars2はR標準のデータセットであるmtcarsに対して次の修正を加えたデータセットです。なお、mtcarsは1974年に発行されたMotor Trend US Magazineに基づき32台の異なる自動車についての情報を収めたものです。\n\n各列にラベル（説明）を追加\nrownamesを新たな列「model」としてデータフレームの一部にする\ngear（ギア数）と cyl（シリンダー数）をカテゴリカル変数として扱う\nvs（エンジンの種類）とam（トランスミッション）を文字列型に変換し、カテゴリデータとして扱う\n\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nhead(mtcars2)\n\n# A tibble: 6 × 14\n  model        mpg   cyl   disp  hp    drat  wt    qsec  vs    am    gear  carb \n  &lt;labelled&gt;   &lt;lab&gt; &lt;fct&gt; &lt;lab&gt; &lt;lab&gt; &lt;lab&gt; &lt;lab&gt; &lt;lab&gt; &lt;lab&gt; &lt;lab&gt; &lt;fct&gt; &lt;lab&gt;\n1 Mazda RX4    21.0  6     160   110   3.90  2.620 16.46 vsha… manu… 4     4    \n2 Mazda RX4 W… 21.0  6     160   110   3.90  2.875 17.02 vsha… manu… 4     4    \n3 Datsun 710   22.8  4     108    93   3.85  2.320 18.61 stra… manu… 4     1    \n4 Hornet 4 Dr… 21.4  6     258   110   3.08  3.215 19.44 stra… auto  3     1    \n5 Hornet Spor… 18.7  8     360   175   3.15  3.440 17.02 vsha… auto  3     2    \n6 Valiant      18.1  6     225   105   2.76  3.460 20.22 stra… auto  3     1    \n# ℹ 2 more variables: hp_date &lt;date&gt;, qsec_posix &lt;dttm&gt;",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>crosstable</span>"
    ]
  },
  {
    "objectID": "articles/crosstable.html#クロス集計表の作成",
    "href": "articles/crosstable.html#クロス集計表の作成",
    "title": "crosstable",
    "section": "クロス集計表の作成",
    "text": "クロス集計表の作成\nまずはcrosstable関数に他の引数を何も設定せずにmtcars2をそのまま入れてみます。このままではよくわかりません。\n\ncrosstable(mtcars2)\n\n# A tibble: 78 × 4\n   .id   label variable           value    \n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;chr&gt;    \n 1 model Model AMC Javelin        1 (3.12%)\n 2 model Model Cadillac Fleetwood 1 (3.12%)\n 3 model Model Camaro Z28         1 (3.12%)\n 4 model Model Chrysler Imperial  1 (3.12%)\n 5 model Model Datsun 710         1 (3.12%)\n 6 model Model Dodge Challenger   1 (3.12%)\n 7 model Model Duster 360         1 (3.12%)\n 8 model Model Ferrari Dino       1 (3.12%)\n 9 model Model Fiat 128           1 (3.12%)\n10 model Model Fiat X1-9          1 (3.12%)\n# ℹ 68 more rows\n\n\n次に、wt（車の重量）とcyl（シリンダー数）について、vs（エンジンの種類）ごとに集計してみます。なお、vs（エンジンの種類）はstraight（直列エンジン）とvshaped（V型エンジン）の２通りです。\n\ncrosstable(mtcars2, c(wt, cyl), by=vs)\n\n# A tibble: 7 × 5\n  .id   label               variable   straight      vshaped      \n  &lt;chr&gt; &lt;chr&gt;               &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;        \n1 wt    Weight (1000 lbs)   Min / Max  1.5 / 3.5     2.1 / 5.4    \n2 wt    Weight (1000 lbs)   Med [IQR]  2.6 [2.0;3.2] 3.6 [3.2;3.8]\n3 wt    Weight (1000 lbs)   Mean (std) 2.6 (0.7)     3.7 (0.9)    \n4 wt    Weight (1000 lbs)   N (NA)     14 (0)        18 (0)       \n5 cyl   Number of cylinders 4          10 (90.91%)   1 (9.09%)    \n6 cyl   Number of cylinders 6          4 (57.14%)    3 (42.86%)   \n7 cyl   Number of cylinders 8          0 (0%)        14 (100.00%) \n\n\nなお、as_flextable関数を使うと、クロス集計表がflextableオブジェクトに変換され、整った見やすい表になります。\n\ncrosstable(mtcars2, c(wt, cyl), by=vs) %&gt;% as_flextable()\n\nlabelvariableEnginestraightvshapedWeight (1000 lbs)Min / Max1.5 / 3.52.1 / 5.4Med [IQR]2.6 [2.0;3.2]3.6 [3.2;3.8]Mean (std)2.6 (0.7)3.7 (0.9)N (NA)14 (0)18 (0)Number of cylinders410 (90.91%)1 (9.09%)64 (57.14%)3 (42.86%)80 (0%)14 (100.00%)\n\n\n出力された集計表から、wt（車の重量）について次のことがわかります。vshapedはstraightよりも重い車が多く、またばらつきが大きいようです。\n\nMin / Max:\n\nstraight:車の重さは1.5〜3.5千ポンド\nvshaped: 車の重さは2.1〜5.4千ポンド\n\nMed [IQR]（中央値と四分位範囲）:\n\nstraight:中央値は2.6千ポンド、四分位範囲は[2.0, 3.2]千ポンド\nvshaped:中央値は3.6千ポンド、四分位範囲は[3.2, 3.8]千ポンド\n\nMean(std)（平均値と標準偏差）:\n\nstraight:平均値は2.6千ポンド、標準偏差は0.7千ポンド\nvshaped:平均値は3.7千ポンド、標準偏差は0.9千ポンド\n\nN(NA)（データ数と欠損値数）:\n\nstraight: データ数は14件、欠損値は0\nvshaped: データ数は18件、欠損値は0\n\n\nまた、cyl（シリンダー数）については次のことがわかります。straightでは4シリンダーの車が多い一方で、vshapedでは8シリンダーの車がほとんどです。\n\n4シリンダー車:\n\nstraight: 10台（90.91%）\nvshaped: 1台（9.09%）\n\n6シリンダー車:\n\nstraight: 4台（57.14%）\nvshaped: 3台（42.86%）\n\n8シリンダー車:\n\nstraight: 0台（0%）\nvshaped: 14台（100%）\n\n\n次に、vs（エンジンの種類）に加えてam（トランスミッション）により分割します。margin=c(“row”, “col”)により行方向、列方向それぞれによる各頻度の占率を出力します。total = “both”により合計行、合計列の両方が追加されます。\n\ncrosstable(mtcars2, c(wt, cyl), by=c(am, vs),\n           margin=c(\"row\", \"col\"), total = \"both\") %&gt;% as_flextable()\n\nEnginestraightvshapedTotalTransmissionautomanualautomanualWeight (1000 lbs)Min / Max2.5 / 3.51.5 / 2.83.4 / 5.42.1 / 3.61.5 / 5.4Med [IQR]3.2 [3.2;3.4]1.9 [1.7;2.3]3.8 [3.6;4.4]2.8 [2.7;3.1]3.3 [2.6;3.6]Mean (std)3.2 (0.3)2.0 (0.4)4.1 (0.8)2.9 (0.5)3.2 (1.0)N (NA)7 (0)7 (0)12 (0)6 (0)32 (0)Number of cylinders43 (42.86% / 27.27%)7 (100.00% / 63.64%)0 (0% / 0%)1 (16.67% / 9.09%)11 (34.38%)64 (57.14% / 57.14%)0 (0% / 0%)0 (0% / 0%)3 (50.00% / 42.86%)7 (21.88%)80 (0% / 0%)0 (0% / 0%)12 (100.00% / 85.71%)2 (33.33% / 14.29%)14 (43.75%)Total7 (21.88%)7 (21.88%)12 (37.50%)6 (18.75%)32 (100.00%)\n\n\n最後に、引数effectをTRUEに設定することで、効果推定を出力することもできます。\n\ncrosstable(mtcars2, where(is.numeric), by=vs, effect=TRUE) %&gt;% as_flextable()\n\nlabelvariableEngineeffectstraightvshapedMiles/(US) gallonMin / Max17.8 / 33.910.4 / 26.0Difference in means (t-test CI), ref='straight'vshaped minus straight: -7.94 [-11.27 to -4.61]Med [IQR]22.8 [21.4;29.6]15.6 [14.8;19.1]Mean (std)24.6 (5.4)16.6 (3.9)N (NA)14 (0)18 (0)Displacement (cu.in.)Min / Max71.1 / 258.0120.3 / 472.0Difference in means (Welch CI), ref='straight'vshaped minus straight: 174.69 [114.36 to 235.02]Med [IQR]120.5 [83.0;162.4]311.0 [275.8;360.0]Mean (std)132.5 (56.9)307.1 (106.8)N (NA)14 (0)18 (0)Gross horsepowerMin / Max52.0 / 123.091.0 / 335.0Difference in means (Welch CI), ref='straight'vshaped minus straight: 98.37 [66.06 to 130.67]Med [IQR]96.0 [66.0;109.8]180.0 [156.2;226.2]Mean (std)91.4 (24.4)189.7 (60.3)N (NA)14 (0)18 (0)Rear axle ratioMin / Max2.8 / 4.92.8 / 4.4Difference in means (bootstrap CI), ref='straight'vshaped minus straight: -0.47 [-0.80 to -0.13]Med [IQR]3.9 [3.7;4.1]3.2 [3.1;3.7]Mean (std)3.9 (0.5)3.4 (0.5)N (NA)14 (0)18 (0)Weight (1000 lbs)Min / Max1.5 / 3.52.1 / 5.4Difference in means (t-test CI), ref='straight'vshaped minus straight: 1.08 [0.48 to 1.68]Med [IQR]2.6 [2.0;3.2]3.6 [3.2;3.8]Mean (std)2.6 (0.7)3.7 (0.9)N (NA)14 (0)18 (0)1/4 mile timeMin / Max16.9 / 22.914.5 / 18.0Difference in means (t-test CI), ref='straight'vshaped minus straight: -2.64 [-3.52 to -1.76]Med [IQR]19.2 [18.6;20.0]17.0 [16.0;17.4]Mean (std)19.3 (1.4)16.7 (1.1)N (NA)14 (0)18 (0)Number of carburetorsMin / Max1.0 / 4.02.0 / 8.0Difference in means (bootstrap CI), ref='straight'vshaped minus straight: 1.83 [0.93 to 2.72]Med [IQR]1.5 [1.0;2.0]4.0 [2.2;4.0]Mean (std)1.8 (1.1)3.6 (1.5)N (NA)14 (0)18 (0)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>crosstable</span>"
    ]
  },
  {
    "objectID": "articles/DALEX.html",
    "href": "articles/DALEX.html",
    "title": "DALEX",
    "section": "",
    "text": "パッケージの概要\nDALEX パッケージは、予測モデルを解釈するために開発された可視化手法を統一的な記法で実行するためのパッケージです。DALEX パッケージでは、予測モデルに explain() 関数を適用することで、explainer オブジェクトが作成されます。この explainer オブジェクトに対して、DALEX パッケージのさまざまな関数を適用することで、PDP、ICE、SHAP などのプロットを簡単に作成することができます。\nそれぞれの手法の詳細については、たとえば、解釈可能な機械学習に関するウェブ書籍 Interpretable Machine Learning（邦訳）などをご参照ください。\nここでは、例として、trees データセットの Volume を Girth と Height から予測するモデルを作成し、そのモデルに対して解釈手法を適用してみましょう。\n# treesデータセットを読み込み、ランダムフォレストモデルを作成する。\nlibrary(ranger)\ndata(trees, package = \"datasets\")\nmodel_rf &lt;- ranger(Volume~., trees)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DALEX</span>"
    ]
  },
  {
    "objectID": "articles/DALEX.html#モデル解釈のためのオブジェクトを作成する",
    "href": "articles/DALEX.html#モデル解釈のためのオブジェクトを作成する",
    "title": "DALEX",
    "section": "モデル解釈のためのオブジェクトを作成する",
    "text": "モデル解釈のためのオブジェクトを作成する\nexplain() 関数は、さまざまなパッケージのもとで作成された予測モデルを、DALEX パッケージの他の関数に対応するように加工するための関数です。加工後の予測モデルは explainer オブジェクトと呼ばれます。\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(DALEX)\nexplainer &lt;- explain(model_rf,\n                     data = select(trees, -Volume),\n                     y = trees$Volume,\n                     quietly = TRUE,\n                     verbose = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DALEX</span>"
    ]
  },
  {
    "objectID": "articles/DALEX.html#個別の予測における特徴量と予測値の関係を解釈する",
    "href": "articles/DALEX.html#個別の予測における特徴量と予測値の関係を解釈する",
    "title": "DALEX",
    "section": "個別の予測における特徴量と予測値の関係を解釈する",
    "text": "個別の予測における特徴量と予測値の関係を解釈する\nexplainer オブジェクトに predict_profile() 関数を適用すると、ICE（Individual Conditional Expectation）プロットを作図することができます。ICE プロットは、注目している特徴量の値だけが違っていた場合に予測値がどのように変化するかを、個々の予測ごとに可視化するものです。\n\nice &lt;- explainer %&gt;% predict_profile(new_observation = trees)\nplot(ice)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DALEX</span>"
    ]
  },
  {
    "objectID": "articles/DALEX.html#モデルにおける特徴量と予測値の関係を解釈する",
    "href": "articles/DALEX.html#モデルにおける特徴量と予測値の関係を解釈する",
    "title": "DALEX",
    "section": "モデルにおける特徴量と予測値の関係を解釈する",
    "text": "モデルにおける特徴量と予測値の関係を解釈する\nexplainer オブジェクトにmodel_profile() 関数を適用すると、PD（Partial Dependence）プロットを作図することができます。PD プロットは、データ全体の ICE プロットを平均したものにほかならず、注目している特徴量の値が変化したときに予測値が平均的にどのように変化するかを表していると解釈できます。\n\npdp &lt;- explainer %&gt;% model_profile()\nplot(pdp, geom = 'profiles') + theme_gray()\n\n\n\n\n\n\n\n\nまた、model_profile() 関数や plot() 関数用のメソッドの引数 geom を調整することで、ALE（Accumulated Local Effects）プロットを作成したり、実際のデータ点を表示したりするほか、さまざまな変更を加えることが可能です。\n\nale &lt;- explainer %&gt;% model_profile(type = \"accumulated\")\nplot(ale, geom = 'points') + theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DALEX</span>"
    ]
  },
  {
    "objectID": "articles/DALEX.html#個別の予測における特徴量の寄与を解釈する",
    "href": "articles/DALEX.html#個別の予測における特徴量の寄与を解釈する",
    "title": "DALEX",
    "section": "個別の予測における特徴量の寄与を解釈する",
    "text": "個別の予測における特徴量の寄与を解釈する\nexplainer オブジェクトに predict_parts() 関数を適用すると、SHAP（SHapley Additive exPlanation）プロットを作図することができます。SHAP は、個別の予測値と平均的な予測値との差を、ゲーム理論的手法によって特徴量ごとの寄与に分解したものです。ここでは、5 番目のインスタンスに対する予測への特徴量ごとの寄与を表示してみます。\n\nshap &lt;- explainer %&gt;% predict_parts(trees[5,], type = 'shap')\nplot(shap) + theme_light() + theme(legend.position = 'null')",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DALEX</span>"
    ]
  },
  {
    "objectID": "articles/DALEX.html#shapをウォーターフォール図として描く",
    "href": "articles/DALEX.html#shapをウォーターフォール図として描く",
    "title": "DALEX",
    "section": "SHAPをウォーターフォール図として描く",
    "text": "SHAPをウォーターフォール図として描く\npredict_parts() 関数が出力したオブジェクト（predict_parts オブジェクト）を shapviz パ ッケージの shapviz() 関数で shapviz オブジェクトに変換することで、ウォーターフォール図を描くことも可能です。\n\nlibrary(shapviz)\nsv_waterfall(shapviz(shap)) + theme_light()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DALEX</span>"
    ]
  },
  {
    "objectID": "articles/DALEX.html#モデルにおける特徴量の重要度を解釈する",
    "href": "articles/DALEX.html#モデルにおける特徴量の重要度を解釈する",
    "title": "DALEX",
    "section": "モデルにおける特徴量の重要度を解釈する",
    "text": "モデルにおける特徴量の重要度を解釈する\nexplainer オブジェクトに model_parts() 関数を適用すると、PFI（Permutation Feature Importance）プロットを作図することができます。PFI は、「データの中で特定の特徴量だけをランダムに並び替えたときに、予測精度がどの程度低下するか」をその特徴量の重要度として解釈するものです。\n\npfi &lt;- explainer %&gt;% model_parts()\nplot(pfi) + theme_bw() + theme(legend.position = 'none')",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DALEX</span>"
    ]
  },
  {
    "objectID": "articles/DALEX.html#参考資料",
    "href": "articles/DALEX.html#参考資料",
    "title": "DALEX",
    "section": "参考資料",
    "text": "参考資料\nDALEX パッケージには、ここで紹介した手法以外にもさまざまな便利な関数が用意されています。以下のウェブ書籍には、それらの手法の説明だけでなく、R と Python の具体的なコード例も紹介されており、大変有用です。\nPrzemyslaw Biecek and Tomasz Burzykowski, Explanatory Model Analysis Explore, Explain, and Examine Predictive Models. With examples in R and Python. https://ema.drwhy.ai/",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DALEX</span>"
    ]
  },
  {
    "objectID": "articles/distr.html",
    "href": "articles/distr.html",
    "title": "distr",
    "section": "",
    "text": "パッケージの概要\ndistrは、RのS4クラスという機能の下で実装された、確率分布の柔軟な扱いを可能にするパッケージです。\n確率分布を扱うための機能として、R標準では特定の確率分布に対して「prefix」+「name」を名前とする関数が用意されています。prefixは以下の4種類で、例えば正規分布に従う乱数を取得するには、当該機能のprefixである「r」と正規分布を示す「norm」を組み合わせた名前の関数「rnorm」を使用します。\n# R標準の関数の例\nrnorm(5, mean = 0, sd = 1)\n\n[1] -0.5733372  0.2626607 -0.8457489  1.3673584 -0.8515542\n\npnorm(1.64, mean = 0, sd = 1)\n\n[1] 0.9494974\nこれに対してdistrでは、まずは対象の確率分布に従う確率変数を生成し、この確率変数を通じて確率分布を取り扱います。distrの確率変数を操作する関数として、例えば 表 24.1 のprefixに示した4種類のアルファベット1文字を関数名とする、R標準と同様の機能を持つ4種類の関数（r、d、p、q）が用意されています。\n以下のコードは確率変数を生成し、当該確率変数を引数としてp関数を呼び出すことで、累積分布関数の値を取得しています。\nlibrary(distr)\nX &lt;- Norm(mean = 0, sd = 1)  # 標準正規分布に従う確率変数の生成\np(X)(1.96)  # 生成した確率変数を引数としてp関数を呼び出す\n\n[1] 0.9750021\nこのようなシンプルな例では、distrの方式はR標準のものと比べて冗長に見えますが、本稿で紹介していくように、より複雑な確率分布を直感的な記述で取り扱うことができる特長があります。例えば、異なる確率分布に従う二つの確率変数X、Yを生成し、X+3Yに対してq関数を呼び出す、といった操作も可能です。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>distr</span>"
    ]
  },
  {
    "objectID": "articles/distr.html#パッケージの概要",
    "href": "articles/distr.html#パッケージの概要",
    "title": "distr",
    "section": "",
    "text": "表 9.1\n\n\n\n\n\nPrefix\nFunction\n\n\n\n\nr\n乱数の生成\n\n\nd\n確率密度関数\n\n\np\n累積分布関数\n\n\nq\n分位点関数\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nS4クラス\n\n\n\ndistrの実装のベースとなっている「S4クラス」は、Rにおけるオブジェクト指向プログラミングをサポートする機能の一つです。上の例では、確率変数XはNormクラスのオブジェクトということになります。また、本稿で紹介する多くの関数や演算子が、確率分布（クラス）の種類を判別して適切に動作するよう、S4クラスの総称関数／ディスパッチの機能を利用しています。S4クラスそのものの解説は本稿の範疇を超えますが、Rにおけるこれらのトピックの参考資料として、例えば[2]があります。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>distr</span>"
    ]
  },
  {
    "objectID": "articles/distr.html#基本的な使用例",
    "href": "articles/distr.html#基本的な使用例",
    "title": "distr",
    "section": "基本的な使用例",
    "text": "基本的な使用例\nここではdistrの基本的な使用例として、確率変数の簡単な操作を行います。\n\ndistroptions(\"WarningArith\" = FALSE)  # 使用例上は警告文を非表示にします\nset.seed(1234)\n\n\n確率変数の生成\n確率変数の生成は、確率分布ごとに用意された生成用の関数（コンストラクタ）により行います。 確率分布ごとに必要なパラメータが異なるので、適切なパラメータを渡します。\n以下はポアソン分布に従う確率変数N及び正規分布に従う確率変数Xを生成しています。\n\nN &lt;- Pois(lambda = 1)\nX &lt;- Norm(mean = 0, sd = 1)\n\n前述したように、NとXはそれぞれの確率分布に対応したS4クラスのオブジェクトです。 すなわち、生成した確率変数は、変数自身がクラス名などの情報や、内部変数（S4クラスでは「スロット」という）を保持しています。 str関数を使用すると、クラス名やスロットを表示することができます。 確率変数Xにstrを適用すると、生成時に指定したパラメータがparamというスロットに保持されていることが確認できます。\n\nstr(X)\n\nFormal class 'Norm' [package \"distr\"] with 12 slots\n  ..@ gaps       : NULL\n  ..@ img        :Formal class 'Reals' [package \"distr\"] with 2 slots\n  .. .. ..@ dimension: num 1\n  .. .. ..@ name     : chr \"Real Space\"\n  ..@ param      :Formal class 'UniNormParameter' [package \"distr\"] with 3 slots\n  .. .. ..@ mean: num 0\n  .. .. ..@ sd  : num 1\n  .. .. ..@ name: chr \"Parameter of a Normal distribution\"\n  ..@ r          :function (n)  \n  ..@ d          :function (x, log = FALSE)  \n  ..@ p          :function (q, lower.tail = TRUE, log.p = FALSE)  \n  ..@ q          :function (p, lower.tail = TRUE, log.p = FALSE)  \n  ..@ .withSim   : logi FALSE\n  ..@ .withArith : logi FALSE\n  ..@ .logExact  : logi TRUE\n  ..@ .lowerExact: logi TRUE\n  ..@ Symmetry   :Formal class 'SphericalSymmetry' [package \"distr\"] with 2 slots\n  .. .. ..@ type      : chr \"spherically symmetric distribution\"\n  .. .. ..@ SymmCenter: num 0\n\n\nS4クラスのスロットは@を使って直接参照できますが、R標準やdistrに必要なスロットを参照するための関数が用意されているため、基本的には@を使った直接の参照は行いません。\n例えばクラス名やパラメータの参照には、getClassやparam関数が使用できます。\n\ngetClass(X)\n\nDistribution Object of Class: Norm\n mean: 0\n sd: 1\n\nX@param  # (参考)直接参照\n\nAn object of class \"UniNormParameter\"\nSlot \"mean\":\n[1] 0\n\nSlot \"sd\":\n[1] 1\n\nSlot \"name\":\n[1] \"Parameter of a Normal distribution\"\n\n\n\n\n基本的な4関数\nポアソン分布に従う確率変数Nに対して、基本的な4つの関数（r、d、p、q）を適用して結果を確認してみましょう。\nこの4関数は、やや特殊に見えるかもしれませんが、[関数名] ([確率変数]) ([引数])の形式で呼び出します。 なお、RStudioではq()がRStudioを終了するコマンドになっているため、distrの分位点関数としてqと同機能のq.lが用意されています。\n\n# 乱数の生成\nr(N)(10)\n\n [1] 0 1 1 1 2 1 0 0 1 1\n\n# 確率密度関数\nd(N)(0:10)\n\n [1] 3.678794e-01 3.678794e-01 1.839397e-01 6.131324e-02 1.532831e-02\n [6] 3.065662e-03 5.109437e-04 7.299195e-05 9.123994e-06 1.013777e-06\n[11] 1.013777e-07\n\n# 分布関数\np(N)(3:5)\n\n[1] 0.9810118 0.9963402 0.9994058\n\n# 分位点関数\nq.l(N)(c(0, 0.2, 0.6, 0.8, 0.95, 0.99, 0.999, 0.9999, 0.99999, 1))\n\n [1]   0   0   1   2   3   4   5   6   8 Inf\n\n\n更にdistrで生成した確率変数は、plot関数で確率密度関数、分布関数、および分位点関数をプロットすることができます。 離散分布に従うNと連続分布に従うXとをplotすると、それぞれの分布の形式に応じたグラフが描写されます。\n\nplot(N)\n\n\n\n\n\n\n\nplot(X)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr、d、p、q関数の実装\n\n\n\nなぜr(N)(10)のような形式で呼び出すのかについて、実はこれらの関数自体の機能は、引数として渡された確率変数がスロットに保持している同名関数を返すことだからです。すなわち、r(N)で確率変数Nが保持しているr関数が返り、その関数に対して引数10を渡している、という構造です。\n以下のコードで、r関数に確率変数Nを渡した結果が関数であることがわかりますが、これがNが保持しているr関数です（N@rで同じ関数が参照できます）。 更には例えばポアソン分布の場合、Nが保持するr関数ですら、実はR標準のrpoisを使用していたということもわかります。\n\nr(N)\n\nfunction (n) \n{\n    rpois(n, lambda = 1)\n}\n&lt;environment: 0x000001a6020285b8&gt;\n\n\n\n\n\n\n確率変数の変換と演算\n\nスカラー値による演算\n確率変数に対するスカラー値の加減乗除は、適用する確率変数の分布に合わせた処理がなされます。\n例えば正規分布に従う確率変数Xに対する加算・乗算は解析的な演算がなされ、適切にパラメータが更新されていることを確認できます。\n\nX1 &lt;- 3 * X + 1\ngetClass(X1)\n\nDistribution Object of Class: Norm\n mean: 1\n sd: 3\n\n\n一方で、ポアソン分布に従うNに対する加算の結果は、（純粋な）ポアソン分布に従う確率変数ではなくなりますので、通常のポアソン分布としてのパラメータ表示はなされません。\nしかしこのような場合でも、distrには一般の離散分布に対するクラスや一般の連続分布に対するクラス、それらの組み合わせに対するクラスなど、演算を行う分布と演算内容に合わせて適切なクラスで処理がなされます。\n\nN1 &lt;- N + 2\ngetClass(N1)\n\nDistribution Object of Class: AffLinLatticeDistribution\n\n\n一般の分布のクラスに対しても同様にplot関数が適用できます。 上のコードで変換した確率変数N1についてplot関数でd、p、qの関数値を表示すると、ポアソン分布を平行移動した分布になっていることがグラフからも確認できます。\n\nplot(N1, cex.inner=0.85)\n\n\n\n\n\n\n\n\n\n\n畳み込み\ndistrでは、通常の変数の和を記述するのと同じように、演算子「+」で確率変数の和の演算（畳み込み）を行うことができます。distrは畳み込みを行う確率分布の組み合わせに従って内的な処理を使い分けており、特定の確率分布の和の分布は解析的な結果が返されます。\n次の例では、3つの指数分布に従う確率変数の和が、ガンマ分布に従う確率変数として返されています。\n\nG &lt;- Exp(rate = 1) + Exp(rate = 1) + Exp(rate = 1)\ngetClass(G)\n\nDistribution Object of Class: Gammad\n shape: 3\n scale: 1\n\n\n一般の和の分布は、両確率変数の分布の離散フーリエ変換で特性関数を算出してからその積を逆変換する、数値的な方法で算出されます。\n次の例は、正規分布と負の二項分布の和の分布に従う確率変数を生成しています。\n\nZ &lt;- Norm(mean = 0, sd = 2) + Nbinom(size = 1, prob = 0.5)\ngetClass(Z)\n\nDistribution Object of Class: AbscontDistribution\n\nplot(Z, cex.inner=0.85)\n\n\n\n\n\n\n\n\n\n\nその他の演算\n確率変数同士の積や商も行うことができます。これは対数変換した確率変数の畳み込みを行ったうえで指数を取るという方法で実装されています。\nまた、R標準のMath groupに属する各種関数（abs、round、exp、log、sin等々）やdistrが用意しているmax/minなど、確率変数に対して種々の演算を行うための関数を使用することができますので、詳細はマニュアル[1]をご参照ください。\n\n\n\n\n\n\n演算内における確率変数の独立性\n\n\n\n一つの演算中で同じ確率変数が複数回表れると、それらは独立な変数とみなされます。 すなわち、以下の二つの演算は異なる意味になりますので、注意が必要です。\n\nX &lt;- Norm()\nX1 &lt;- X + X  # 独立な確率変数の和の分布\nX2 &lt;- 2 * X  # 確率変数のスカラー倍",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>distr</span>"
    ]
  },
  {
    "objectID": "articles/distr.html#データを用いた使用例",
    "href": "articles/distr.html#データを用いた使用例",
    "title": "distr",
    "section": "データを用いた使用例",
    "text": "データを用いた使用例\nここからは、実際のデータを用いた例を通して、distrの更なる機能を紹介します。\n損害保険のデータ例として、デンマーク火災保険データを使用します。 このデータセットは確率分布や保険数理に関するいくつかのパッケージに含まれていますが、今回はfitdistrplus[4]というパッケージから読み込みます。\n\nlibrary(fitdistrplus)\ndata(\"danishuni\")\n\nデータを見ると火災の発生頻度は経時的に変化しており、特に11年間のうち前半年と後半年の差が大きいため、今回は1987年から1990年までのデータを使用します。\n\nfire &lt;- data.frame(\n  accident_ymd = danishuni$Date,\n  accident_year = as.numeric(format(danishuni$Date, \"%Y\")),\n  loss = danishuni$Loss\n  )\nfire &lt;- fire[fire$accident_year &gt;= 1987,]\nhead(fire)\n\n     accident_ymd accident_year     loss\n1279   1987-01-01          1987 1.023191\n1280   1987-01-01          1987 1.304267\n1281   1987-01-02          1987 1.066790\n1282   1987-01-05          1987 1.808905\n1283   1987-01-07          1987 3.246753\n1284   1987-01-08          1987 1.417440\n\nsummary(fire$loss)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.003   1.270   1.732   3.526   3.038 152.413 \n\n\n\n確率分布の作成\ndistrでは、正規分布やポアソン分布等のパッケージで既に用意されている確率分布だけではなく、自身で確率分布を作成することもできます。確率密度関数を数式で指定したり、取りうる値と発生確率のセットを指定する方法などがあります。\n次の例ではこの機能を利用して、支払金額の経験分布を作成しています。経験分布に対しては、データを指定するだけで簡単に分布が作成できる専用の関数が用意されています。\n\nX_emp &lt;- EmpiricalDistribution(fire$loss)\nplot(X_emp, cex.inner=0.85)\n\n\n\n\n\n\n\n\n経験分布に対しても、他の確率分布と同様に種々の関数を適用できます。以下ではr関数を利用して、ブートストラップによる平均値の分布を作成しています。\n\nN &lt;- 3000  # 1回あたりのサンプル数\nB &lt;- 5000  # ブートストラップ数\nres &lt;- numeric(B)\n\nfor (i in 1:B) {\n  res[i] &lt;- mean(r(X_emp)(N))\n}\n\nhist(res, main = \"Bootstrap of mean(X)\", xlab = \"X\")\n\n\n\n\n\n\n\nsummary(res)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.027   3.418   3.515   3.522   3.620   4.162 \n\n\nこの支払単価の分布に対するモデルのあてはめを考えます。\n経験分布をみると、極めて高額の支払が数件存在することがわかります。高額ロスは別途評価のもとローディングするという想定で、q関数を使用して取得した特定のパーセント点で高額ロスのキャッピングを行い、以降はこの高額ロスをキャッピングしたデータを使用することとします。\n\ncapping_line &lt;- q.l(X_emp)(0.9975)  # 99.75%点でキャッピングすることとする\nfire$loss_c &lt;- ifelse(fire$loss &gt; capping_line, capping_line, fire$loss)\nX_emp_c &lt;- EmpiricalDistribution(fire$loss_c)\n\n支払額の実績に対して、まずは指数分布を当てはめてみます。\n指数分布に従う確率変数を生成し、q関数を使ったQQプロット、d関数を使ったヒストグラムで、指数分布モデルと実績データとを比較しました。\n\nX_exp &lt;- Exp(rate = 1/mean(fire$loss_c))\n\n# QQ-plot\nn &lt;- length(fire$loss_c)\nx &lt;- q.l(X_emp_c)((1:n)/(n+1))\ny &lt;- q.l(X_exp)((1:n)/(n+1))\nplot(x, y, xlab = \"emprical value\", ylab = \"theoretical value\")\npar(new = TRUE)\nabline(0, 1)\n\n\n\n\n\n\n\n# histogram\npar(mfrow = c(1, 2))\nhist(fire$loss_c, prob=TRUE, \n     main = \"Histogram of Danish fire loss\\n(1987-1990)\", cex.main = 0.8,\n     xlab = \"loss\")\ncurve(d(X_exp)(x), add=TRUE, col=\"red\")\n# tail part\nhist(fire$loss_c, prob=TRUE, xlim = c(10, 50), ylim=c(0, 0.01),\n     main = \"enlarged view on tail part\",  cex.main = 0.8, xlab = \"loss\")\ncurve(d(X_exp)(x), add=TRUE, col=\"red\")\n\n\n\n\n\n\n\n\n実績データはより裾が重く、指数分布によるモデルはうまく当てはまっていないと思われます。distrでは対数正規分布、ワイブル分布などの裾の重い分布が用意されています。現時点では、例えばパレート分布は用意されていないので、確率密度関数を指定して分布を作成してみることにします。\ndistrで用意されている確率分布よりも更に数値的な誤差に注意が必要ですが、以下では参考として、適当なパラメータでパレート分布を作成したうえで、指数分布によるモデルと比較しています。\n\np_shape &lt;- 5.175117\np_scale &lt;- 13.419728\nX_par &lt;- AbscontDistribution(\n  d = function(x) p_shape * (p_scale^p_shape) / (x+p_scale)^(p_shape+1), \n  low1 = 0, withStand = TRUE)\n\n# QQ-plot\ny2 &lt;- q.l(X_par)((1:n)/(n+1))\nmatplot(x, cbind(y, y2), pch = 1:2, col = 1:2,\n        ylim = c(0, 40),\n        xlab = \"emprical value\", ylab = \"theoretical value\")\nlegend(2, 38, c(\"exp\", \"par\"), pch = 1:2, col = 1:2)\npar(new = TRUE)\nabline(0, 1)\n\n\n\n\n\n\n\n\n\n\n複合分布\n最後に、総支払額分布の作成を通して、distrの複合分布の機能を紹介します。\n1日当たりの事故発生件数がポアソン分布に従うものと仮定し、指数分布による支払額のモデルとの、複合ポアソン分布を作成します。複合分布はCompoundDistributionクラスを使用します。\n\nN_pois &lt;- Pois(lambda = \n               nrow(fire)/as.numeric(as.Date(\"1991-1-1\")-as.Date(\"1987-1-1\")))\nS &lt;- CompoundDistribution(NumbOfSummandsDistr = N_pois, SummandsDistr = X_exp)\n\n複合分布に対してはr、p、q関数が使用できます。 また複合分布の機能として、もととなっている分布（今回の場合ポアソン分布と指数分布）を取り出すことも可能です。\nこれらの機能を使用して、複合分布Sの期待値について(1)累積分布関数の数値積分、(2)乱数シミュレーション、および(3)定義に従った複合ポアソン分布の期待値\\lambda E[X]の三つを比較してみました。\n\n# (1)数値積分\nE1 &lt;- integrate(function(x) 1-p(S)(x), 0, Inf)$value\n\n# (2)乱数シミュレーション\nE2 &lt;- mean(r(S)(20000))\n\n# (3)複合ポアソン分布の期待値（lambda * E[X]）\nE3 &lt;- lambda(NumbOfSummandsDistr(S)) * scale(SummandsDistr(S))\n\nsprintf(\"(1):%.6f (2):%.6f (3):%.6f\", E1, E2, E3)\n\n[1] \"(1):2.006595 (2):1.995217 (3):2.006593\"",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>distr</span>"
    ]
  },
  {
    "objectID": "articles/distr.html#参考資料",
    "href": "articles/distr.html#参考資料",
    "title": "distr",
    "section": "参考資料",
    "text": "参考資料\n[1] Peter Ruckdeschel, Matthias Kohl, Thomas Stabla & Florian Camphausen. S4 Classes for Distributions—a manual for packages.\n[2] Hadley Wickham. Advanced R.\n[3] Arthur Charpentier編 (2014). Computational Actuarial Science with R, CRC Press.\n[4] Marie Laure Delignette-Muller, Christophe Dutang (2015). fitdistrplus: An R Package for Fitting Distributions. Journal of Statistical Software.. DOI 10.18637/jss.v064.i04.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>distr</span>"
    ]
  },
  {
    "objectID": "articles/distrEx.html",
    "href": "articles/distrEx.html",
    "title": "distrEx",
    "section": "",
    "text": "パッケージの概要\ndistrExは、確率変数をS4クラスで扱うことのできるパッケージdistrの機能を拡張するパッケージです。 distrで生成した確率変数に対して期待値・分散を計算する関数、多変量確率分布のクラス、確率変数間の距離を計算する関数などを提供します。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>distrEx</span>"
    ]
  },
  {
    "objectID": "articles/distrEx.html#パッケージの概要",
    "href": "articles/distrEx.html#パッケージの概要",
    "title": "distrEx",
    "section": "",
    "text": "distrパッケージの使用方法\n\n\n\ndistrExはdistrの拡張機能であるため、distrの使用が前提となっています。 本稿ではdistrExの機能に絞って解説を行いますので、distrの基本的な知識や使用方法はdistrのコード使用例をご参照ください。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>distrEx</span>"
    ]
  },
  {
    "objectID": "articles/distrEx.html#パッケージの使用例",
    "href": "articles/distrEx.html#パッケージの使用例",
    "title": "distrEx",
    "section": "パッケージの使用例",
    "text": "パッケージの使用例\ndistrExの主な機能の使用例を紹介します。 一部の数値例にirisデータセットを用います。\n\nlibrary(distr)\nlibrary(distrEx)\n\ndata(iris)\nspecies_num &lt;- as.numeric(iris$Species)\nplot(x=iris$Sepal.Width, y=iris$Sepal.Length, \n     pch = species_num, col = species_num,\n     xlab = \"Sepal Width\", ylab = \"Sepal Length\")\nlegend(\"topright\", levels(iris$Species), pch = 1:3, col = 1:3)\n\n\n\n\n\n\n\n\n\n期待値と分散\ndistrで生成した確率変数を引数にとり、当該確率変数の期待値を計算するE関数、および分散を計算するvar関数が用意されています。 以下にいくつかの例を示します。\n\n正規分布\n\nX &lt;- Norm(mean = 3, sd = 2)\ncat(sprintf(\"期待値:%.2f 分散:%.2f\", E(X), var(X)))\n\n期待値:3.00 分散:4.00\n\n\n\n\nポアソン分布\n\nN &lt;- Pois(lambda = 4)\ncat(sprintf(\"期待値:%.2f 分散:%.2f\", E(N), var(N)))\n\n期待値:4.00 分散:4.00\n\n\n\n\n正規変数を変換した確率変数\n\nX_affine &lt;- X * 2 + 5\ncat(sprintf(\"期待値:%.2f 分散:%.2f\", E(X_affine), var(X_affine)))\n\n期待値:11.00 分散:16.00\n\n\n\n\n経験分布\n\nd_setosa &lt;- iris[iris$Species==\"setosa\",]\nn &lt;- nrow(d_setosa)\n\nX_emp &lt;- EmpiricalDistribution(data = d_setosa$Sepal.Length)\ncat(sprintf(\"期待値:%.5f 分散:%.5f\", E(X_emp), var(X_emp)))\n\n期待値:5.00600 分散:0.12176\n\ncat(sprintf(\"(経験分布との比較用)実データ  平均:%.5f 分散:%.5f\", \n            mean(d_setosa$Sepal.Length), (n-1)/n*stats::var(d_setosa$Sepal.Length)))\n\n(経験分布との比較用)実データ  平均:5.00600 分散:0.12176\n\n\n\n\n\n\n\n\nE関数およびvar関数の誤差\n\n\n\ndistrExのE関数とvar関数は、AbsContDistributionなどの一般の分布のクラスに対しては、内部的には数値積分で処理されています。 計算する確率変数の分布によっては数値的な誤差が無視できない大きさになる場合があるため、別の方法による計算結果との比較など、検証を行いながら使用することをおすすめします。\n\n# 例. 指数分布に従うXの二乗の期待値\n#X &lt;- Norm(mean = 0, sd = 1)\nX &lt;- Exp(rate = 1)\nX2 &lt;- X^2\n\n## 計算方法(1) distrExのE関数を直接適用\nres_1 &lt;- E(X2)\n\n## 計算方法(2) 分散 + 期待値の二乗で計算\nres_2 &lt;- var(X) + E(X)^2\n\ncat(sprintf(\"(1):%.5f (2):%.5f\", res_1, res_2))\n\n(1):1.79658 (2):2.00000\n\n\n\n\n\n\n\n多変量分布\ndistrExには多変量分布を表現するためのクラスが用意されています。 これらの分布についても、例えばE関数を適用して、変数ごとの期待値を計算することも可能です。 以下ではdistrExに用意されている多変量分布の例として、経験多変量分布と条件付分布のクラスを紹介します。\n\n経験多変量分布\n多変量のデータを引数として与えることで、経験多変量分布を生成できます。 以下の例では、経験多変量分布を生成し、生成した多変量分布に対してE関数で期待値を算出しています。\n\nX &lt;- EmpiricalMVDistribution(\n  data = as.matrix(iris[iris$Species == \"setosa\", c(\"Sepal.Length\", \"Sepal.Width\")]))\n\nWarning in DiscreteMVDistribution(supp = data, Symmetry = Symmetry): collapsing\nto unique support values\n\nE(X)\n\nSepal.Length  Sepal.Width \n       5.006        3.428 \n\n\n\n\n条件付分布\ndistrExに用意されている条件付分布の一つが線形モデルです。 次の例では、比較用にR標準の線形モデルを構築したうえで、distrExのLMCondDistributionクラスを使用して、 Sepal.LengthをSepal.Widthで回帰した線形モデルに従う確率変数Xを生成しています。\n\n# R標準の線形モデル\nd_setosa &lt;- iris[iris$Species == \"setosa\",]\nlm.model &lt;- lm(Sepal.Length ~ Sepal.Width, data = d_setosa)\nplot(d_setosa$Sepal.Width, d_setosa$Sepal.Length, xlab=\"Sepal Width\", ylab=\"Sepal Length\")\nabline(lm.model)\n\n\n\n\n\n\n\n# distrExの確率変数として生成\nX &lt;- LMCondDistribution(Error = Norm(sd = var(lm.model$residuals)), \n                   theta = lm.model$coefficients[2],\n                   intercept = lm.model$coefficients[1])\n\n条件付分布に対しては、E関数の条件付期待値の機能が使用できます。 すなわちこの例の場合、線形モデルにおいて特定のSepal.Widthの値の下での、Sepal.lengthの期待値を計算します。 distrExのE関数で計算した条件付期待値は、R標準のpredict関数を線形モデルに適用した予測値とほぼ一致していることが確認できます。\n\n# distrExのE関数による条件付期待値と、R標準による線形モデルのpredict\ncat(sprintf(\"distrEx : %.5f\\nR標準 : %.5f\",\n            E(X, cond = 3.5), predict(lm.model, list(Sepal.Width = 3.5))))\n\ndistrEx : 5.05571\nR標準 : 5.05572\n\n\n\n\n\n確率分布間の距離\ndistrExには確率変数が従う確率分布間の距離を計算する関数が用意されています。 以下はKolmogorovDist関数を使用した、Kolmogorov距離の計算例です。\n(二つの分布関数FとGに対して、d(F,G) = sup|F(x)-G(x)|)\nirisのSepal.Widthについて、Speceies間の経験分布の距離を計算しています。 グラフ上でも度数分布の重なりが大きいversicolorとvirginica間の経験分布の距離が、setosaとversicolor間の距離より小さい結果となっています。\n\nX_setosa &lt;- EmpiricalDistribution(\n  data = as.matrix(iris[iris$Species == \"setosa\", \"Sepal.Width\"]))\nX_versicolor &lt;- EmpiricalDistribution(\n  data = as.matrix(iris[iris$Species == \"versicolor\", \"Sepal.Width\"]))\nX_virginica &lt;- EmpiricalDistribution(\n  data = as.matrix(iris[iris$Species == \"virginica\", \"Sepal.Width\"]))\n\ncols &lt;- c(\"#FF00007F\", \"#0000FF7F\")\n\n# setosa vs versicolor\nhist(iris[iris$Species == \"setosa\", \"Sepal.Width\"], \n     breaks=seq(1, 5, 0.2), col=cols[1], main=\"\", xlab=\"Sepal Width\")\nhist(iris[iris$Species == \"versicolor\", \"Sepal.Width\"],\n     breaks=seq(1, 5, 0.2), col=cols[2], add=TRUE, main=\"\", xlab=\"\")\nlegend(\"topright\",legend=c(\"setosa\", \"versicolor\"), fill=cols, )\n\n\n\n\n\n\n\n# Kolmogorov距離\nKolmogorovDist(X_setosa, X_versicolor)\n\nKolmogorov distance \n               0.68 \n\n\n\n# versicolor vs virginica\nhist(iris[iris$Species == \"versicolor\", \"Sepal.Width\"], \n     breaks=seq(1, 5, 0.2), col=cols[1], main=\"\", xlab=\"Sepal Width\")\nhist(iris[iris$Species == \"virginica\", \"Sepal.Width\"],\n     breaks=seq(1, 5, 0.2), col=cols[2], add=TRUE, main=\"\", xlab=\"\")\nlegend(\"topright\",legend=c(\"versicolor\", \"virginica\"), fill=cols, )\n\n\n\n\n\n\n\n# Kolmogorov距離\nKolmogorovDist(X_versicolor, X_virginica)\n\nKolmogorov distance \n               0.26 \n\n\nなお、紹介したKolmogorov距離以外にも様々な距離関数が用意されているため、詳細はマニュアル[1]をご参照ください。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>distrEx</span>"
    ]
  },
  {
    "objectID": "articles/distrEx.html#参考資料",
    "href": "articles/distrEx.html#参考資料",
    "title": "distrEx",
    "section": "参考資料",
    "text": "参考資料\n[1] Peter Ruckdeschel, Matthias Kohl, Thomas Stabla & Florian Camphausen. S4 Classes for Distributions—a manual for packages.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>distrEx</span>"
    ]
  },
  {
    "objectID": "articles/dplyr.html",
    "href": "articles/dplyr.html",
    "title": "dplyr",
    "section": "",
    "text": "パッケージの概要\ndplyrパッケージは、データフレームに対する「特定の行・列の抽出」や「ソート」などの重要な操作を、とてもシンプルなコードで実現するための関数を備えています。規模の大きなデータを扱うことの多いアクチュアリーにとって、必要不可欠なパッケージの一つです。\nlibrary(dplyr)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>dplyr</span>"
    ]
  },
  {
    "objectID": "articles/dplyr.html#パイプ演算子でコードを簡略化する",
    "href": "articles/dplyr.html#パイプ演算子でコードを簡略化する",
    "title": "dplyr",
    "section": "パイプ演算子でコードを簡略化する",
    "text": "パイプ演算子でコードを簡略化する\ndplyrパッケージを呼び出すと、magrittrパッケージが提供するパイプ演算子（%&gt;%）も使えるようになります。この演算子は、左側にあるオブジェクトを、右側の関数の未指定の引数のうち一番最初のものに代入した結果を返します。\n\n# irisデータの先頭3行を表示\niris  %&gt;% slice_head(n = 3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\n\nなお、バージョン 4.1.0 以降の R には、標準演算子の一つとしてパイプ演算子 “|&gt;” が追加されています。\n\n# バージョン 4.1.0 以降でのみ利用可能\niris |&gt; slice_tail(n = 3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n1          6.5         3.0          5.2         2.0 virginica\n2          6.2         3.4          5.4         2.3 virginica\n3          5.9         3.0          5.1         1.8 virginica\n\n\nデータフレームに対して多くの操作を重ねたいとき、パイプ演算子を活用すれば、多重括弧による入れ子構造を避けて可読性の高いコードを書くことができます。\n\niris %&gt;%\n  select(Species, Sepal.Length, Sepal.Width) %&gt;%\n  mutate(Sepal.Ratio　=　round(Sepal.Length / Sepal.Width, 3)) %&gt;%\n  group_by(Species) %&gt;%\n  arrange(Sepal.Ratio) %&gt;%\n  slice_head()\n\n# A tibble: 3 × 4\n# Groups:   Species [3]\n  Species    Sepal.Length Sepal.Width Sepal.Ratio\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa              5.2         4.1        1.27\n2 versicolor          6           3.4        1.76\n3 virginica           6.2         3.4        1.82",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>dplyr</span>"
    ]
  },
  {
    "objectID": "articles/dplyr.html#データフレームから指定した列を抽出する",
    "href": "articles/dplyr.html#データフレームから指定した列を抽出する",
    "title": "dplyr",
    "section": "データフレームから指定した列を抽出する",
    "text": "データフレームから指定した列を抽出する\n\niris %&gt;%\n  select(Species, Sepal.Width, Petal.Width) %&gt;%\n  slice_tail(n=3)\n\n    Species Sepal.Width Petal.Width\n1 virginica         3.0         2.0\n2 virginica         3.4         2.3\n3 virginica         3.0         1.8",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>dplyr</span>"
    ]
  },
  {
    "objectID": "articles/dplyr.html#データフレームから条件を満たす行を抽出する",
    "href": "articles/dplyr.html#データフレームから条件を満たす行を抽出する",
    "title": "dplyr",
    "section": "データフレームから条件を満たす行を抽出する",
    "text": "データフレームから条件を満たす行を抽出する\n\niris %&gt;%\n  filter(Sepal.Width == 2.8, Petal.Width == 1.3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1          5.7         2.8          4.5         1.3 versicolor\n2          6.1         2.8          4.0         1.3 versicolor\n3          5.7         2.8          4.1         1.3 versicolor",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>dplyr</span>"
    ]
  },
  {
    "objectID": "articles/dplyr.html#データフレームの行をソートする",
    "href": "articles/dplyr.html#データフレームの行をソートする",
    "title": "dplyr",
    "section": "データフレームの行をソートする",
    "text": "データフレームの行をソートする\n\niris %&gt;%\n  arrange(Sepal.Length, Sepal.Width) %&gt;% slice_head(n=3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          4.3         3.0          1.1         0.1  setosa\n2          4.4         2.9          1.4         0.2  setosa\n3          4.4         3.0          1.3         0.2  setosa",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>dplyr</span>"
    ]
  },
  {
    "objectID": "articles/dplyr.html#データフレームの列を作成する",
    "href": "articles/dplyr.html#データフレームの列を作成する",
    "title": "dplyr",
    "section": "データフレームの列を作成する",
    "text": "データフレームの列を作成する\n\niris %&gt;%\n  mutate(Sepal.Ratio=round(Sepal.Length / Sepal.Width, 3)) %&gt;%\n  slice_head(n=3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.Ratio\n1          5.1         3.5          1.4         0.2  setosa       1.457\n2          4.9         3.0          1.4         0.2  setosa       1.633\n3          4.7         3.2          1.3         0.2  setosa       1.469",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>dplyr</span>"
    ]
  },
  {
    "objectID": "articles/dplyr.html#データを要約する",
    "href": "articles/dplyr.html#データを要約する",
    "title": "dplyr",
    "section": "データを要約する",
    "text": "データを要約する\n\niris %&gt;% summarize(n_distinct(Sepal.Length),\n                   mean(Sepal.Length),\n                   max(Sepal.Length),\n                   sum(log(Sepal.Width)))\n\n  n_distinct(Sepal.Length) mean(Sepal.Length) max(Sepal.Length)\n1                       35           5.843333               7.9\n  sum(log(Sepal.Width))\n1              166.1159",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>dplyr</span>"
    ]
  },
  {
    "objectID": "articles/dplyr.html#データフレームをグループ化して集計する",
    "href": "articles/dplyr.html#データフレームをグループ化して集計する",
    "title": "dplyr",
    "section": "データフレームをグループ化して集計する",
    "text": "データフレームをグループ化して集計する\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarize(n(), mean(Sepal.Length), sd(Sepal.Length))\n\n# A tibble: 3 × 4\n  Species    `n()` `mean(Sepal.Length)` `sd(Sepal.Length)`\n  &lt;fct&gt;      &lt;int&gt;                &lt;dbl&gt;              &lt;dbl&gt;\n1 setosa        50                 5.01              0.352\n2 versicolor    50                 5.94              0.516\n3 virginica     50                 6.59              0.636",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>dplyr</span>"
    ]
  },
  {
    "objectID": "articles/e1071.html",
    "href": "articles/e1071.html",
    "title": "e1071",
    "section": "",
    "text": "パッケージの概要\nRのパッケージe1071は、サポートベクトルマシン（Support Vector Machines）やカーネル法に関連する機械学習アルゴリズムやツールを提供するパッケージです。サポートベクトルマシンを実装するためのsvm()関数は、サポートベクトルマシンに特化したC++ライブラリであるlibsvmの機能を利用しています。\n本コード活用例では、iris（アイリスの花の種類ごとに測定された花弁などの長さ・幅のデータ）を用いて、サポートベクトルマシンを実装します。\nlibrary(e1071)\ndata(iris)\nattach(iris)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>e1071</span>"
    ]
  },
  {
    "objectID": "articles/e1071.html#サポートベクトルマシンの実装例",
    "href": "articles/e1071.html#サポートベクトルマシンの実装例",
    "title": "e1071",
    "section": "サポートベクトルマシンの実装例",
    "text": "サポートベクトルマシンの実装例\ne1071パッケージでサポートベクトルマシンを実装する際は、svm関数を利用します。\n以下のコードで、iris（アヤメの花ごとの種類、花弁などの長さ・幅のデータ）のSpecies（種類）を目的変数、Species以外を説明変数として、サポートベクトルマシンを作成し、print(model)で、モデルのParametersなどを出力します。\n\nx &lt;- subset(iris, select = -Species)\ny &lt;- Species\nmodel &lt;- svm(x, y)\nprint(model)\n\n\nCall:\nsvm.default(x = x, y = y)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  51\n\n\nprint(model)の出力結果のうち、Parametersに記載されている「SVM-Type」は、サポートベクトルマシンの種類を示しており、「SVM-Kernel」はカーネル関数の種類を示しています。\nsvm関数で利用可能なサポートベクトルマシンの種類およびカーネル関数の種類は以下の通りです。詳細については、リンク先を参照してください。\nなお、e1071パッケージで利用可能なカーネル関数は下表の4種類のみです。kernlabパッケージでは、カーネル関数を自作するなど、e1071パッケージよりも柔軟にカーネル関数を設定可能です。\nhttps://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf\n＜サポートベクトルマシンの種類＞\n\n\n\n区分\n種類\nパラメータ\n\n\n\n\n分類\nC-classification\ncost\n\n\n分類\nnu-classification\nnu\n\n\n分類\none-classification\nnu\n\n\n回帰\neps-regression\ncost, epsilon\n\n\n回帰\nnu-regression\ncost, nu\n\n\n\n＜カーネル関数の種類＞\n\n\n\n種類\nパラメータ\n\n\n\n\nlinear\nなし\n\n\npolynomial\ngamma, degree, coef0\n\n\nradial\ngamma\n\n\nsigmoid\ncoef0\n\n\n\n\nテストデータと訓練データの分割\n本項では、irisをテストデータと訓練データに分割し、訓練データを用いて、先ほどと同じようにサポートベクトルマシンを作成します。\nprint(model)の出力結果の通り、サポートベクトルマシンの種類は”C-classification”,カーネル関数は”radial”です。\n\nindex&lt;-1:nrow(iris)\nN&lt;-trunc(length(index)/3)\ntestindex&lt;-sample(index, N)\ntestset&lt;-iris[testindex,]\ntrainset&lt;-iris[-testindex,]\nx &lt;- subset(trainset, select = -Species)\ny &lt;- trainset$Species\nmodel &lt;- svm(x, y)\nprint(model)\n\n\nCall:\nsvm.default(x = x, y = y)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  40\n\n\n\n結果の確認\nテストデータを対象に、モデルによって予測した結果と正解データを比較すると、ほとんどのデータで品種を正しく予測していることが確認できます。\n\nsvm.pred &lt;- predict(model, testset[, -5])\ntable(pred = svm.pred, true = testset[, 5] )\n\n            true\npred         setosa versicolor virginica\n  setosa         20          0         0\n  versicolor      0         10         2\n  virginica       0          2        16\n\n\n\n\nパラメータのチューニング\n最後に、パラメータのチューニングに利用するtune関数を紹介します。先ほど作成したmodelでチューニング可能なパラメータは”cost”(サポートベクトルマシン”C-classification”のパラメータ)と”gamma”（カーネル関数”radial”のパラメータ）の2つです。\n以下のコードでは、rangesで指定したパラメータの組み合わせを対象にモデルのチューニングを行います。\n\nsvm.tune &lt;- tune(svm, Species~., data = iris,\n            ranges = list(gamma = 2^(-6:2), cost = 2^(2:10)),\n            tunecontrol = tune.control(sampling = \"fix\")\n)\n\nsummary関数を利用することで、最も良いパフォーマンスとなったパラメータの組み合わせや、パラメータの各組み合わせのパフォーマンスを確認することができます。\n\nsummary(svm.tune)\n\n\nParameter tuning of 'svm':\n\n- sampling method: fixed training/validation set \n\n- best parameters:\n gamma cost\n     1    4\n\n- best performance: 0.02 \n\n- Detailed performance results:\n      gamma cost error dispersion\n1  0.015625    4  0.04         NA\n2  0.031250    4  0.06         NA\n3  0.062500    4  0.06         NA\n4  0.125000    4  0.04         NA\n5  0.250000    4  0.06         NA\n6  0.500000    4  0.04         NA\n7  1.000000    4  0.02         NA\n8  2.000000    4  0.06         NA\n9  4.000000    4  0.08         NA\n10 0.015625    8  0.06         NA\n11 0.031250    8  0.06         NA\n12 0.062500    8  0.06         NA\n13 0.125000    8  0.06         NA\n14 0.250000    8  0.06         NA\n15 0.500000    8  0.04         NA\n16 1.000000    8  0.02         NA\n17 2.000000    8  0.06         NA\n18 4.000000    8  0.08         NA\n19 0.015625   16  0.06         NA\n20 0.031250   16  0.04         NA\n21 0.062500   16  0.06         NA\n22 0.125000   16  0.04         NA\n23 0.250000   16  0.04         NA\n24 0.500000   16  0.06         NA\n25 1.000000   16  0.04         NA\n26 2.000000   16  0.06         NA\n27 4.000000   16  0.08         NA\n28 0.015625   32  0.06         NA\n29 0.031250   32  0.06         NA\n30 0.062500   32  0.04         NA\n31 0.125000   32  0.04         NA\n32 0.250000   32  0.06         NA\n33 0.500000   32  0.04         NA\n34 1.000000   32  0.04         NA\n35 2.000000   32  0.06         NA\n36 4.000000   32  0.08         NA\n37 0.015625   64  0.06         NA\n38 0.031250   64  0.04         NA\n39 0.062500   64  0.04         NA\n40 0.125000   64  0.04         NA\n41 0.250000   64  0.06         NA\n42 0.500000   64  0.04         NA\n43 1.000000   64  0.04         NA\n44 2.000000   64  0.06         NA\n45 4.000000   64  0.08         NA\n46 0.015625  128  0.04         NA\n47 0.031250  128  0.04         NA\n48 0.062500  128  0.04         NA\n49 0.125000  128  0.06         NA\n50 0.250000  128  0.06         NA\n51 0.500000  128  0.04         NA\n52 1.000000  128  0.04         NA\n53 2.000000  128  0.06         NA\n54 4.000000  128  0.08         NA\n55 0.015625  256  0.04         NA\n56 0.031250  256  0.04         NA\n57 0.062500  256  0.04         NA\n58 0.125000  256  0.06         NA\n59 0.250000  256  0.06         NA\n60 0.500000  256  0.04         NA\n61 1.000000  256  0.04         NA\n62 2.000000  256  0.06         NA\n63 4.000000  256  0.08         NA\n64 0.015625  512  0.02         NA\n65 0.031250  512  0.04         NA\n66 0.062500  512  0.04         NA\n67 0.125000  512  0.06         NA\n68 0.250000  512  0.06         NA\n69 0.500000  512  0.04         NA\n70 1.000000  512  0.04         NA\n71 2.000000  512  0.06         NA\n72 4.000000  512  0.08         NA\n73 0.015625 1024  0.04         NA\n74 0.031250 1024  0.04         NA\n75 0.062500 1024  0.04         NA\n76 0.125000 1024  0.06         NA\n77 0.250000 1024  0.06         NA\n78 0.500000 1024  0.04         NA\n79 1.000000 1024  0.04         NA\n80 2.000000 1024  0.06         NA\n81 4.000000 1024  0.08         NA",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>e1071</span>"
    ]
  },
  {
    "objectID": "articles/evd.html",
    "href": "articles/evd.html",
    "title": "evd",
    "section": "",
    "text": "パッケージの概要\nevdは、極値理論に基づく統計解析を行うためのパッケージです。 シミュレーション、分布、分位点、密度関数をパラメトリック極値分布に拡張し、以下を計算するフィッティング関数を提供します。\n極値理論は、分布から大きく外れるような極端な事象をモデル化するための手法であり、保険分野においても、再保険や巨大自然災害リスクの分析、リスク管理でのテイル評価等に利用されています。\nrequire(evd)\nrequire(knitr) #「従属パラメータと従属関数の関係」の表作成に使用",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>evd</span>"
    ]
  },
  {
    "objectID": "articles/evd.html#パッケージの概要",
    "href": "articles/evd.html#パッケージの概要",
    "title": "evd",
    "section": "",
    "text": "一変量および二変量の最大値モデルの最尤推定\n一変量および二変量の閾値モデルの最尤推定",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>evd</span>"
    ]
  },
  {
    "objectID": "articles/evd.html#一変量極値分布のフィッティング",
    "href": "articles/evd.html#一変量極値分布のフィッティング",
    "title": "evd",
    "section": "一変量極値分布のフィッティング",
    "text": "一変量極値分布のフィッティング\nまずは、一変量のデータを用いて極値分布のフィッティングを行います。rgev関数により一般化極値分布（GEV）に従う乱数を100個生成し、fgev関数を用いて生成したデータに最尤推定（MLE）でフィットさせます。\nなお、rgev関数の引数で次のとおりGEVのパラメータを指定しています。\n\nloc = 0 ：位置パラメータ \\mu\nscale = 1 ：尺度パラメータ \\sigma\nshape = 0.2：形状パラメータ \\xi\n\n\n# サンプルデータ生成 (GEV)\nset.seed(123)\nuv_data &lt;- rgev(100, loc = 0, scale = 1, shape = 0.2)\n\n# GEVモデルをデータにフィットさせる\nfit_gev &lt;- fgev(uv_data)\n\nprint関数により、逸脱度（Deviance）やパラメータの推定値等のフィッティング結果を確認してみましょう。\n\n# フィット結果の表示\nprint(fit_gev)\n\n\nCall: fgev(x = uv_data) \nDeviance: 323.7182 \n\nEstimates\n     loc     scale     shape  \n-0.06327   0.93540   0.18417  \n\nStandard Errors\n    loc    scale    shape  \n0.10331  0.08055  0.06814  \n\nOptimization Information\n  Convergence: successful \n  Function Evaluations: 44 \n  Gradient Evaluations: 13 \n\n# フィット結果のプロット\nplot(fit_gev)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>evd</span>"
    ]
  },
  {
    "objectID": "articles/evd.html#クラスタリング",
    "href": "articles/evd.html#クラスタリング",
    "title": "evd",
    "section": "クラスタリング",
    "text": "クラスタリング\nclusters関数により、閾値を超える極値のクラスタリングを行うことができます。\n下記のコードの場合は、次の手順でクラスタリングを行っています。\n\n90％分位点を閾値とし、閾値を超過する値を検出\n閾値超過が現れると、クラスタを開始する\nクラスタは、5個の連続した値が閾値を下回る（または閾値と等しくなる）まではアクティブのままである\n5個の連続した値が閾値を下回る（または閾値と等しくなる）とクラスタ終了となる\n次に閾値超過が現れると、また新たなクラスタを開始する\n上記を繰り返す\n\n\n# 閾値を超える値をクラスタリング\nclusters &lt;- clusters(uv_data, u = quantile(uv_data, 0.90), r = 5, plot = TRUE)\n\n\n\n\n\n\n\nprint(clusters)\n\n$cluster1\n         4          5          6          7          8          9         10 \n 4.9791797  3.8921295  1.2935390  1.3026215  2.3542219 -0.9087379  5.1398615 \n\n$cluster2\n     29 \n4.96719 \n\n$cluster3\n      51 \n3.082629 \n\n$cluster4\n      64       65       66 \n4.421884 2.946345 2.947338 \n\n$cluster5\n      78 \n4.413705 \n\n$cluster6\n        91         92         93         94         95         96 \n 9.6701628 -0.1021885  1.3614241 -0.1725861 -0.1076155  3.5756905 \n\nattr(,\"acs\")\n[1] 1.666667\n\n\n\n\n\n\n\n\nなぜクラスタリングを行うか\n\n\n\n時系列データでは、閾値を超える極端な値が連続して現れることが珍しくありません。例えば、一度の台風において高波が何度も起こったり、大地震の余震が連続して発生することはよくあるかと思います。このとき、こうした時系列データをそのまま統計モデルに当てはめると、独立同分布の仮定が成立しなくなってしまいます。そこで、極値理論では、これらの連続した極値を一つのクラスタとしてまとめる方法が使われているのです。",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>evd</span>"
    ]
  },
  {
    "objectID": "articles/evd.html#二変量極値分布のフィッティング",
    "href": "articles/evd.html#二変量極値分布のフィッティング",
    "title": "evd",
    "section": "二変量極値分布のフィッティング",
    "text": "二変量極値分布のフィッティング\n次に、二変量のデータを用いて同様に極値分布のフィッティングを行います。一変量と同じように、print関数によってフィッティング結果の確認も可能です。\n\n# サンプルデータ生成 (二変量ロジスティックモデル)\nset.seed(123)\nbv_data &lt;- rbvevd(100, dep = 0.7, model = \"log\")\n\n# ロジスティックモデルをデータにフィットさせる\nfit_bv &lt;- fbvevd(bv_data, model = \"log\")\nprint(fit_bv)\n\n\nCall: fbvevd(x = bv_data, model = \"log\") \nDeviance: 578.8536 \nAIC: 592.8536 \nDependence: 0.3276571 \n\nEstimates\n    loc1    scale1    shape1      loc2    scale2    shape2       dep  \n 0.01609   0.97473  -0.04024   0.12028   0.93140  -0.08733   0.74187  \n\nStandard Errors\n   loc1   scale1   shape1     loc2   scale2   shape2      dep  \n0.10914  0.08230  0.07296  0.10225  0.07103  0.05012  0.06162  \n\nOptimization Information\n  Convergence: successful \n  Function Evaluations: 44 \n  Gradient Evaluations: 11 \n\n\n作成した二変量極値分布について、abvevd関数を用いれば従属構造を確認できます。「plot = TRUE」とすることで従属関数がプロット表示されるので、2つの変数間の極端な値の同時発生に対する従属関係を可視化できます。\n先ほどのフィッティング結果より、生成したデータの従属パラメータdepは0.74187となっており、弱い従属関係と言えます。このため、従属関数の形状も緩やかに下に凹んでいるのがわかります。\n\n# 従属関数のプロット\nabvevd(dep = fit_bv$estimate[\"dep\"], model = \"log\", plot = TRUE)\n\n\n\n\n\n\n\nfit_bv$estimate[\"dep\"]\n\n      dep \n0.7418707 \n\n\n\n\n\n\n\n\n従属パラメータと従属関数の関係\n\n\n\n従属パラメータdepは0から1までの値を取り、従属パラメータの大小によって従属関数の形状や従属の程度は下表のとおりとなります。\n\n\n\n従属パラメータと従属関数の関係\n\n\ndep\n従属関数の形\n意味\n\n\n\n\n1\nA(w) = 1\n完全独立\n\n\n0.7\n少し凹む\n弱い従属\n\n\n0.4\nかなり凹む\n中程度の従属\n\n\n0.1\nV字に近い\n強い従属\n\n\n0\nA(w) = max(w, 1 − w)\n完全従属",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>evd</span>"
    ]
  },
  {
    "objectID": "articles/evd.html#独立性の仮説検定",
    "href": "articles/evd.html#独立性の仮説検定",
    "title": "evd",
    "section": "独立性の仮説検定",
    "text": "独立性の仮説検定\n最後に、独立性の仮説検定を行います。evind.test関数により検定が可能です。二変量ロジスティックモデルを仮定して、帰無仮説dep=1（独立）に対して、対立仮説dep&lt;1（従属あり）を検定します。なお、evind.test関数ではmethodの引数により、スコア検定（score）か尤度比検定（ratio）が選択できます。次のコードではスコア検定を行っています。\n\n# 独立性検定（ロジスティックモデル）\nind_test &lt;- evind.test(bv_data, method = \"score\")\nprint(ind_test)\n\n\n    Score Test Of Independence\n\ndata:  bv_data\nnorm.score = -14.17, p-value &lt; 2.2e-16\nalternative hypothesis: true dependence is greater than independence\nsample estimates:\n      dep \n0.7418707 \n\n\n検定の結果を見ると、p値が非常に小さくなっており、帰無仮説（独立）を棄却できると考えられます。このため、変数間には統計的に有意な従属関係があると判断できます。先ほどabvevd関数により確認した内容と整合する結果となりました。",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>evd</span>"
    ]
  },
  {
    "objectID": "articles/fastshap.html",
    "href": "articles/fastshap.html",
    "title": "fastshap",
    "section": "",
    "text": "パッケージの概要\nfastshapは、予測モデルの解釈手法の一種であるSHAPを、モンテカルロシミュレーションにより計算するパッケージです。 他のパッケージと比較すると、非常に軽量かつ動作原理がシンプルであるのが特徴です。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>fastshap</span>"
    ]
  },
  {
    "objectID": "articles/fastshap.html#パッケージの概要",
    "href": "articles/fastshap.html#パッケージの概要",
    "title": "fastshap",
    "section": "",
    "text": "SHAPとは\nSHAPという手法については データサイエンス関連基礎調査WG　大江麗地 (2024) に解説があるため、こちらを参照することをお勧めします。 以下では詳細な説明は避け、概要のみを記載します。\nSHAP(SHapley Additive exPlanation)とは予測モデルの解釈に用いられる手法の一種で、 ある予測モデルの入力（説明変数）と出力（予測値）の組に対して、 どの説明変数の寄与によってその予測値となったのかを加法的に分解するものです。\n 個別の予測値 = その予測の説明変数1の寄与 + \\cdots + その予測の説明変数Nの寄与 + 予測値平均\nこのようにして分解された各サンプル・説明変数の寄与をSHAP値と呼びます。\n個別サンプルの予測に対する解釈を与える、いわゆるローカルな手法だと考えられますが、 多くのサンプルのSHAPを計算してそれをグラフにする、平均値で要約する等により、 モデル全体の解釈を与える、いわゆるグローバルな手法としても使用することが出来ます。\n\n\nfastshapの特徴\n説明変数ごとの寄与は、「その説明変数が入力されていない場合とされた場合の予測値の差」の（加重）平均で計算されます。 しかし、実際には「ある説明変数だけ予測モデルに入力しない」ということは通常出来ないため、 条件付期待値や、その説明変数を実データからランダムに選び出したときの予測値の平均のようなものを当てはめることとなります。 そのうえで、例えばN個の説明変数がある場合は、入力されているかどうかの組み合わせは2^N通りあることになるので、 たった1サンプルに対して、予測値の平均値のようなものを2^N通り分計算する必要があります。\n1サンプルならまだしも、グローバルな手法として使用するために多数のサンプルで計算する場合は計算量が莫大なものとなります。 そこで高速に計算する手法がいくつか提案されており、 そのうち fastshap は Štrumbelj, Erik and Kononenko, Igor (2014) によって提案されるモンテカルロシミュレーションによる近似を実装したものです。\n原理は非常にシンプルで、例えばあるサンプルのk番目の説明変数の寄与を計算する場合、\n\n「入力」する説明変数の数をランダムに決める（個数に関して一様な分布）\nその数だけ、「入力」する説明変数をk番目以外からランダムに選び出す\nk番目以外の説明変数について、「入力」する説明変数はそのサンプルそのまま、「入力」しない説明変数は全サンプルからランダムに選ぶ\nk番目の説明変数について「入力」した場合と「入力」しない（全サンプルからランダムに選んだ）場合の2回の予測を計算し、その差分を取る\nこの操作を複数回行い、差分の平均値をk番目の説明変数の寄与とする\n\nという流れになります。\n全サンプルのSHAPを計算したとしても予測を行う回数が 説明変数の数×2×試行回数×用意したサンプルの数 となり、 現実的な計算時間でグローバルな手法としてのSHAPを用いることが可能です。 ただし、特徴量の数・サンプル数によっては計算値を収束させるほど試行回数を増やすのは現実的ではないこともあるため、 あくまで近似値を計算するものと捉えるべきでしょう。\nなお、乱数の適用にC++を使用する、並列処理に対応させるという工夫により高速化を図っているのも本パッケージの特徴です。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>fastshap</span>"
    ]
  },
  {
    "objectID": "articles/fastshap.html#準備",
    "href": "articles/fastshap.html#準備",
    "title": "fastshap",
    "section": "準備",
    "text": "準備\n\nパッケージの読み込み\n\nlibrary(AER) #データセット\n\nlibrary(tibble) #data.frame拡張版\nlibrary(dplyr) #data.frameの操作\nlibrary(rsample) #データ分割\nlibrary(recipes) #前処理\n\nlibrary(xgboost) #今回使用するモデルのパッケージ\nlibrary(ranger)\nlibrary(glmnet)\n\nlibrary(ROCR) #精度評価\n\nlibrary(fastshap) #SHAPを計算できるパッケージ群\nlibrary(treeshap)\nlibrary(kernelshap)\nlibrary(DALEX)\n\nlibrary(foreach) #並列計算による高速化\nlibrary(doParallel)\n\nlibrary(ggplot2) #グラフの描画\nlibrary(patchwork) #複数のgpplotを組み合わせる\nlibrary(shapviz) #SHAPの可視化\n\n\n\nデータセットの読み込み\nChristian Kleiber and Achim Zeileis (2008) で使用されたデータセット等をまとめたパッケージAERに含まれる、 HealthInsuranceというデータセットを使用します。\n性別・年齢・学歴・家族構成・雇用状態（自営業か否か）健康保険の加入状況等に関する 約9,000個のサンプルが含まれています。 今回は、健康保険に加入しているかどうかを予測するモデルを作成することとします。\nデータセットの詳細については Achim Zeileis (2024) を参照してください1。\n\ndata(\"HealthInsurance\")\ndf_all &lt;- HealthInsurance\n\nsummary(df_all)\n\n health          age        limit         gender     insurance  married   \n no : 629   Min.   :18.00   no :7571   female:4169   no :1750   no :3369  \n yes:8173   1st Qu.:30.00   yes:1231   male  :4633   yes:7052   yes:5433  \n            Median :39.00                                                 \n            Mean   :38.94                                                 \n            3rd Qu.:48.00                                                 \n            Max.   :62.00                                                 \n                                                                          \n selfemp        family             region     ethnicity         education   \n no :7731   Min.   : 1.000   northeast:1682   other: 365   none      :1119  \n yes:1071   1st Qu.: 2.000   midwest  :2023   afam :1083   ged       : 374  \n            Median : 3.000   south    :3075   cauc :7354   highschool:4434  \n            Mean   : 3.094   west     :2022                bachelor  :1549  \n            3rd Qu.: 4.000                                 master    : 524  \n            Max.   :14.000                                 phd       : 135  \n                                                           other     : 667  \n\n\n\n\n前処理\n今回例として使用するモデルでは、説明変数が数値型である必要があるので、factor型変数を数値型に変換しておきます2。\n\nrec_init &lt;- df_all %&gt;% recipe(insurance ~ .) %&gt;% #前処理手順の定義\n  #ethinicityは最も多いカテゴリがcaucなので、これを基準カテゴリに変更\n  step_relevel(ethnicity, ref_level = \"cauc\") %&gt;% \n  #educationは学歴を表す説明変数で、大きいほど高学歴であるため、そのままダミー変数にするのではなく、数値に変換\n  step_mutate(education_main = as.numeric(education) - 1) %&gt;%\n  #ただし、最後のカテゴリだけは「その他」を表しているので、これだけは別のダミー変数に分離する\n  step_mutate(education_other = if_else(education_main == 6, 1, 0)) %&gt;%\n  step_mutate(education_main = if_else(education_main &lt; 6, education_main, 0)) %&gt;%\n  step_rm(education) %&gt;%\n  step_dummy(all_factor_predictors()) %&gt;% #他のfactor型変数は単純にダミー変数化\n  step_relevel(insurance, ref_level = \"yes\")\n  #目的変数は健康保険に加入しているかを表すinsurance\n\ndf_baked &lt;- rec_init %&gt;% prep() %&gt;% bake(new_data = NULL) #上記で定義した前処理手順を実際に実行\n\n上記前処理を施したうえで、学習データとテストデータに分割します。\n\nset.seed(2024)\nsplit_df &lt;- rsample::initial_split(df_baked, prop = 0.8) #80%を学習データ、20%をテストデータとする\ndf_train &lt;- rsample::training(split_df)\ndf_test &lt;- rsample::testing(split_df)\n\ndf_train_x &lt;- df_train %&gt;% dplyr::select(-insurance)\ndf_train_y &lt;- df_train$insurance\ndf_test_x &lt;- df_test %&gt;% dplyr::select(-insurance)\ndf_test_y &lt;- df_test$insurance\n\n\n\nモデル構築\n続いてXGBoostによる予測モデルを学習データをもとに構築します。3\n2値分類の問題ですが、予測モデルの出力としては加入しているか否かの2通りではなく、 加入している確率を出力するようにしています。\n\nset.seed(2024)\nmodel_xgboost &lt;- xgboost(data = as.matrix(df_train_x), label = as.matrix(2 - as.numeric(df_train_y)), nrounds = 100,\n                  params = list(eta = 0.3, max_depth = 2, gamma = 0, min_child_weight = 1, \n                             subsample = 1, colsample_bytree = 1, colsample_bynode = 2/14, objective = \"binary:logistic\"),\n                  verbose = 0)\n\n構築した予測モデルの精度をテストデータを用いて確認しておきます。\nまずはAUC(ROC)を確認します。これは2値分類モデルで使用される評価指標で、高いほど精度が良いという評価になります。\n\ncalc_logloss &lt;- function(act, pred){mean(-act *log(pred)-(1-act)*log(1-pred))}\ncalc_score &lt;- function(object, predfun, df_test_x, df_test_y){\n  yhat &lt;- object %&gt;% predfun(df_test_x)\n  pr &lt;- ROCR::prediction(yhat, df_test_y)\n  auc &lt;- pr %&gt;% ROCR::performance(\"auc\")\n  auc_plot &lt;- pr %&gt;% ROCR::performance(\"tpr\", \"fpr\")\n  list(\n    auc_plot = auc_plot,\n    auc = auc@y.values %&gt;% as.numeric(),\n    logloss = calc_logloss(act = 2 - as.numeric(df_test_y), pred = yhat)\n    )\n}\n\npredfun_xgboost &lt;- function(object, newdata){\n  dt &lt;- as.matrix(newdata)\n  object %&gt;% predict(newdata = dt)\n}\nscore &lt;- calc_score(model_xgboost, predfun_xgboost, df_test_x, df_test_y)\nscore$auc_plot %&gt;% plot()\n\n\n\n\n\n\n\nscore$auc\n\n[1] 0.7503933\n\n\n0.75は高くもなく低くもないといった程度ではあるものの、用途によってはこれでも十分でしょう。 （例えば True Positive Rate = 0.6, False Positive Rate = 0.2 あたりとなるしきい値をとれば、 　全体の8割程度を占める加入者のうち6割を削減しつつ、少数派の非加入者のうち8割を残した集団が作れる） 　 　 後で他のモデルと比較する際の参考のため、LogLossスコアも計算しておきます。 こちらは出力される確率値の正確性を評価するもので、低いほうが精度が良いという評価になります。\n\n#後で別のモデルと比較するため、スコアをデータフレームに格納\ndf_scores &lt;- tibble(model = \"xgboost\", auc = score$auc, logloss = score$logloss)\nscore$logloss\n\n[1] 0.4424059",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>fastshap</span>"
    ]
  },
  {
    "objectID": "articles/fastshap.html#fastshapの使用方法",
    "href": "articles/fastshap.html#fastshapの使用方法",
    "title": "fastshap",
    "section": "fastshapの使用方法",
    "text": "fastshapの使用方法\n\n基本的な使用方法\nまず、実際にSHAPを計算したいサンプルと、SHAP計算時に「入力しない説明変数」のためにランダムで選ぶ元になるサンプルを選びます。 この両者は同じでもよいですが、前者は数百件程度が一応の目安です。 後者は計算パフォーマンス次第ですが、前者を多めに取りたい場合は前者よりも少なめにします。\n\nset.seed(2024) #SHAPを計算したいサンプル\nnrow_shap &lt;- 100\ndf_shap &lt;- df_train[sample(nrow(df_train), nrow_shap), ]\ndf_shap_x &lt;- df_shap %&gt;% dplyr::select(-insurance)\n\nset.seed(2024+1) #ランダムで選ぶ元になるサンプル\nnrow_shapbg &lt;- 30\ndf_shapbg &lt;- df_train[sample(nrow(df_train), nrow_shapbg), ]\ndf_shapbg_x &lt;- df_shapbg %&gt;% dplyr::select(-insurance)\n\n次に、explain関数で実際にSHAPを計算します。\nここで、0から1の確率値を加法的に分解するよりも、 ロジット変換により実数全体の数値に変換してから分解したほうが説明変数ごとの寄与を比較する際には有用と考えられます。\npredict関数で出力される確率値をロジット変換したものを出力する関数を作成し、 引数pred_wrapperにこの関数を指定することでこれを実現することが出来ます。\n\nlogit &lt;- function(x) log(x) - log(1-x) \npredfun_xgboost_logit &lt;- function(object, newdata){ #predict関数の結果をロジット変換する関数\n  predfun_xgboost(object, newdata) %&gt;% logit()\n}\n\nt1 &lt;- proc.time()\nset.seed(2024)\nshap_fs &lt;- fastshap::explain(model_xgboost, #予測モデルのオブジェクト\n                             X = df_shapbg_x, #ランダムで選ぶもとになるサンプル\n                             # SHAPを計算したいサンプル　こちらは厳密にdata.frame型でないとエラー(バグ?)\n                             newdata = as.data.frame(df_shap_x),\n                             pred_wrapper = predfun_xgboost_logit, #予測値を生成する関数\n                             nsim = 10, #試行回数\n                             parallel = FALSE) #並列処理の設定\nt2 &lt;- proc.time()\nt0 &lt;- (t2-t1)[3]\nnames(t0) &lt;- NULL\n\ncat(\"処理時間:\", t0, \"秒\")\n\n\n\n処理時間: 2.07 秒\n\n\n最後にこれを可視化します。これにはshapvizパッケージを用いるとよいでしょう。 まず、個別のサンプルに対する寄与の分解を表示するには次のようにします。\n\nsv &lt;- shapviz::shapviz(shap_fs, X = df_shap_x) #shapvizパッケージで可視化できるオブジェクトに変換\nshapviz::sv_waterfall(sv, row_id = 1) #1つ目のサンプルの予測結果に対してプロット\n\n\n\n\n\n\n\n\nこのサンプルでは、自営業であること（selfemp_yes=1）や独身である（married_yes=0）ことによって、 平均的な被験者よりも健康保険に加入しない傾向にあると判断されたようです。\nまた、グローバルな手法として全サンプルの結果を一覧に表示し、 説明変数ごとに全般的にどの程度寄与しているかをプロットするには次のようにします。\n\nshapviz::sv_importance(sv, kind = \"beeswarm\")\n\n\n\n\n\n\n\n\n横軸は寄与の大きさを、色付けは説明変数の値を示しており、 例えば明るい色の点が右側にある場合は、その説明変数が高いほど予測確率が高くなることを示します。\nshapviz::sv_importance関数はデフォルトでは寄与が大きい説明変数から順に並べられるので、 最も予測確率への寄与が大きい説明変数は学歴（education_main）であることがわかります。 また、学歴が高いほど健康保険に加入する傾向があることがわかります。\n\n\n並列計算\nfastshapにはforeachパッケージによる並列計算が実装されています。\n事前にdoParallelパッケージの関数を使用して適切に並列計算の設定を行ったのちに、 引数parallelをTRUEにしたうえで、foreach関数に渡したい引数を追加することで並列計算が可能になります。\nforeachパッケージによる並列計算の際は複数の独立したR環境が生成されますが、 その環境に引き渡すべきパッケージや関数は明示的に指定する必要があることに注意してください。\n\ncluster &lt;- makeCluster(detectCores()-1)\nregisterDoParallel(cluster)\n\nt1 &lt;- proc.time()\nset.seed(2024)\nshap_fs &lt;- fastshap::explain(model_xgboost,\n                             X = df_shapbg_x,\n                             newdata = as.data.frame(df_shap_x),\n                             pred_wrapper = predfun_xgboost_logit,\n                             nsim = 100,\n                             parallel = TRUE,\n                             #独立したR環境に引き渡すべきパッケージや関数を記述\n                             .packages=c('dplyr'), .export=c(\"logit\", \"predfun_xgboost\"))\nt2 &lt;- proc.time()\nt &lt;- (t2-t1)[3]\nnames(t) &lt;- NULL\n\ncat(\"処理時間:\", t, \"秒\")\n\nstopCluster(cluster)\n\n\n\n処理時間: 10.97 秒\n\n\n並列計算しない場合は10回の試行に2.07秒かかっていました。 上記では試行回数を100回に増やしましたが、並列計算の恩恵によってその10倍よりは短い時間で計算できています。\nなお、どのような状況であっても並列計算で劇的に高速化する訳ではなく、また環境によって効果は異なってくる点に注意してください。\nちなみに、shapviz::sv_importance関数の結果は次のとおりであり、 試行回数を増やすことによって計算結果が収束してきていることがわかります。 逆に、試行回数10回では流石に少なすぎるかもしれません。\n\nsv &lt;- shapviz::shapviz(shap_fs, X = df_shap_x)\nshapviz::sv_importance(sv, kind = \"beeswarm\")\n\n\n\n\n\n\n\n\n\n\n引数adjustによる局所正確性の確保\n説明変数ごとの寄与を積み上げると元の予測値になるというSHAPの性質を局所正確性（local accuracy）といいます。 しかし、fastshapは近似的な手法のため、そのままでは局所正確性が満たされないという弱点があります。 引数adjustをTRUEにすることで、局所正確性を確保できるように補正することができます。\n\nset.seed(2024)\nshap_fs_notadjusted &lt;- fastshap::explain(model_xgboost,\n                             X = df_shapbg_x,\n                             newdata = as.data.frame(df_shap_x),\n                             pred_wrapper = predfun_xgboost_logit,\n                             nsim = 4,\n                             parallel = FALSE,\n                             adjust = FALSE)\nsv_notadjusted &lt;- shapviz::shapviz(shap_fs_notadjusted, X = df_shap_x)\n\nset.seed(2024)\nshap_fs_adjusted &lt;- fastshap::explain(model_xgboost,\n                             X = df_shapbg_x,\n                             newdata = as.data.frame(df_shap_x),\n                             pred_wrapper = predfun_xgboost_logit,\n                             nsim = 4,\n                             parallel = FALSE,\n                             adjust = TRUE)\nsv_adjusted &lt;- shapviz::shapviz(shap_fs_adjusted, X = df_shap_x)\n\n\n(shapviz::sv_waterfall(sv_notadjusted, row_id = 1) + ggtitle(\"補正前\")) +\n(shapviz::sv_waterfall(sv_adjusted, row_id = 1)  + ggtitle(\"補正後\"))\n\n\n\n\n\n\n\ncat(\"実際の予測値（のロジット変換後）:\", predfun_xgboost_logit(model_xgboost, df_shap_x[1, ]))\n\n実際の予測値（のロジット変換後）: -0.1223738\n\n\n補正後は、プロットの上部にあるf(x)の値が実際の予測値と一致していることが確認できます。\n\n\n引数exactによる理論値の計算\n引数exactをTRUEにした場合、次のモデルに限り、 モンテカルロシミュレーションを行うのではなくパラメータから理論的な値を計算することができます。\n\nstats::lm() ：線形回帰モデル…最初から加法的に関数が分解されているため、係数×説明変数を計算すればよい\nxgboost::xgboost(), lightgbm::lightgbm() ：ブースティング木モデル…Tree SHAPを用いる\n\nXGBoostの場合を示すと次のとおり。\n\nt1 &lt;- proc.time()\nset.seed(2024)\nshap_exact &lt;- fastshap::explain(model_xgboost,\n                             X = as.matrix(df_shap_x),\n                             exact = TRUE,\n                             parallel = FALSE)\nt2 &lt;- proc.time()\nt0 &lt;- (t2-t1)[3]\nnames(t0) &lt;- NULL\n\ncat(\"処理時間:\", t0, \"秒\")\n\n\n\n処理時間: 0.12 秒\n\n\nshapviz::sv_importance関数で可視化してみると次のとおりです。4\n\nsv_exact &lt;- shapviz::shapviz(shap_exact, X = df_shap_x)\nshapviz::sv_importance(sv_exact, kind = \"beeswarm\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>fastshap</span>"
    ]
  },
  {
    "objectID": "articles/fastshap.html#他のパッケージとの比較",
    "href": "articles/fastshap.html#他のパッケージとの比較",
    "title": "fastshap",
    "section": "他のパッケージとの比較",
    "text": "他のパッケージとの比較\nSHAPの計算を実装したパッケージは他にもいくつかあり、代表的なものを比較して整理すると次のようになります。\n\n\n\n\n\n\n\n\n\n\nパッケージ名\n概要\n対応モデル\n計算速度\n計算精度\n\n\n\n\nfastshap\nモンテカルロシミュレーションによる計算\nすべての予測モデル\n高速\n低い\n\n\nkernelshap\nKernel SHAPの高速な実装\nすべての予測モデル\n普通\n高い\n\n\ntreeshap\nTree SHAP\nランダムフォレスト、ブースティング木\n非常に高速\n高い\n\n\nDALEX\n他の手法(breakdown等)も統合的に扱える\nすべての予測モデル\n非常に低速\n高い\n\n\n\n最も高速なのはTree SHAPを実装したtreeshapです。\nTree SHAPは、決定木の場合は条件付期待値の理論値が予測モデルのパラメータから計算可能であり、 さらにアンサンブルモデルの場合には平均値でSHAPを計算できるという特徴によって高速に計算する手法です。 そのため、対応しているのはランダムフォレストやブースティング木を実装した一部のパッケージに限られます。\nそれ以外のモデルの場合、kernelshapは計算精度の高さと計算速度をある程度両立してはいるものの、 サンプル数や説明変数の数が多い場合には実行が難しい場合もあります。\nこれらに比べてfastshapは非常に軽量かつシンプルなため、 利用者側で精度と計算速度の調整が行いやすいという点が特徴と言えます。\nDALEXはSHAP以外の他の解釈手法をも統合的に扱える点が特徴ですが、 計算速度は低速であり、グローバルな手法としては扱いづらいかもしれません。 グローバルな手法として用いる場合はshapvizパッケージが使用できない（2024.8時点）という弱点もあります。\n\nkernelshapの例\n今回の例の場合はfastshapで並列計算を採用するよりも、高速で良い結果が得られているように思われます。\n\nt1 &lt;- proc.time()\nset.seed(2024)\nshap_ks &lt;- kernelshap::kernelshap(model_xgboost, X = df_shap_x, bg_X = df_shapbg_x,\n                                  pred_fun = predfun_xgboost_logit)\n\nKernel SHAP values by the hybrid strategy of degree 2\n\nt2 &lt;- proc.time()\nt0 &lt;- (t2-t1)[3]\nnames(t0) &lt;- NULL\n\ncat(\"処理時間:\", t0, \"秒\")\n\n\n\n処理時間: 3.16 秒\n\n\n\nsv &lt;- shapviz::shapviz(shap_ks) #kernelshapの場合引数Xは不要\nshapviz::sv_importance(sv, kind = \"beeswarm\")\n\n\n\n\n\n\n\n\n\n\nDALEXの例\nDALEXの場合はshapvizで可視化できるのは個別サンプルの寄与を計算するDALEX::predict_parts関数のみで、 グローバルな手法で用いるDALEX::shap_aggregated関数は対応していません。\n\nexplainer &lt;- DALEX::explain(model_xgboost,\n                     data = df_shapbg_x,\n                     predict_function = predfun_xgboost_logit,\n                     quietly = TRUE,\n                     verbose = FALSE)\n\nt1 &lt;- proc.time()\nset.seed(2024)\nshap_dalex &lt;- DALEX::predict_parts(explainer, df_shap_x[1, ], type = 'shap')\nt2 &lt;- proc.time()\nt0 &lt;- (t2-t1)[3]\nnames(t0) &lt;- NULL\n\ncat(\"処理時間:\", t0, \"秒\")\n\n\n\n処理時間: 1.97 秒\n\n\n\nsv_dalex &lt;- shapviz::shapviz(shap_dalex)\nshapviz::sv_waterfall(sv_dalex, row_id = 1)\n\n\n\n\n\n\n\n\nさらに、DALEX::shap_aggregated関数は計算時間が非常に長く、実用的とはいえません。\n\ndf_shap_x_dalex &lt;- df_shap_x\n#DALEX::shap_aggregatedには説明変数の型がすべて同じではエラーになるバグがある（2024.8時点）ため、\n#一つだけ別の型（numericからinteger）に変換しておく\ndf_shap_x_dalex$education_main &lt;- as.integer(df_shap_x$education_main)\ndf_shap_x_dalex &lt;- df_shap_x_dalex[1:4, ] #あまりにも計算時間が長いので、4サンプルに限定\n\nexplainer &lt;- DALEX::explain(model_xgboost,\n                     data = df_shapbg_x,\n                     predict_function = predfun_xgboost_logit,\n                     quietly = TRUE,\n                     verbose = FALSE)\n\nt1 &lt;- proc.time()\nset.seed(2024)\nshap_dalex_agg &lt;- DALEX::shap_aggregated(explainer, df_shap_x_dalex, type = 'shap', B = 10) #Bは試行回数\nt2 &lt;- proc.time()\nt0 &lt;- (t2-t1)[3]\nnames(t0) &lt;- NULL\n\ncat(\"処理時間:\", t0, \"秒\")\n\n\n\n処理時間: 2.9 秒\n\n\n\nshap_dalex_agg %&gt;% plot()\n\n\n\n\n\n\n\n\n\n\n他のモデルの例\n\nGLM\n最も単純な予測モデルの例として、ロジスティック回帰の例を用意します。\n実は単純なロジスティック回帰でもそれなりの予測精度になります。 最も重要な説明変数である学歴については単純に学歴の低いものから0, 1, 2, …と並べて数値に変換しただけですが、 これでも非常によくフィットします。 前述までのSHAPによるグローバルな分析にて、education_mainの点がおおむね等間隔に群団を作っていたことと整合します。\n\nmodel_glm &lt;- glm(insurance ~ ., data = df_train, family = \"binomial\")\npredfun_glm &lt;- function(object, newdata){\n  res &lt;- object %&gt;% predict(newdata = newdata, type = \"response\")\n  1 - res\n}\npredfun_glm_logit &lt;- function(object, newdata){\n  res &lt;- object %&gt;% predict(newdata = newdata, type = \"link\")\n  -res\n}\nscore &lt;- calc_score(model_glm, predfun_glm, df_test_x, df_test_y)\ndf_scores &lt;- df_scores %&gt;% bind_rows(tibble(model = \"glm\", auc = score$auc, logloss = score$logloss))\n\nscore$auc_plot %&gt;% plot()\n\n\n\n\n\n\n\ncat(\"auc:\", score$auc, \"logloss:\", score$logloss)\n\nauc: 0.7400438 logloss: 0.4504438\n\n\n\n\n特徴量エンジニアリングを加えた（正則化）GLM\n特徴量エンジニアリング（説明変数の加工）を行うことで、非線形な作用をGLMで捉えられるようになります。 この場合はモデル自身の係数が複雑になってしまい解釈可能性が低下してしまうので、 SHAPのような手法を駆使して解釈することは考えられます。\nTree SHAPを用いることができない例として、この特徴量エンジニアリングを加えたGLMを用意します。\n\nrecp_glm_prep &lt;- df_train %&gt;% recipe(insurance ~ .) %&gt;%\n  #高学歴層では等間隔とならないことへの対応\n  step_mutate(education_geq_4 = 1*(education_main &gt;= 4)) %&gt;% \n  #年齢が非常に複雑に作用することを捉える\n  step_mutate(age_geq_20 = 1*(age &gt;= 20)) %&gt;% \n  step_mutate(age_geq_24 = 1*(age &gt;= 24)) %&gt;%\n  step_mutate(age_geq_26 = 1*(age &gt;= 26)) %&gt;%\n  step_mutate(age_geq_27 = 1*(age &gt;= 27)) %&gt;%\n  step_mutate(age_geq_57 = 1*(age &gt;= 57)) %&gt;%\n  step_mutate(age_geq_58 = 1*(age &gt;= 58)) %&gt;% \n  #家族の数もわずかに非線形な作用がある\n  step_mutate(family_geq_2 = 1*(family &gt;= 2)) %&gt;%\n  step_mutate(family_geq_3 = 1*(family &gt;= 3)) %&gt;%\n  step_mutate(family_geq_7 = 1*(family &gt;= 7)) %&gt;%\n  step_mutate(family_geq_11 = 1*(family &gt;= 11)) %&gt;%\n  step_mutate(family_geq_12 = 1*(family &gt;= 12)) %&gt;%\n  #重要な説明変数に関連する交互作用項を追加\n  step_interact(terms = ~ (tidyselect::starts_with(\"age\") + tidyselect::starts_with(\"education_\") +\n                       married_yes + selfemp_yes + education_other)*all_numeric_predictors(), sep = \":\") %&gt;%\n  prep()\n\ndf_train_x_glm_prep &lt;- recp_glm_prep %&gt;% bake(new_data = df_train_x)\n\nlambda_glm_prep &lt;- 0.0036 #正則化項の係数\n#交互作用項が多数作られてしまうので、Lasso回帰による変数選択を組み合わせる\nmodel_glm_prep &lt;- glmnet::glmnet(x = df_train_x_glm_prep, y = 2 - as.numeric(df_train_y),\n                                 alpha = 1, lambda = lambda_glm_prep, family = \"binomial\")\n\n#予測関数の中に上記前処理を行う処理を差し込むことで\n#他のモデルと同じ形式のデータフレームを入力できるようにしておく\npredfun_glm_prep &lt;- function(object, newdata){\n  df &lt;- recp_glm_prep %&gt;% bake(new_data = newdata)\n  res &lt;- object %&gt;% predict(newx = as.matrix(df), type = \"response\")\n  res[,1]\n}\npredfun_glm_prep_logit &lt;- function(object, newdata){\n  df &lt;- recp_glm_prep %&gt;% bake(new_data = newdata)\n  res &lt;- object %&gt;% predict(newx = as.matrix(df), type = \"link\")\n  res[,1] #ベクトルにしておかないとfastshapの計算に失敗する\n}\n\nscore &lt;- calc_score(model_glm_prep, predfun_glm_prep, df_test_x, df_test_y)\ndf_scores &lt;- df_scores %&gt;% bind_rows(tibble(model = \"glm_prep\", auc = score$auc, logloss = score$logloss))\n\nscore$auc_plot %&gt;% plot()\n\n\n\n\n\n\n\ncat(\"auc:\", score$auc, \"logloss:\", score$logloss)\n\nauc: 0.7446471 logloss: 0.4475717\n\n\n\n\nrangerによるランダムフォレスト\nTree SHAPが使用できる他の例として、rangerパッケージのランダムフォレストを用意します。\n\nset.seed(2024)\ndf_train_ranger &lt;- df_train\ndf_train_ranger$insurance &lt;- 2 - as.numeric(df_train_ranger$insurance)\nmodel_ranger &lt;- ranger::ranger(formula = insurance ~ ., data = df_train_ranger, \n                               importance = 'none', #probability = TRUE,\n                               num.trees = 100, min.node.size = 40, mtry = 4)\n#分類問題の場合はprobability = TRUEとすべきだが、treeshap::unifyでエラーとなるため通常の回帰モデルとする\n\npredfun_ranger &lt;- function(object, newdata){\n  res &lt;- object %&gt;% predict(data = newdata)\n  res$predictions\n}\npredfun_ranger_logit &lt;- function(object, newdata){ #predict関数の結果をロジット変換する関数\n  predfun_ranger(object, newdata) %&gt;% logit()\n}\n\nscore &lt;- calc_score(model_ranger, predfun_ranger, df_test_x, df_test_y)\ndf_scores &lt;- df_scores %&gt;% bind_rows(tibble(model = \"ranger\", auc = score$auc, logloss = score$logloss))\n\nscore$auc_plot %&gt;% plot()\n\n\n\n\n\n\n\ncat(\"auc:\", score$auc, \"logloss:\", score$logloss)\n\nauc: 0.7360556 logloss: 0.4536576\n\n\n\n\n予測モデルの精度比較\n本稿の主題を外れるため、各予測モデルの解釈を比較したりはしませんが、 いずれもうまく学習できているということを確かめるために XGBoostと比較しても精度があまり変わらないということを確認しておきます。\n\ndf_scores\n\n\n  \n\n\n\n\n\n\n計算時間比較\nここまでに用意した4つのモデル（XGBoost、rangerによるランダムフォレスト、GLM、特徴量エンジニアリング付GLM）に対して、 fastshap、kernelshap、treeshapによるSHAPの計算時間を比較してみます。\nSHAPを計算するサンプル数を増やした時に計算時間がどのように変化するかをプロットすると次のとおり。 （treeshap以外で使用するnrow_shapbgは50、fastshapの試行回数は10）\nサンプル数を増やしてもfastshapはあまり計算時間が増えませんが、 kernelshapは比例的に計算時間が増加します。 treeshapは非常に高速ですが、rangerではツリーの構造のためなのか、サンプル数を増やすとfastshapと逆転します。\n特徴量エンジニアリング付きGLM（glm_prep）は予測関数を呼び出すたびに前処理を実行するため、 他のモデルと比べて計算時間が長くなっています。\n\nggplot(data = results %&gt;% dplyr::filter(nrow_shapbg &lt;= 50, nsim &lt;= 10),\n       mapping = aes(x = nrow_shap, y = time, color = model, linetype = method)) +\n  geom_line()\n\n\n\n\n\n\n\n\n特徴量エンジニアリング付きGLMを除くと次のとおり。\n\nggplot(data = results %&gt;% dplyr::filter(nrow_shapbg &lt;= 50, nsim &lt;= 10, model != \"glm_prep\"),\n       mapping = aes(x = nrow_shap, y = time, color = model, linetype = method)) +\n  geom_line()\n\n\n\n\n\n\n\n\nさらにXGBoostに着目すると次のとおり。\n\nggplot(data = results %&gt;% dplyr::filter(nrow_shapbg &lt;= 50, nsim &lt;= 10, model == \"xgboost\"),\n       mapping = aes(x = nrow_shap, y = time, color = model, linetype = method)) +\n  geom_line()\n\n\n\n\n\n\n\n\n横軸をnrow_shapbg（「入力しない説明変数」のためにランダムで選ぶ元になるサンプル数）とすると次のとおり。 kernelshapではおおむね件数に比例して計算時間が増加しています。 一方fastshapではSHAPを計算するサンプル数ほど計算時間との関係性が明確ではありませんでした。\n\nggplot(data = results %&gt;% dplyr::filter(nrow_shap == 50, nrow_shapbg &gt;= 50, nsim &lt;= 10),\n       mapping = aes(x = nrow_shapbg, y = time, color = model, linetype = method)) +\n  geom_line()\n\n\n\n\n\n\n\n\nfastshapの試行回数を横軸にとると、計算時間は試行回数と正比例していることがわかります。\n\nggplot(data = results %&gt;% dplyr::filter(nrow_shap == 50, nrow_shapbg == 50, nsim &gt;= 10),\n       mapping = aes(x = nsim, y = time, color = model, linetype = method)) +\n  geom_line()\n\n\n\n\n\n\n\n\n参考までに、上記グラフを作成するためのコードは次のとおりです。\n\n#計算時間比較をしたいモデルの定義\nmodelsets &lt;- list()\n\nmodelsets$glm &lt;- list(object = model_glm, predfun = predfun_glm_logit)\nmodelsets$glm_prep &lt;- list(object = model_glm_prep, predfun = predfun_glm_prep_logit)\nmodelsets$xgboost &lt;- list(object = model_xgboost, predfun = predfun_xgboost_logit)\nmodelsets$ranger &lt;- list(object = model_ranger, predfun = predfun_ranger_logit)\n\n\n#計算パターンの定義\ngrid &lt;- bind_rows(\n          tidyr::expand_grid(\n            model = c(\"glm\", \"glm_prep\", \"xgboost\", \"ranger\"),\n            bind_rows(\n              tidyr::expand_grid(\n                method = \"kernelshap\",\n                bind_rows(tidyr::expand_grid(nrow_shap = seq(1,5)*50, nrow_shapbg = 50, nsim = 10),\n                          tidyr::expand_grid(nrow_shap = 50, nrow_shapbg = seq(2,5)*50, nsim = 10))\n              ),\n              tidyr::expand_grid(\n                method = \"fastshap\",\n                bind_rows(tidyr::expand_grid(nrow_shap = seq(1,5)^2*50, nrow_shapbg = 50, nsim = 10),\n                          tidyr::expand_grid(nrow_shap = 50, nrow_shapbg = seq(2,5)^2*50, nsim = 10),\n                          tidyr::expand_grid(nrow_shap = 50, nrow_shapbg = 50, nsim = seq(2,5)*10))\n              )\n            )\n          ),\n          tidyr::expand_grid(\n            model = c(\"xgboost\", \"ranger\"),\n            tidyr::expand_grid(\n              tidyr::expand_grid(\n                method = \"treeshap\",\n                bind_rows(tidyr::expand_grid(nrow_shap = seq(1,5)^2*50, nrow_shapbg = 0, nsim = 10))\n              )\n            )\n          )\n        )\n\n\n#計算パターン1つに対して計算時間を測る関数\ncalc_shap &lt;- function(object, predfun, method, nrow_shap = 500, nrow_shapbg = 60, nsim = 1){\n  t1 &lt;- proc.time()\n  \n  set.seed(2024)\n  df_shap &lt;- df_train[sample(nrow(df_train), nrow_shap), ]\n  df_shap_x &lt;- df_shap %&gt;% dplyr::select(-insurance)\n  \n  set.seed(2024+1)\n  df_shapbg &lt;- df_train[sample(nrow(df_train), nrow_shapbg), ]\n  df_shapbg_x &lt;- df_shapbg %&gt;% dplyr::select(-insurance)\n\n  nsim_res &lt;- 0\n  nrow_shapbg_res &lt;- nrow_shapbg\n\n  if(method == \"fastshap\"){\n    shap_fs &lt;- fastshap::explain(object, \n                                 X = df_shapbg_x, \n                                 newdata = as.data.frame(df_shap_x), \n                                 pred_wrapper = predfun, nsim = nsim)\n    sv &lt;- shapviz(shap_fs, X = df_shap_x)\n    nsim_res &lt;- nsim\n  }else if(method == \"treeshap\"){\n    obj_uni &lt;- treeshap::unify(object, df_shap_x)\n    shap_ts &lt;- treeshap::treeshap(obj_uni, x = df_shap_x)\n    sv &lt;- shapviz(shap_ts)\n    nrow_shapbg_res &lt;- 0\n  }else if(method == \"kernelshap\"){\n    shap_ks &lt;- kernelshap(object, X = df_shap_x, bg_X = df_shapbg_x, pred_fun = predfun)\n    sv &lt;- shapviz(shap_ks)\n  }\n  plot &lt;- sv_importance(sv, kind = \"beeswarm\")\n  \n  t2 &lt;- proc.time()\n  t0 &lt;- (t2-t1)[3]\n  names(t0) &lt;- NULL\n  return(list(method = method, nrow_shap = nrow_shap, nrow_shapbg = nrow_shapbg_res,\n              nsim = nsim_res, sv = sv, sv_importance = plot, time = t0))\n}\n\n\n#実際に計算時間の計測を行うループ\nresults &lt;- tibble()\nresults_plots &lt;- list()\nundebug(calc_shap)\nfor(r in 1:nrow(grid)){\n  model &lt;- grid$model[[r]]\n  res &lt;- calc_shap(object = modelsets[[model]]$object,\n            predfun = modelsets[[model]]$predfun,\n            method = grid$method[[r]],\n            nrow_shap = grid$nrow_shap[[r]],\n            nrow_shapbg = grid$nrow_shapbg[[r]],\n            nsim = grid$nsim[[r]])\n  rw &lt;- list()\n  rw$model &lt;- model\n  rw$method &lt;- res$method\n  rw$nrow_shap &lt;- res$nrow_shap\n  rw$nrow_shapbg &lt;- res$nrow_shapbg\n  rw$nsim &lt;- res$nsim\n  rw$time &lt;- res$time\n  rw$id &lt;- paste(rw$model, rw$method, rw$nrow_shap, rw$nrow_shapbg, rw$nsim, sep = \",\")\n  results &lt;- bind_rows(results, as.tibble(rw))\n  \n  results_plots[[rw$id]] &lt;- res\n  print(paste(rw$id, rw$time))\n}",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>fastshap</span>"
    ]
  },
  {
    "objectID": "articles/fastshap.html#参考文献",
    "href": "articles/fastshap.html#参考文献",
    "title": "fastshap",
    "section": "参考文献",
    "text": "参考文献",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>fastshap</span>"
    ]
  },
  {
    "objectID": "articles/fastshap.html#footnotes",
    "href": "articles/fastshap.html#footnotes",
    "title": "fastshap",
    "section": "",
    "text": "元の研究は、自営業者の健康保険加入率の低さと健康状態の関係性を調べたものです。この研究で用いられた、1996年の米国医療費パネル調査(MEPS(Agency for Healthcare Research and Quality, n.d.))から抽出されたものがこのデータセットです。↩︎\nなお、ここではデータ前処理にrecipesパッケージを使用しています。 また、%&gt;%はmagrittrパッケージによるパイプ演算子で、右辺の関数の第1引数に左辺を渡すという働きがあります。 たとえばa %&gt;% f %&gt;% g(b)という記述はg(f(a),b)と同等です。↩︎\nハイパーパラメータは事前にチューニングしたものを入力しています。 チューニングの過程については本稿の主題を外れるので、割愛します。↩︎\n実はXGBoostの場合fastshapパッケージを用いずとも直接shapvizパッケージによる可視化を行うことが出来るので、実用性はあまりありません。↩︎",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>fastshap</span>"
    ]
  },
  {
    "objectID": "articles/forcats.html",
    "href": "articles/forcats.html",
    "title": "forcats",
    "section": "",
    "text": "パッケージの概要\nforcatsは、factor型のデータを取り扱うための様々な関数を提供しているパッケージです。\n\nsuppressMessages(require(tidyverse))\nrequire(forcats)\n\n\n\nfactor型について\nfactor型のベクトルは、ベクトルの要素に加えて、levelsという情報を持っています。例えば、a,b,cの要素を持つcharacter型のベクトルに対して、factor関数を適用するとfactor型のベクトルが得られます。\nなお、ベクトル内の要素が重複している場合、Levelsではまとめて表示されます。以下の例では、「c」が重複していますが、Levelsで表示されるcは１つだけになっています。\n\nx &lt;- c(\"a\", \"c\", \"b\", \"c\")\nfactor(x)\n\n[1] a c b c\nLevels: a b c\n\n\n\n\nfactor型の活用方法\nfactor型のlevelsは、例えば、データを可視化する際に活用できます。\n具体例を確認するために、まず、Rのiris（あやめのがく片、花弁の幅・長さに関するデータセット）を用いて、あやめの種類（Species）別のがく片の幅の平均値をmean_Sepal_Widthに格納し、その結果を棒グラフで表示します。この際、iris_mean_by_speciesのSpeciesカラムはfactor型になっていることに注意してください。\n\niris_mean_by_species &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(across(everything(), mean))\niris_mean_by_species\n\n# A tibble: 3 × 5\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa             5.01        3.43         1.46       0.246\n2 versicolor         5.94        2.77         4.26       1.33 \n3 virginica          6.59        2.97         5.55       2.03 \n\niris_mean_by_species |&gt;\n  ggplot(aes(x = Species, y = Sepal.Width, fill = Species)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Mean Values by Species\", x = \"Species\", y = \"Mean Sepal.Width\")\n\n\n\n\n\n\n\n\nデータを可視化した際、結果をSepal.Widthが大きい順（つまり、setosa、versicolor、virginicaの順）に並べたいとします。\nまず、可視化するデータセットをSpela.Widthが大きい順に並べ替えた結果を表示してみます。結果は以下の通り、元の結果と変わりません。\n\niris_mean_by_species$Species\n\n[1] setosa     versicolor virginica \nLevels: setosa versicolor virginica\n\ndesc_iris_mean_by_species &lt;- iris_mean_by_species |&gt;\n  arrange(desc(Sepal.Width))\ndesc_iris_mean_by_species\n\n# A tibble: 3 × 5\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa             5.01        3.43         1.46       0.246\n2 virginica          6.59        2.97         5.55       2.03 \n3 versicolor         5.94        2.77         4.26       1.33 \n\ndesc_iris_mean_by_species |&gt;\n  ggplot(aes(x = Species, y = Sepal.Width, fill = Species)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Mean Values by Species\", x = \"Species\", y = \"Mean Sepal.Width\")\n\n\n\n\n\n\n\n\n次に、iris_mean_by_speciesのSpeciesのlevelsをsetosa、virginica、verscolorの順番に変更します。その後、これまでと同様に棒グラフを作成すると、結果がSepal.Widthが大きい順番に表示されます。\nlevelsの変更に利用したfct_relevel関数は、forcatsパッケージの関数で指定した順番でlevelsを設定できます。なお、全ての要素を指定する必要はありません。（指定しなかった要素は後ろに回されます）\n\ndesc_iris_mean_by_species_2 &lt;- iris_mean_by_species\ndesc_iris_mean_by_species_2$Species &lt;- desc_iris_mean_by_species_2$Species |&gt;\n  fct_relevel(\"setosa\", \"virginica\")\ndesc_iris_mean_by_species_2\n\n# A tibble: 3 × 5\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa             5.01        3.43         1.46       0.246\n2 versicolor         5.94        2.77         4.26       1.33 \n3 virginica          6.59        2.97         5.55       2.03 \n\ndesc_iris_mean_by_species_2 |&gt;\n  ggplot(aes(x = Species, y = Sepal.Width, fill = Species)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Mean Values by Species\", x = \"Species\", y = \"Mean Sepal.Width\")\n\n\n\n\n\n\n\n\n\n\nfactor型のlevelsの順序を変更する関数\nforcatsパッケージには、factor型のlevelsの順序を変更する関数が提供されています。以下で、いくつか利用例を示します。\nfct_infreq関数は、登場頻度が多い要素の順番にlevelsを整理します。\n\nf &lt;- factor(c(\"c\", \"c\", \"a\"))\nf\n\n[1] c c a\nLevels: a c\n\nfct_infreq(f)\n\n[1] c c a\nLevels: c a\n\n\nfct_inorder関数は、要素の登場した順番でlevelsを整理します。\n\nf &lt;- factor(c(\"b\", \"a\"))\nf\n\n[1] b a\nLevels: a b\n\nfct_inorder(f)\n\n[1] b a\nLevels: b a\n\n\nfct_rev関数は、levelsを反転させます。\n\nf &lt;- factor(c(\"a\", \"b\", \"c\"))\nf\n\n[1] a b c\nLevels: a b c\n\nfct_rev(f)\n\n[1] a b c\nLevels: c b a\n\n\nfct_shift関数は、levelsをずらします。デフォルトでは、levelsが１つ左にずれます（最初のlevelsは最後に回ります）が、引数で指定することで、ずれる大きさを変更可能です。また、引数に負の値を指定した場合は右にずれます。\n\nf &lt;- factor(c(\"a\", \"b\", \"c\", \"d\"))\nf\n\n[1] a b c d\nLevels: a b c d\n\nfct_shift(f)\n\n[1] a b c d\nLevels: b c d a\n\nfct_shift(f, 2)\n\n[1] a b c d\nLevels: c d a b\n\nfct_shift(f, -1)\n\n[1] a b c d\nLevels: d a b c\n\n\nfct_reorder関数は、他の変数を使って、levelsを並べ替えることが可能です。\nfactor型の活用方法で実施した「Sepal.Widthの平均値が大きい順に並べ替える」対応をfct_reorder関数で実施するには、以下のように記載します。\nfct_reorder(Species, Sepal.Width, .fun = mean, .desc = TRUE)は、SpeciesのlevelsをSepal.Widthの平均値で変更しています。デフォルトは昇順ですが、「.desc = TRUE」と指定すると降順となります。\n\niris_mean_by_species |&gt;\n  ggplot(aes(x = Species, y = Sepal.Width, fill = Species)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Mean Values by Species\", x = \"Species\", y = \"Mean Sepal.Width\")\n\n\n\n\n\n\n\niris_mean_by_species |&gt;\n  ggplot(aes(x = fct_reorder(Species, Sepal.Width, .fun = mean, .desc = TRUE), y = Sepal.Width, fill = Species)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Mean Values by Species\", x = \"Species\", y = \"Mean Sepal.Width\")",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>forcats</span>"
    ]
  },
  {
    "objectID": "articles/genlasso.html",
    "href": "articles/genlasso.html",
    "title": "genlasso",
    "section": "",
    "text": "パッケージの概要\ngenlassoパッケージはラッソ正則化線形回帰モデルについて、 通常のものよりも罰則項を一般化したモデルを取り扱うものです。 加えて、フューズドラッソ（Fused LASSO）等の特別なパターンについてはより効率的なアルゴリズムを用意しています。 なお、GLM には対応していません。",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>genlasso</span>"
    ]
  },
  {
    "objectID": "articles/genlasso.html#パッケージの概要",
    "href": "articles/genlasso.html#パッケージの概要",
    "title": "genlasso",
    "section": "",
    "text": "一般化ラッソ回帰問題とは\nまず、本パッケージが取り扱う一般化ラッソ回帰問題について簡単に説明します。\n実数値の説明変数が k 個・目的変数が1個あり、 これらについて n 組の観測がある場合の線形回帰問題を考えます。\n通常の線形回帰問題は、説明変数の実測値を表す n 行 k 列実数値行列 X = (x_{ij}) と目的変数の実測値を表す y = (y_i)_{i=1}^n \\in \\mathbb{R} ^ n に対して、\n\n\\sum_{i=1}^{n}\\left(y_i - \\sum_{j=1}^{k} \\left( x_{ij} \\cdot \\beta_j \\right) \\right)^2\n を最小化するような \\beta = (\\beta_j)_{j=1}^k \\in \\mathbb{R}^k を求める問題（最小二乗法）として定式化されます。\nベクトル v = (v_i)_{i=1}^m \\in \\mathbb{R}^m と 1 \\leq p &lt; \\infty に対し、 p - ノルム \\|v\\|_p := \\left( \\sum_{i=1}^m |v_i|^p \\right)^{1/p} が定義されます。これを用いると、最小化すべき関数は\n\n\\| y - X\\beta \\|_2^2\n\nと簡単に書けます。\nこれを少し変更した、\n\n\\frac12\\| y - X\\beta \\|_2^2 + \\lambda\\| \\beta \\|_1\n\nを最小化する問題をラッソ回帰問題 1 といいます。 ここで、\\lambda \\geq 0 はモデルのハイパーパラメータです。\nこの第2項をラッソ罰則項といい、これにより線形回帰モデルの解が安定しやすくなります。 このように、最小化問題にラッソ罰則項を付け加えることをラッソ正則化といい、 特に予測モデル構築の観点では、予測精度が向上し、かつ変数選択が自動的に行われることが知られています(岩沢 宏和 and 平松 雄司 2019)。 入力データに比して説明変数が非常に多い場合のような、いわゆるスパースモデリングのための重要手法として深く研究されています。\n本パッケージで取り扱う一般化ラッソ回帰問題とは、この罰則項をさらに一般化した\n\n\\frac12\\| y - X\\beta \\|_2^2 + \\lambda\\| D \\beta \\|_1\n の最小化問題を取り扱うものです。ただし、D は l 行 k 列実数値行列です。\nこの中には、罰則項を \\lambda \\sum_{i=1}^{k-1} | \\beta_{i+1} - \\beta_{i}  | とした、 いわゆるフューズドラッソ回帰問題も含まれます。 隣接した説明変数同士の係数が同じ値に近づくような作用を与え、ノイズ除去などに用いられます。 一般化ラッソ回帰問題にて D を次のように設定することでこの問題に帰着します。\n\nD = \\begin{bmatrix}\n-1 & 1 & 0 & 0 & \\cdots & 0 & 0\\\\\n0 & -1 & 1 & 0 & \\cdots & 0 & 0 \\\\\n&  & \\vdots &  & \\ddots & \\vdots &  \\\\\n0 & 0 & 0 & 0 & \\cdots & -1 & 1 \\\\\n\\end{bmatrix}\n\n本パッケージが優れているのは、この一般化ラッソ回帰問題について Taylor B. Arnold and Ryan J. Tibshirani (2016) の手法を実装することにより、 解パス、すなわちすべての \\lambda に対する解を一度に与えることができることです。 これにより、\\lambda を変更したときの振る舞いを観察したり、 ハイパーパラメータ \\lambda のチューニングに活用したりすることができます。",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>genlasso</span>"
    ]
  },
  {
    "objectID": "articles/genlasso.html#準備",
    "href": "articles/genlasso.html#準備",
    "title": "genlasso",
    "section": "準備",
    "text": "準備\n\nパッケージの読み込み\n\nlibrary(genlasso)\nlibrary(ggplot2) #可視化に使用\n\n\n\nデータセットの読み込み\nBostonデータセットとはボストンの住宅価格に関するデータで、 今回はmedv（住宅価格）を目的変数とし、これを他の説明変数で予測するモデルを構築することとします。\n\nset.seed(42)\n#学習用データと評価用データを分割します。\ndf_all &lt;- MASS::Boston\nsplit_df_all &lt;- rsample::initial_split(df_all)\n\n#学習用データを説明変数と目的変数に分割します。\ndf_train &lt;- rsample::training(split_df_all)\ndf_train_X &lt;- dplyr::select(df_train, -medv)\ndf_train_y &lt;- df_train$medv\n\n#評価用データを説明変数と目的変数に分割します。\ndf_test &lt;- rsample::testing(split_df_all)\ndf_test_X &lt;- dplyr::select(df_test, -medv)\ndf_test_y &lt;- df_test$medv",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>genlasso</span>"
    ]
  },
  {
    "objectID": "articles/genlasso.html#使用方法",
    "href": "articles/genlasso.html#使用方法",
    "title": "genlasso",
    "section": "使用方法",
    "text": "使用方法\n\n通常のラッソ回帰問題\nBostonデータセットにて、罰則項を \\lambda \\|\\beta\\| とする通常のラッソ回帰モデルを構築することを考えます。\n罰則項の存在から、説明変数のスケールの違いにより最小化問題の解が変わるという性質があります。\n例えば、ある説明変数だけを全ての観測で一様に10倍すると、 通常の線形回帰問題では対応する係数を10分の1すればよいだけですが、 ラッソ回帰問題では罰則項における評価が他の説明変数に比べて10分の1になってしまい、 最小化問題の解が変わってしまいます。\nスケールの違いによって解が変わらないようにするため、 平均がゼロ、分散を1になるように線形変換する標準化（normalize）を最初に行うこととします。\n\n#標準化のため、recipesパッケージを使用します。\n#「標準化する」という手順書を作成し、これを変数に格納します。\nrec_nm &lt;- recipes::recipe(df_train, medv ~ .) |&gt; \n  #全ての説明変数を標準化\n  recipes::step_normalize(recipes::all_numeric_predictors())\n\n#学習用データに基づき、標準化に使用する変換方法を具体的に決定します。\n#（これにより、評価用データに対しても学習データと同じルールで変換できます）\nrecp_nm &lt;- rec_nm |&gt; recipes::prep()\n\n#標準化したデータを別の変数に格納します。\ndf_train_nm &lt;- recp_nm |&gt; recipes::bake(new_data = df_train)\ndf_train_X_nm &lt;- dplyr::select(df_train_nm, -medv)\n\ndf_test_nm &lt;- recp_nm |&gt; recipes::bake(new_data = df_test)\ndf_test_X_nm &lt;- dplyr::select(df_test_nm, -medv)\n\nまた、genlassoパッケージには切片項を明示的に追加する機能がないため、 学習用データに全件1を入力した列を追加することで対応します。\nmodel.matrix関数は計画行列を作成するための汎用的な関数ですが、 今回の場合は以下のようにすることで、切片項の列を追加するのに使用できます。\n\ndf_train_1X_nm &lt;- model.matrix(~ ., df_train_X_nm)\n\n次に、罰則項の係数行列 D の設定をします。 通常のラッソ回帰の場合は単位行列 I を指定した場合に対応しますが、 切片項の分だけ D の列数も1列増やしておく必要があります。\n\nD &lt;- diag(1, ncol(df_train_X)+1)\n\nさらに、切片項に対しては罰則項の対象外とするため、1行目は取り除きます。\n\nD &lt;- D[-1,]\n\nこれで、回帰問題の設定に用いる変数は全て用意できました。\n後は、genlasso関数に目的変数の実測値y、説明変数の実測値（計画行列）X、 罰則項の係数行列Dを入力することで回帰モデルが構築できます。\n\nmodel_genlasso &lt;- genlasso(y = df_train_y,\n                           X = df_train_1X_nm, \n                           D = D)\n\n構築したモデルに対してplot関数を適用すると、 λによってどのように係数が変化するか（解パス）をプロットすることができます。\n\nplot(model_genlasso)\n\n\n\n\n\n\n\n\nこのプロットにより、正則化によってどのように変数選択されていくかを可視化することができます。\n今回のようにモデリングで使用する部分がグラフの左の方に固まってしまうケースでは その部分を拡大して観察したほうが良いように思われますが、 標準ではこのグラフを拡大する方法は用意されていません。\nグラフの範囲は $lambda[1] によって決定されているので、 これを上書きすることで強引に拡大することは可能です。\n\nmodel_genlasso_tmp &lt;- model_genlasso\nmodel_genlasso_tmp$lambda[1] &lt;- 1*nrow(df_train_X)\nplot(model_genlasso_tmp)\n\n\n\n\n\n\n\nmodel_genlasso_tmp$lambda[1] &lt;- 0.1*nrow(df_train_X)\nplot(model_genlasso_tmp)\n\n\n\n\n\n\n\n\nまた、λと説明変数の個数の関係はsummary関数でも確認できます。\n\nsummary(model_genlasso)\n\n   df    lambda     rss\n    1   2511.89   30958\n    2   2082.40   25737\n    3   1072.16   15226\n    4    404.69   10990\n    5    294.27   10549\n    6    291.33   10538\n    7    248.56   10355\n    8    173.02    9839\n    9     94.48    9210\n   10     94.16    9208\n   11     87.99    9131\n   12     54.55    8764\n   13     30.97    8507\n\n\n特定のλでの係数を確認するには、coef関数を使用します。\n\ncoef(model_genlasso, lambda = c(0.01, 0.1, 1, 10)*nrow(df_train_X))\n\n$beta\n             3.79          37.9           379          3790\n [1,] 22.44828496  2.244828e+01  2.244828e+01  2.244828e+01\n [2,] -1.07583207 -7.910061e-01  9.508378e-16  4.149459e-15\n [3,]  0.82295065  4.282012e-01  3.012037e-16 -6.106227e-15\n [4,] -0.03736614 -1.739666e-01  3.363591e-16  3.171075e-15\n [5,]  0.47094307  4.555891e-01 -7.536400e-16  2.844947e-16\n [6,] -2.10978311 -1.679800e+00 -1.552797e-15  4.274359e-15\n [7,]  2.49902980  2.618992e+00  2.546729e+00 -1.122713e-14\n [8,] -0.16356541 -5.582320e-16  2.360366e-15  5.152129e-15\n [9,] -3.24170209 -2.466214e+00  2.635306e-16  4.676814e-15\n[10,]  2.63338674  1.281014e+00 -3.570671e-15  1.942890e-15\n[11,] -1.74110608 -5.748326e-01  3.045836e-15 -7.410739e-15\n[12,] -2.15187674 -2.025676e+00 -1.220700e+00 -5.925815e-15\n[13,]  0.68719108  6.315578e-01  5.366666e-02 -2.997602e-15\n[14,] -3.84759123 -3.899315e+00 -3.662125e+00 -2.975051e-15\n\n$lambda\n[1]    3.79   37.90  379.00 3790.00\n\n$df\n[1] 14 13  5  1\n\n\nこの回帰モデルで予測を行うには、predict関数を使用します。\n\n#λの値はそれなりの精度になるものを手で検証して設定しました。\nlambda_gen &lt;- 0.027 * nrow(df_train_X)\npreds &lt;- predict(\n  object = model_genlasso,\n  Xnew = model.matrix(~ ., df_test_X_nm),\n  lambda = lambda_gen #λの値を引数lambdaに与えます。\n)\npreds &lt;- c(preds$fit)# 予測値を取り出します。\n\n#精度評価のため、RMSEを計算します。\n.rmse &lt;- round(yardstick::rmse_vec(df_test_y, preds), 3)\n\n#横軸に目的変数の実測値、縦軸に予測値をプロットします。\nggplot(mapping = aes(x = df_test_y, y = preds)) +\n  geom_point() +\n  ggtitle(paste(\"Lasso @\", lambda_gen, \" | RMSE:\", .rmse))\n\n\n\n\n\n\n\n\n\n\nλのチューニング\n予測モデルとして用いる場合、 ラッソ回帰における \\lambda を決定する方法として交差検証法（CV:Cross Validation）が考えられます。 今回はこれを実装してみましょう。\n本パッケージの特徴として、1回のモデル構築で全てのλに対する解が得られるため、 λの候補数だけモデル構築を繰り返す必要はありません。 例えば 5-fold 交差検証法の場合は、累計5回モデル構築を行えば十分です。\n\nset.seed(2024)\n#k-fold 交差検証法に用いるデータセットを用意します。\n#学習用データを5等分したうえで、\n#そのうち4個（分析セット）でモデル構築 → 1個（検証セット）で精度確認　を5回繰り返すものですが、\n#以下の関数により、分析セット・検証セットの組を計5組用意することができます。\nsplits_cv &lt;- rsample::vfold_cv(df_train, v = 5)\n\n#各foldにおける予測モデルを格納するリスト\nmodels_cv &lt;- list()\n#各fold, λにおける結果を格納するデータフレーム\nresults_cv &lt;- tibble::tibble(id = character(), lambda = numeric(), rmse = numeric())\n#λの候補を設定します。\nlambdas &lt;- c(0, 10^(seq(-100, 0, 1)*3/100)) * nrow(df_train)\n\n#5回モデル構築を繰り返します。\nfor(i in 1:nrow(splits_cv)){\n  #i番目の分析セットを抽出します。\n  split &lt;- splits_cv$splits[[i]]\n  df_analysis &lt;- rsample::analysis(split)\n  df_analysis_nm &lt;- recp_nm |&gt; recipes::bake(new_data = df_analysis)\n  df_analysis_X_nm &lt;- dplyr::select(df_analysis_nm, -medv)\n  df_analysis_y &lt;- df_analysis$medv\n\n  #i版目の分析セットでモデルを構築し、出来上がったモデルをリストに格納します。\n  models_cv[[splits_cv$id[[i]]]] &lt;- \n    genlasso(y = df_analysis_y,\n             X = model.matrix(~ ., df_analysis_X_nm), \n             D = D)\n}\n\n構築した5つのモデルに対して、事前に用意したλの候補に対する精度評価を行います。 評価指標（ここではRMSEを使用します）の平均が良好なλを、最終的なλとして採用します。\n\nfor(i in 1:nrow(splits_cv)){\n  #i番目の検証セットを抽出します。\n  split &lt;- splits_cv$splits[[i]]\n  df_assessment &lt;- rsample::assessment(split)\n  df_assessment_nm &lt;- recp_nm |&gt; recipes::bake(new_data = df_assessment)\n  df_assessment_X_nm &lt;- dplyr::select(df_assessment_nm, -medv)\n  df_assessment_1X_nm &lt;- model.matrix(~ ., df_assessment_X_nm)\n  df_assessment_y &lt;- df_assessment$medv\n  \n  #i番目のモデルに対して、検証セットにおける予測精度を確認します。\n  model &lt;- models_cv[[splits_cv$id[[i]]]]\n  \n  #λは事前に設定した候補(lambdas)分だけ検証します。\n  for(lambda in lambdas){\n    preds &lt;- predict(\n      object = model,\n      Xnew = df_assessment_1X_nm,\n      lambda = lambda\n    )\n    preds &lt;- c(preds$fit)\n    .rmse &lt;- yardstick::rmse_vec(df_assessment_y, preds)\n    \n    #評価結果をデータフレームに記録します。\n    results_cv &lt;- dplyr::bind_rows(\n      results_cv,\n      tibble::tibble(id = splits_cv$id[[i]],\n                     lambda = lambda,\n                     rmse = .rmse))\n  }\n}\n\n#各λでRMSEの平均値を計算し、昇順にソートします。\nresults_tmp &lt;- results_cv |&gt;\n  dplyr::group_by(lambda) |&gt;\n  dplyr::summarise(mean_rmse = mean(rmse)) |&gt;\n  dplyr::arrange(mean_rmse)\n\n#結果を表示します。\nresults_tmp |&gt; head()\n\n\n  \n\n\n\nこのλにおける予測精度は次のとおりです。\n\nlambda_best &lt;- results_tmp$lambda[[1]]\n\npreds &lt;- predict(\n  object = model_genlasso,\n  Xnew = model.matrix(~ ., df_test_X_nm),\n  lambda = lambda_best\n)\npreds &lt;- c(preds$fit)\n.rmse &lt;- round(yardstick::rmse_vec(df_test_y, preds), 3)\nggplot(mapping = aes(x = df_test_y, y = preds)) +\n  geom_point() +\n  ggtitle(paste(\"Lasso @\", round(lambda_best, 3), \" | RMSE:\", .rmse))\n\n\n\n\n\n\n\n\n\n\n1次元フューズドラッソ\n\nビニングを行う場合\nフューズドラッソとは罰則項を\\lambda \\sum_{i=1}^{k-1} | \\beta_{i+1} - \\beta_{i}  | としたもののことで、 隣接した説明変数同士の係数が同じ値に近づくような作用を与えます。\n今回は例として、説明変数lstatと目的変数の関係に着目することとします。\n横軸にlstat2、縦軸に目的変数をとって散布図を描くと次のとおりです。\n\nlambda &lt;- lambda_best\nggplot(mapping = aes(x = df_train_X_nm$lstat, y = df_train_y)) +\n  geom_point() +\n  ggtitle(paste(\"Lasso @\", round(lambda, 3), \" | RMSE:\", .rmse))\n\n\n\n\n\n\n\n\nこの2変数の関係を、フューズドラッソにより要約して捉えることを考えましょう。\nまず、lstatの値を離散化します。\n値域をいくつかの領域（ビン）に区分し、 それぞれの領域に属しているときに1、そうでないときに0というダミー変数をビンの数だけ作成します。 なお、このような処理をビニングといいます。\n\nX_raw &lt;- df_train_X_nm$lstat\n#領域を区切る点を指定します。\nbreaks &lt;- seq(min(X_raw), max(X_raw), length.out = 21) #20等分\n\n#各行に対する、ダミー変数化の処理をを定義します。\nbreaks_r &lt;- c(breaks, +Inf)\nfn_discretize &lt;- function(x) {\n  as.integer(x &gt;= breaks_r[-length(breaks_r)] & x &lt; breaks_r[-1])\n}\n#この処理を、全ての行で同時に行います。\nX &lt;- sapply(X_raw, fn_discretize)\n#行と列が入れ替わってしまうため、転置します。\nX &lt;- t(X)\n\n#もともとのlstatと、各ダミー変数の値を横に並べて表示します。\nhead(cbind(X_raw, X))\n\n           X_raw                                          \n[1,]  2.53339277 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n[2,]  0.09071772 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[3,] -0.76778399 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[4,] -0.07986406 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[5,] -0.72024481 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[6,] -0.88523374 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\n同じことを後で評価用データにも行う必要があるので、関数化しておきます。\n\nfn_getdisclstat &lt;- function(df_X_nm){\n  X &lt;- df_X_nm$lstat\n  X &lt;- sapply(X, fn_discretize)\n  t(X)\n}\n\n計画行列はこれで完成です。 フューズドラッソに対応する D はgetD1d関数で取得できるため、 次のように指定することで実行可能ですが……\n\nmodel_fusedlasso1d &lt;- genlasso(y = df_train_y, X = X, D = getD1d(ncol(X)))\n\nフューズドラッソの場合はより実行が簡単で、かつ効率的なアルゴリズムを用いることが出来る fusedlasso1d関数が用意されています。\n\nmodel_fusedlasso1d &lt;- fusedlasso1d(y = df_train_y, X = X)\n\nplot関数により、フューズドラッソの係数（要約した結果）をプロットすることができます。\n\nplot(model_fusedlasso1d, lambda = lambda)\n\n\n\n\n\n\n\n\n予測の方法などはgenlasso関数を用いた場合と同じです。\n\nlambda_fused &lt;- 0.027 * nrow(df_train)\n\npreds &lt;- predict(\n  object = model_fusedlasso1d,\n  Xnew = fn_getdisclstat(df_train_X_nm),\n  lambda = lambda_fused\n)\npreds &lt;- data.frame(lstat = df_train_X_nm$lstat, y = df_train_y, source = \"actual\") |&gt;\n  rbind(data.frame(lstat = df_train_X_nm$lstat, y = c(preds$fit), source = \"model\"))\nggplot(preds, mapping = aes(x = lstat, y = y, color = source)) +\n  geom_point() +\n  ggtitle(paste(\"1 Var Fused Lasso @\", round(lambda_fused, 3)))\n\n\n\n\n\n\n\n\nこの1変数だけの予測モデルの評価用データにおける予測精度を確認すると次のとおりです。\n\npreds &lt;- predict(\n  object = model_fusedlasso1d,\n  Xnew = fn_getdisclstat(df_test_X_nm),\n  lambda = lambda\n)\npreds &lt;- c(preds$fit)\n.rmse &lt;- round(yardstick::rmse_vec(df_test_y, preds), 3)\nggplot(mapping = aes(x = df_test_y, y = preds)) +\n  geom_point() +\n  ggtitle(paste(\"1 Var Fused Lasso @\", lambda, \" | RMSE:\", .rmse))\n\n\n\n\n\n\n\n\n\n\nビニングを行わない場合\nラッソ回帰の場合はモデルの係数の個数が非常に多くとも問題ないという特徴があるため、 ビニングしないでそのままのデータを用いることも考えられます。\nXを省略した場合はyの長さと同じ数だけの説明変数が用意されます。 このとき、「隣り合う説明変数」を正しく認識させるため、 yを元の説明変数の順序で並び替えておきます。\n\n#見た目がそれなりに滑らかになるλに設定します。\nlambda_fused2 &lt;- 0.3 * nrow(df_train)\n#説明変数の順序で並べ替えます。\ny_sorted &lt;- sort_by(df_train_y, X_raw)\n#モデルを構築します。\nmodel_fusedlasso1d_a &lt;- fusedlasso1d(y = y_sorted)\n#構築したモデルと元のデータを重ねてプロットします。\nplot(model_fusedlasso1d_a, lambda = lambda_fused2)\n\n\n\n\n\n\n\n\n引数posで位置情報を与えることも可能ですが、 フューズドラッソの場合は特に計算結果に影響しません。\n\n#昇順という制限があるため、ソートしておきます。\npos &lt;- sort(X_raw)\n#重複なしという制限があるため、重複している箇所はわずかに数値をずらします。\npos &lt;- pos + seq(0, 1e-7, length.out = length(X_raw))\n\nmodel_fusedlasso1d_ap &lt;- fusedlasso1d(y = y_sorted, pos = pos)\nplot(model_fusedlasso1d_ap, lambda = lambda_fused2)\n\n\n\n\n\n\n\n\n\n\n\nトレンドフィルター\n罰則項を \\lambda \\sum_{i=1}^{k-2} | (\\beta_{i+2} - \\beta_{i+1}) - (\\beta_{i+1} - \\beta_{i}) | とすることで、 隣接した「説明変数の差」が同じ値に近づくような作用を与えることができます。 一般化ラッソ回帰問題にて D を次のように設定することでこの問題に帰着します。\n\nD = \\begin{bmatrix}\n1 & -2 & 1 & 0 & \\cdots & 0 & 0 & 0\\\\\n0 & 1 & -2 & 1 & \\cdots & 0 & 0 & 0 \\\\\n&  & \\vdots &  & \\ddots & & \\vdots &  \\\\\n0 & 0 & 0 & 0 & \\cdots & 1 & -2 & 1 \\\\\n\\end{bmatrix}\n\n「なるべく傾きの変わらない（少ない数の）直線でつなぐ」ことで、長期的なトレンドを抽出することができます。\ngenlasso関数にて引数DをgetDtfPosSparse関数で与えることでも実行できますが、 より実行が簡単で、かつ効率的なアルゴリズムを用いることが出来る trendfilter関数が用意されています。\n\nmodel_trendfilter1d&lt;- trendfilter(y = y_sorted, ord = 1)\nplot(model_trendfilter1d, lambda = lambda_fused2)\n\n\n\n\n\n\n\n\nさらに高階の差分を考えることもできます。\nつまり、罰則項を \\lambda \\sum_{i=1}^{k-3} | \\beta_{i+3} - 3\\beta_{i+2} + 3\\beta_{i+1} - \\beta_{i} | とすることで「なるべく少ない数の放物線でつなぐ」（2次のトレンドフィルター）、 罰則項を \\lambda \\sum_{i=1}^{k-4} | \\beta_{i+4} - 4\\beta_{i+3} + 6\\beta_{i+2} - 4\\beta_{i+1} + \\beta_{i}| とすることで「なるべく少ない数の3次曲線でつなぐ」（3次のトレンドフィルター）という回帰問題とすることができます。\n引数ordを変更することでこのような計算を行うことができます。 なお、ゼロの場合はフューズドラッソになります。\n\nmodel_trendfilter2d&lt;- trendfilter(y = y_sorted, ord = 2)\nplot(model_trendfilter2d, lambda = lambda_fused2)\n\n\n\n\n\n\n\nmodel_trendfilter3d&lt;- trendfilter(y = y_sorted, ord = 3)\nplot(model_trendfilter3d, lambda = lambda_fused2)\n\n\n\n\n\n\n\n\n各点の位置posを与えることも可能です。 ordが1以上の場合は、罰則項の係数が隣り合う点の距離に応じて調整される（「1/距離」倍だけ補整される）ため、 結果が前述したものとは異なってきます。\nただし、今回のデータのように点が密集している箇所がある場合、あまり意味のある結果にはなりません。\n\nmodel_trendfilter1d_p&lt;- trendfilter(y = y_sorted, ord = 1, pos = pos)\nplot(model_trendfilter1d_p, lambda = lambda_fused2)\n\n\n\n\n\n\n\nmodel_trendfilter2d_p&lt;- trendfilter(y = y_sorted, ord = 2, pos = pos)\nplot(model_trendfilter2d_p, lambda = lambda_fused2)\n\n\n\n\n\n\n\nmodel_trendfilter3d_p&lt;- trendfilter(y = y_sorted, ord = 3, pos = pos)\nplot(model_trendfilter3d_p, lambda = lambda_fused2)\n\n\n\n\n\n\n\n\nなお、このtrendfilterに対しては 交差検証法（CV）でλをチューニングするcv.trendfilterという関数が存在しています。\n\nmodel_trendfilter0d&lt;- trendfilter(y = y_sorted, ord = 0)\ncv &lt;- cv.trendfilter(model_trendfilter0d)\n\nFold 1 ... Fold 2 ... Fold 3 ... Fold 4 ... Fold 5 ... \n\nplot(model_trendfilter0d, lambda=cv$lambda.min, \n     main=paste0(\"lambda = \", cv$lambda.min))\n\n\n\n\n\n\n\n\n\n\n2次元フューズドラッソ\n2次元の数値データに対しては、 水平方向と垂直方向の両方の隣接関係に対する係数の差分を罰則とするよう D を設定することで、 例えば画像のノイズ除去のような処理を行うこともできます。\nこのような設定でのフューズドラッソ回帰を行う関数fusedlasso2dが用意されています。 実際に、この2次元フューズドラッソにより画像のノイズ除去を行ってみましょう。\nまず、サンプル画像を読み込みます。\n\nimg_raw &lt;- png::readPNG(system.file(\"img\", \"Rlogo.png\", package=\"png\"))\nstr(img_raw)\n\n num [1:76, 1:100, 1:4] 0 0 0 0 0 0 0 0 0 0 ...\n\n\nサンプルデータは、縦76ピクセル、横100ピクセルの画像ファイルです。 各ピクセルはRGB（赤、緑、青）の3色の強さと、 不透明度を表すアルファチャンネルを合わせた4つのデータの組で表されるため、 縦ピクセル数×横ピクセル数×4 の3次元配列となってデータが格納されています。\nこれをRで描画するためにはrasterImage関数を用います。\n\nfn_plotimg &lt;- function(img, ...){\n  img_width &lt;- dim(img)[2]\n  img_height &lt;- dim(img)[1]\n  \n  par_old &lt;- par(mar=c(1, 1, 1, 1)) #余白の調整\n  if (img_width &gt; img_height){\n    plot(0:img_width, type='n', xlab = \"\", ylab = \"\", axes = FALSE, ...)\n    rasterImage(img, 0, (img_width - img_height)/2, img_width, \n                img_height + (img_width - img_height)/2)\n  }else{\n    plot(0:img_height, type='n', xlab = \"\", ylab = \"\", axes = FALSE, ...)\n    rasterImage(img, (img_height - img_width)/2, 0, \n                img_width + (img_height - img_width)/2, img_height)\n  }\n  par(par_old) #余白をもとに戻す\n}\nfn_plotimg(img_raw, main = \"Raw Image\")\n\n\n\n\n\n\n\n\nこれに、正規分布に基づくノイズを付加してみます。\n\nset.seed(42)\nimg_noisy &lt;- img_raw\nimg_noisy &lt;- img_noisy + array(rnorm(length(img_noisy), sd = 0.3), dim(img_noisy))\nimg_noisy[img_noisy &gt; 1] &lt;- 1\nimg_noisy[img_noisy &lt; 0] &lt;- 0\nfn_plotimg(img_noisy, main = \"Noisy Image\")\n\n\n\n\n\n\n\n\nRGB、アルファチャンネルの4チャンネルそれぞれに対して、 2次元フューズドラッソによるノイズ除去を試みます。\nただし、回帰モデルの説明変数の数が「縦ピクセル数×横ピクセル数」となることから、 この画像全体に対して1つのモデルを構築しようとするとモデル構築に時間がかかってしまうため、 画像の一部を切り出してその部分にのみ処理を行うこととします。\nλによってノイズと判断して除去される凸凹が変化することが分かります。 今回の例の場合、0.1～0.3あたりがちょうど良いようです。\n\n#後で全体のノイズ除去を試みるにあたり、\n#区画を size_window × size_window ピクセルずつに分けることとします。\nsize_window &lt;- 15\n#ただし、区画の境界で不自然な線が現れないよう、\n#区画の右側と下側は size_pad ピクセル分広くとった領域でモデルを構築します。\nsize_pad &lt;- 3\n\n#モデルを格納するリストを用意します。\nmodels_fusedlasso2d &lt;- list()\n\nc &lt;- 1 #色のチャンネル\ni &lt;- 2 #Y軸方向のインデックス\nj &lt;- 3 #X軸方向のインデックス\n\n#ノイズ除去を行う領域(上下左右の端)を設定します。\nx1 &lt;- (j - 1) * size_window + 1\ny1 &lt;- (i - 1) * size_window + 1\nx2 &lt;- min(x1 + size_window + size_pad - 1, dim(img_raw)[2])\ny2 &lt;- min(y1 + size_window + size_pad - 1, dim(img_raw)[1])\n\n#cat(\"i = \",i, \", j = \",j, \", c = \",c,\"\\n\")\n#全体の画像から対象領域を取り出します。\nimg_tmp &lt;- img_noisy[y1:y2,x1:x2,c]\n\nmodels_fusedlasso2d[[i]] &lt;- list()\nmodels_fusedlasso2d[[i]][[j]] &lt;- list()\n#2次元フューズドラッソ回帰モデルを構築します。\nmodels_fusedlasso2d[[i]][[j]][[c]] &lt;- fusedlasso2d(y = img_tmp)\n\n#2x2で画像を表示します。\npar_old &lt;- par(mfrow = c(2, 2))\nfn_plotimg(img_raw[y1:y2,x1:x2,c], main = \"Raw Image\")\nfn_plotimg(img_noisy[y1:y2,x1:x2,c], main = \"Noisy Image\")\n\n#各λの結果を並べて表示します。\nfor(lambda in c(0.01, 0.03, 0.1, 0.3, 1, 3)){\n  co &lt;- coef(models_fusedlasso2d[[i]][[j]][[c]], lambda = lambda)\n  img_processed_tmp &lt;- array(co$beta, c(dim(img_tmp)[1], dim(img_tmp)[2]))\n  fn_plotimg(img_processed_tmp, main = paste0(\"Processed Image, lambda = \",lambda))\n}\n#描画設定をもとに戻します。\npar(par_old)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n全体にこれを適用すると次のようになります。\n今回の例のようなそれほど大きくない画像でも数十分程度の時間がかかるため、実行時は注意してください。\n\nmodels_fusedlasso2d &lt;- list()\nni &lt;- ((dim(img_raw)[1]-size_pad-1) %/% size_window + 1)\nnj &lt;- ((dim(img_raw)[2]-size_pad-1) %/% size_window + 1)\nfor(i in 1:ni){\n  models_fusedlasso2d[[i]] &lt;- list()\n  for(j in 1:nj){\n    models_fusedlasso2d[[i]][[j]] &lt;- list()\n    \n    x1 &lt;- (j - 1) * size_window + 1\n    y1 &lt;- (i - 1) * size_window + 1\n    x2 &lt;- min(x1 + size_window + size_pad - 1, dim(img_raw)[2])\n    y2 &lt;- min(y1 + size_window + size_pad - 1, dim(img_raw)[1])\n    for(c in 1:dim(img_raw)[3]){\n      #cat(\"i = \",i, \", j = \",j, \", c = \",c,\"\\n\")\n      img_tmp &lt;- img_noisy[y1:y2,x1:x2,c]\n      models_fusedlasso2d[[i]][[j]][[c]] &lt;- fusedlasso2d(y = img_tmp)\n    }\n  }\n}\n\npar_old &lt;- par(mfrow = c(2, 2))\nfn_plotimg(img_raw, main = \"Raw Image\")\nfn_plotimg(img_noisy, main = \"Noisy Image\")\nimgs_processed &lt;- list()\nfor(lambda in c(0.01, 0.03, 0.1, 0.3, 1, 3)){\n  img_processed &lt;- array(1, dim = dim(img_raw))\n  for(c in 1:dim(img_raw)[3]){#\n    img_processed_tmp &lt;- array(NA, dim = c(dim(img_raw)[1:2], ni*nj))\n    for(i in 1:ni){\n      for(j in 1:nj){\n        x1 &lt;- (j - 1) * size_window + 1\n        y1 &lt;- (i - 1) * size_window + 1\n        x2 &lt;- min(x1 + size_window + size_pad - 1, dim(img_raw)[2])\n        y2 &lt;- min(y1 + size_window + size_pad - 1, dim(img_raw)[1])\n        co &lt;- coef(models_fusedlasso2d[[i]][[j]][[c]], lambda = lambda)\n        img_processed_tmp[y1:y2,x1:x2,nj*(i-1)+j] &lt;- array(co$beta, c(y2-y1+1, x2-x1+1))\n      }\n    }\n    #size_padによって広げた箇所は複数モデルの結果があるため、これらは単純平均します。\n    img_tmp &lt;- apply(img_processed_tmp, 1:2, mean, na.rm = T)\n    img_tmp[is.na(img_tmp)] &lt;- 1\n    img_processed[,,c] &lt;- img_tmp\n  }\n  imgs_processed[[as.character(lambda)]] &lt;- img_processed\n  fn_plotimg(imgs_processed[[as.character(lambda)]], main = paste0(\"Processed Image, lambda = \",lambda))\n}\npar(par_old)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>genlasso</span>"
    ]
  },
  {
    "objectID": "articles/genlasso.html#他パッケージとの比較",
    "href": "articles/genlasso.html#他パッケージとの比較",
    "title": "genlasso",
    "section": "他パッケージとの比較",
    "text": "他パッケージとの比較\n\nglmnet\nglmnet パッケージは正則化GLMのモデリングに用いられるパッケージです。\ngenlasso パッケージと比較すると次のような特徴があります。\n\nglmnet パッケージはGLMを取り扱うもので、よく使われるリンク関数や残差分布（ガウス分布以外）に対応しています。genlasso パッケージは通常の線形回帰問題のみです。\ngenlasso パッケージ利用時は、標準化や切片項の追加を自分で行う必要がありましたが、glmnet パッケージでは自動で行えます。\nglmnet パッケージは各観測に対する重み付けにも対応しています。\nglmnet パッケージも genlasso パッケージ同様、一度の学習で全てのλに対する解を得ることができます。\nglmnet パッケージは、交差検証法（CV）によるλのチューニングにも対応しています。genlasso パッケージの場合、同機能があるのはトレンドフィルターのみです。\nglmnet パッケージは、ラッソ正則化のみならず、リッジ（2 - ノルムによる罰則項）やエラスティックネット（ラッソとリッジ両方の罰則項を持つもの）にも対応しています。一方、D\\beta の形の罰則項には対応していません。\n\nこのように、予測モデリングに便利な機能が多数備わっているため、 glmnetパッケージが使用できる状況（罰則項が通常のラッソ回帰である場合）ではこちらの方が有用なケースが多いでしょう。\n実際、 通常のラッソ回帰問題 節で述べた例は（λのチューニングも含めて） 次のとおり簡単に記述することができます。\n\nlibrary(glmnet)\n#CVで最適なλを求めながらモデルを構築します。\nset.seed(42)\nmodel_cv.lasso &lt;- glmnet::cv.glmnet(\n  #matrix型である必要があるため変換します。\n  x = as.matrix(df_train_X_nm),\n  y = df_train_y,\n  #正則化項のαを指定（1：ラッソ、0：リッジ）します。\n  alpha = 1, \n  #CVの分割数を指定します。\n  nfolds = 5 \n)\n#最適なλを抽出します。\nlambda_glm &lt;- model_cv.lasso$lambda.min |&gt; round(3)\n\n#予測精度の確認のため、評価用データでの予測値を取得します。\npreds &lt;- predict(\n  object = model_cv.lasso,\n  #matrix型である必要があるため変換します。\n  newx = as.matrix(df_test_X_nm), \n  #λは引数sに与えます（lambdaではありません）。\n  s = lambda_glm\n) |&gt; c()\n.rmse &lt;- round(yardstick::rmse_vec(df_test_y, preds), 3)\nggplot(mapping = aes(x = df_test_y, y = preds)) +\n  geom_point() +\n  ggtitle(paste(\"Lasso @\", lambda_glm, \" | RMSE:\", .rmse))\n\n\n\n\n\n\n\n#λごとのCVの結果をプロットします。\nplot(model_cv.lasso)\n\n\n\n\n\n\n\n#λ別の係数プロット（解パス）をプロットします。\nplot(model_cv.lasso$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n#指定したλにおける係数を出力します。\n#genlassoの係数とほぼ同等となっていることがわかります。\ncoef(model_cv.lasso$glmnet.fit, s = c(0.01, 0.1, 1, 10))\n\n14 x 4 sparse Matrix of class \"dgCMatrix\"\n                     s1         s2          s3       s4\n(Intercept) 22.44828496 22.4482850 22.44828496 22.44828\ncrim        -1.07290435 -0.7884986  .           .      \nzn           0.81966961  0.4262189  .           .      \nindus       -0.03945084 -0.1777379  .           .      \nchas         0.47170645  0.4563839  .           .      \nnox         -2.10738128 -1.6769990  .           .      \nrm           2.50046418  2.6201820  2.54685806  .      \nage         -0.16251918  .          .           .      \ndis         -3.23779431 -2.4650921  .           .      \nrad          2.61941760  1.2670168  .           .      \ntax         -1.72933402 -0.5630096  .           .      \nptratio     -2.15099633 -2.0243315 -1.22146843  .      \nblack        0.68711370  0.6314864  0.05446536  .      \nlstat       -3.84813288 -3.8995978 -3.66282065  .      \n\n\nなお、glmnet パッケージで最小化される関数は次のようなものです。\n\n\\frac1n\\sum_{i=1}^nw_il\n\\left(y_i, \\beta_0 + \\sum_{j=1}^{k} \\left( x_{ij} \\cdot \\beta_j \\right) \\right)\n+ \\lambda \\left[ \\frac{(1-\\alpha)}2 \\| \\beta \\| _2^2 + \\alpha \\| \\beta \\|_1 \\right]\n\nただし、\\beta_0 はモデルの切片項（罰則項のノルムの計算には含まれません）、 l(y_i, \\eta_i) は i 番目の観測に対する負の対数尤度関数、 w = (w_i)_{i=1}^n は観測の重みを表すベクトル、 \\alpha \\in [0, 1] は罰則項の形状を表すハイパーパラメータです。\nリンク関数として恒等関数、残差分布としてガウス分布を選ぶと、 l(y_i, \\eta_i) = \\frac12 (y_i - \\eta_i)^2 となり、 また切片項 \\beta_0 = 0 、重み w を全て1、\\alpha = 1 （ラッソ回帰）とすることで genlasso が最小化する関数と同等の形式になります。\n\n\\frac1{2n}\\| y - X\\beta \\|_2^2 + \\lambda\\| \\beta \\|_1\n\nただし、第1項に係数 1/n がかかっている点のみ genlasso のものとは異なっています。 これは、第1項が表す残差が観測数に比例して増加する点を補整するためです。\nこのため、解として得られる λ のスケールが異なることに注意してください。 具体的には、genlasso 側の λ が観測数 n に比例して大きくなります 3。\n\n\naglm\naglm とは正則化 GLM の変種であるAGLM（Accurate Generalized Linear Model） Suguru Fujita et al. (2020) のモデリングを実装したパッケージです。\nAGLM は GLM とよく似たモデルですが、 連続変数に対しては ビニングを行う場合 で述べたような ビニングを伴うフューズドラッソと同等の変換を行います。 GLM のような加法的モデルであるという性質を維持しながら、 説明変数と目的変数の間の非線形な関係を表現することが可能になっています。\nここでダミー変数化の際に順序付きダミー変数（閾値以上なら 1 となるダミー変数）を利用することで、 フューズドラッソが通常の（罰則項を \\lambda \\| \\beta \\| とする）ラッソ回帰に帰着できるという性質を用いており、 ダミー変数化の処理を行った後の内部の計算処理に glmnet を用いているのが特徴です。 そのため、前述した glmnet の特徴（利点）がそのまま継承されています。\n次のようにすることで、ビニングを行う場合 の例を再現するモデルを構築することができます。\n\nlibrary(aglm)\nmodel_aglm_1v &lt;- aglm(\n  x = df_train_X_nm$lstat,\n  #ビンの端点をfusedlasso1dで使用したものと同じにします。\n  bins_list = list(X1 = breaks), \n  y = df_train_y,\n  add_linear_columns = FALSE,\n  #glmnetで説明変数を標準化しないようにします。\n  standardize = F, \n  #OD化する際に離散化する（線形補間しない）ようにします。\n  OD_type_of_quantitatives = \"J\", \n  alpha = 1 # 正則化項のαパラメーターを指定（1：ラッソ、0：リッジ）\n)\n#λはgenlassoの例から流用します。\nlambda_aglm &lt;- lambda_best / nrow(df_train_X)\n\n#予測精度の確認のため、評価用データでの予測値を取得します。\npreds &lt;- predict(\n  object = model_aglm_1v,\n  newx = df_test_X_nm$lstat,\n  s = lambda_aglm\n) |&gt; c()\n.rmse &lt;- round(yardstick::rmse_vec(df_test_y, preds), 3)\nggplot(mapping = aes(x = df_test_y, y = preds)) +\n  geom_point() +\n  ggtitle(paste(\"1 Var Fused Lasso by AGLM @\", round(lambda_aglm, 3),\n                \" | RMSE:\", .rmse))\n\n\n\n\n\n\n\n#関数の形をプロットします。\npar(mar = c(4, 4, 1, 1)) # プロット領域中の余白を調整\nplot(model_aglm_1v, verbose = FALSE, vars = NULL, s = lambda_aglm, ask = FALSE)\n\n\n\n\n\n\n\n\nこれは説明変数 lstat にのみ着目した1変数のモデルでしたが、 全変数を説明変数としてモデルを構築すると次のとおりです4。\n\n#CVで最適なλを求めながらモデル構築\nset.seed(42)\nmodel_cv.aglm &lt;- cv.aglm(\n  x = df_train_X,\n  y = df_train_y,\n  add_linear_columns = FALSE,\n  alpha = 1, # 正則化項のαパラメーターを指定（1：ラッソ、0：リッジ）\n  nfolds = 5 # クロスバリデーションの分割数を指定\n)\nlambda_aglm_best &lt;- model_cv.aglm@lambda.min |&gt; round(3)\n\n非線形な関係を捉えることができているため、 GLMと比べて予測精度が大幅に向上していることがわかります。\n\n#予測精度の確認\npreds &lt;- predict(\n  object = model_cv.aglm,\n  newx = df_test_X,\n  s = lambda_aglm_best\n) |&gt; c() # 行列形式の出力をベクトルに変換\n.rmse &lt;- round(yardstick::rmse_vec(df_test_y, preds), 3)\nggplot(mapping = aes(x = df_test_y, y = preds)) +\n  geom_point() +\n  ggtitle(paste(\"AGLM@\", lambda_aglm_best, \" | RMSE:\", .rmse))\n\n\n\n\n\n\n\n\n予測関数は説明変数の1変数関数の和で表現されているので、 説明変数ごとに予想関数を分けてプロットすることで 予測関数の形を把握することも可能です。\n\npar(mar = c(4, 4, 1, 1)) # プロット領域中の余白を調整\nplot(model_cv.aglm, verbose = FALSE, vars = NULL, \n     s = lambda_aglm_best, ask = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nこのように AGLM は予測精度と解釈可能性をある程度両立したモデルであり、 さらに glmnet パッケージの利点を多く継承していることから、 genlasso パッケージよりもこちらを利用したほうがよいケースも多いでしょう。",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>genlasso</span>"
    ]
  },
  {
    "objectID": "articles/genlasso.html#参考文献",
    "href": "articles/genlasso.html#参考文献",
    "title": "genlasso",
    "section": "参考文献",
    "text": "参考文献\n\n\nRyan J. Tibshirani, and Jonathan Taylor. 2011. “The Solution Path of the Generalized Lasso.” https://www.stat.cmu.edu/~ryantibs/papers/genlasso.pdf.\n\n\nSuguru Fujita, Toyoto Tanaka, Kenji Kondo, and Hirokazu Iwasawa. 2020. “A Hybrid Modeling Method of GLM and Data Science Techniques.” https://www.institutdesactuaires.com/global/gene/link.php?doc_id=16273&fg=1.\n\n\nTaylor B. Arnold, and Ryan J. Tibshirani. 2016. “Efficient Implementations of the Generalized Lasso Dual Path Algorithm.” https://www.stat.cmu.edu/~ryantibs/papers/fastgl.pdf.\n\n\nTaylor B. Arnold, and Ryan Tibshirani. 2019. “Introduction to the Genlasso Package.” https://ryantibs.r-universe.dev/genlasso/doc/article.pdf.\n\n\n岩沢 宏和, and 平松 雄司. 2019. 入門 Rによる予測モデリング. 東京図書.\n\n\n藤田 卓, 田中 豊人, and 岩沢 宏和. 2019. “アクチュアリー実務のためのデータサイエンスの技術を用いたglmの拡張.” https://www.jarip.org/publication/risk_and_insurance/pdf/RI_v15_045.pdf.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>genlasso</span>"
    ]
  },
  {
    "objectID": "articles/genlasso.html#footnotes",
    "href": "articles/genlasso.html#footnotes",
    "title": "genlasso",
    "section": "",
    "text": "ここでは切片項を省略して表示しています。↩︎\n以下断りが無ければ、通常のラッソ回帰問題 で標準化したものです。↩︎\n本稿ではこれを踏まえて、定数×観測数 の形で λ を指定している箇所が存在します。↩︎\n直上の例での標準化やビンの設定などは genlasso パッケージの結果を再現するために設定したものです。 以下の例では再現の必要が無いため、aglm パッケージのデフォルト値に戻しています。↩︎",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>genlasso</span>"
    ]
  },
  {
    "objectID": "articles/gghighlight.html",
    "href": "articles/gghighlight.html",
    "title": "gghighlight",
    "section": "",
    "text": "パッケージの概要\ngghighlight パッケージは、ggplot2 で作成したプロットの中から、特定の部分だけを強調表示するための機能に特化したツールです。使い方は非常にシンプルで、“ggplot” オブジェクトの最後のレイヤーとして gghighlight() 関数を追加し、強調したいデータの条件を指定するだけ利用できます。",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>gghighlight</span>"
    ]
  },
  {
    "objectID": "articles/gghighlight.html#プロットの一部を強調する",
    "href": "articles/gghighlight.html#プロットの一部を強調する",
    "title": "gghighlight",
    "section": "プロットの一部を強調する",
    "text": "プロットの一部を強調する\n\nlibrary(gghighlight)\n\nWarning: package 'gghighlight' was built under R version 4.5.1\n\n\n\n折れ線グラフの一部を強調する\n\nggplot(ChickWeight, aes(x = Time, y = weight, group = Chick)) +\n  geom_line(color='gray20') +\n  gghighlight(max(weight) &gt; 330)\n\n\n\n\n\n\n\n\n\n\n散布図の一部を強調する\n\nlibrary(tibble) # rownames_to_column() 関数を利用\nggplot(rownames_to_column(USArrests, 'State'), # 行の名前を列に変換\n       aes(x = Murder, y = Assault)) +\n  geom_point(col='darkcyan') + \n  gghighlight(grepl('New', State), label_key = State)\n\n\n\n\n\n\n\n\n\n\n棒グラフ、ヒストグラムの一部を強調する\n\nggplot(iris, aes(x = Sepal.Width, fill = Species)) +\n  geom_histogram(bins = 10) +\n  gghighlight() +\n  facet_wrap(~Species)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>gghighlight</span>"
    ]
  },
  {
    "objectID": "articles/gghighlight.html#参考資料",
    "href": "articles/gghighlight.html#参考資料",
    "title": "gghighlight",
    "section": "参考資料",
    "text": "参考資料\n[1] 湯谷啓明. “gghighlight の作者です。すべてをお話しします。”. パッケージの作者による解説スライド. https://yutani.quarto.pub/tokyor-109-gghighlight/",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>gghighlight</span>"
    ]
  },
  {
    "objectID": "articles/ggplot2.html",
    "href": "articles/ggplot2.html",
    "title": "ggplot2",
    "section": "",
    "text": "パッケージの概要\nggplot2は、一貫性のある文法を用いてグラフを作成することのできるパッケージであり、Rでモダンな分析環境を構築するためのパッケージ群である「tidyberse」に含まれます。 なお、ggplot2については、アクチュアリージャーナル第１１２号内の「Rを用いたデータの可視化技術解説書」の付録２でも紹介されています。\n\nsuppressMessages(require(tidyverse))\nrequire(ggplot2)\n\n\n\nグラフの作成方法\nggplot2では、+演算子を利用してggplot関数に様々な関数を接続することでグラフを作成します。 以下では、geom_point関数を用いて、Rのデータセットiris（アヤメの花びらやがく片の長さなどに関するデータセット）を用いた散布図を作成しています。\n\n# 利用するdataと、グラフに表示する特徴量を指定（あとで指定する場合は省略可能）\n# これだけでは何も表示されない\nP1 &lt;- ggplot(data = iris, mapping = aes(x=Sepal.Length, y=Sepal.Width))\nP1\n\n\n\n\n\n\n\n# geom_pointで散布図を表示。aes関数でcolourを指定すると色分けできる\nP2 &lt;- P1 + geom_point(aes(colour = Species))\nP2\n\n\n\n\n\n\n\n\n+演算子で接続できる関数は１つだけではなく、作成したグラフに対して、様々な要素を付加できます。\n\n# facet_gridでcolsを指定すると、グラフを分解可能\nP3 &lt;- P1 + geom_point() + facet_grid(cols = vars(Species))\nP3\n\n\n\n\n\n\n\n# グラフのタイトルをつけられる\nP4 &lt;- P3 +labs(title=\"title\")\nP4\n\n\n\n\n\n\n\n\n\n\nグラフの重ね合わせ\nggplot2では、グラフの重ね合わせも+演算子で実施することができ、先ほど作成した散布図に対して、再度geom_point関数を接続すれば重ね合わせたグラフが作成可能です。\n以下では、アヤメの種類別のがく片の長さと幅を計算したiris_meanを用いて、散布図を重ね合わせています（表示されたグラフの赤い点が平均値です）\n\niris_mean &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(across(c(Sepal.Length,Sepal.Width), ~mean(.x, na.rm = TRUE)))\niris_mean\n\n# A tibble: 3 × 3\n  Species    Sepal.Length Sepal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa             5.01        3.43\n2 versicolor         5.94        2.77\n3 virginica          6.59        2.97\n\nP5 &lt;- P4 + geom_point(\n  data = iris_mean,\n  colour = 'red',\n  size = 3\n)\nP5\n\n\n\n\n\n\n\n\n\n\n公式サイトについて\nどのようなグラフが作成することができるかについては、公式サイトから確認可能です。特に、レファレンスからは関数の一覧が確認でき、それぞれの関数の詳細にもアクセス可能です。\n\n公式サイトなどのアドレス\n公式サイト https://ggplot2.tidyverse.org/index.html\nレファレンス https://ggplot2.tidyverse.org/reference/index.html",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot2</span>"
    ]
  },
  {
    "objectID": "articles/igraph.html",
    "href": "articles/igraph.html",
    "title": "igraph",
    "section": "",
    "text": "パッケージの概要\nigraph は、グラフ（ネットワークグラフ）を扱うためのパッケージで、グラフ（igraph オブジェクト）の作成や基本的な分析を簡単なコードで実行することができます。\nlibrary(dplyr)\nlibrary(igraph)\n\nWarning: package 'igraph' was built under R version 4.5.1",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>igraph</span>"
    ]
  },
  {
    "objectID": "articles/igraph.html#グラフを作成する",
    "href": "articles/igraph.html#グラフを作成する",
    "title": "igraph",
    "section": "グラフを作成する",
    "text": "グラフを作成する\nmake_graph 関数の引数 edges に、グラフの頂点の名前を要素とするベクトルを渡すことで、igraph オブジェクトを作成することができます。edges に渡されたベクトルは c(1番目の辺の始点、1番目の辺の終点、2番目の辺の始点、2番目の辺の終点…) と解釈されます。\n\ng1 &lt;- make_graph(edges = c(1,2, 1,3, 2,3, 3,4))\nplot(g1)\n\n\n\n\n\n\n\n\n引数 directed を FALSE にすることで、無向グラフを作成することができます。また、plot 関数の引数を指定することで、プロットの見た目を柔軟に変更することができます。\n\ng2 &lt;- make_graph(edges = c(1,2, 1,3, 2,3, 3,4),\n                directed = FALSE)\nplot(g2, vertex.color = NA, vertex.size = 30,\n     vertex.label.cex = 1.5, vertex.label.color = \"#202020\")\n\n\n\n\n\n\n\n\ngraph_from_literal 関数では、X---Y, X--+Y などの直感的な記法によって、グラフの辺を指定することができます。ここで、--- は無向辺を表します。- の数に制限はなく、-- や ---- としても同じ結果を得られます。\n\ng3 &lt;- graph_from_literal(W---X:Y, X---Y, Y---Z)\nplot(g3)\n\n\n\n\n\n\n\n\nまた、--+ は有向辺を表します。- の数に制限はなく、-+ や ---+ としても同じ結果を得られます。\n\ng4 &lt;- graph_from_literal(W--+X:Y, X--+Y, Y--+Z)\nplot(g4, vertex.shape = \"none\", vertex.size = 30,\n     vertex.label.cex = 1.5, vertex.label.color = \"#202020\")",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>igraph</span>"
    ]
  },
  {
    "objectID": "articles/igraph.html#データフレームからグラフを作成する",
    "href": "articles/igraph.html#データフレームからグラフを作成する",
    "title": "igraph",
    "section": "データフレームからグラフを作成する",
    "text": "データフレームからグラフを作成する\ngraph.data.frame では、辺の情報を記録したデータフレームからグラフを作成することができます。第1引数 x に渡されたデータフレームの第1列と第2列が、辺の始点と終点として解釈されます。また、第3列目以降は、各辺の属性として用いられます。\n\ndf &lt;- tibble(\n  tails = c(1, 1, 2, 3),\n  heads = c(2, 3, 3, 4),\n  label = c('w', 'x', 'y', 'z'),\n  label.color = rep(\"#151515\", 4),\n  )\ng5 &lt;- graph_from_data_frame(df, directed = FALSE)\nplot(g5)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>igraph</span>"
    ]
  },
  {
    "objectID": "articles/igraph.html#頂点や辺を追加する",
    "href": "articles/igraph.html#頂点や辺を追加する",
    "title": "igraph",
    "section": "頂点や辺を追加する",
    "text": "頂点や辺を追加する\nadd_vertices 関数によって igraph オブジェクトに頂点を追加することができます。\n\ng &lt;- make_empty_graph(directed=FALSE)\ng &lt;- add_vertices(g, nv = 2, color = \"#303030\",\n                  label.color = \"white\")\ng &lt;- add_vertices(g, nv = 2, color = \"#E03030\",\n                  label.color = \"white\")\nplot(g, vertex.size = 25)\n\n\n\n\n\n\n\n\n同様に、add_edges 関数によって igraph オブジェクトに辺を追加することができます。2つの頂点の間に2本以上の辺を張ることもできます。\n\ng &lt;- add_edges(g, c(1,2, 2,3, 1,4), color = \"#303030\")\ng &lt;- add_edges(g, c(3,4, 4,1, 3,2), color = \"#E03030\")\nplot(g, vertex.size = 25)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>igraph</span>"
    ]
  },
  {
    "objectID": "articles/igraph.html#頂点や辺を削除する",
    "href": "articles/igraph.html#頂点や辺を削除する",
    "title": "igraph",
    "section": "頂点や辺を削除する",
    "text": "頂点や辺を削除する\ndelete_vertices 関数によって、頂点を削除することができます。なお、グラフに残される頂点の番号が連続になるように、削除された頂点より後ろの番号を持っていた頂点の番号が修正されます。\n\ng &lt;- delete_vertices(g, 3)\nplot(g, vertex.size = 25)\n\n\n\n\n\n\n\n\n同様に、delete_edges 関数によって、辺を削除することができます。\n\ng &lt;- delete_edges(g, \"1|2\")\nplot(g, vertex.size = 25)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>igraph</span>"
    ]
  },
  {
    "objectID": "articles/igraph.html#木や格子などの特別なグラフを作成する",
    "href": "articles/igraph.html#木や格子などの特別なグラフを作成する",
    "title": "igraph",
    "section": "木や格子などの特別なグラフを作成する",
    "text": "木や格子などの特別なグラフを作成する\nmake_tree 関数によって、頂点数 n、各頂点の子の数が children であるような木を作成することができます。\n\ng_tree &lt;- make_tree(n = 15, children = 2, mode = 'undirected')\nplot(g_tree,\n     layout = layout_as_tree(g_tree),\n     vertex.size = 25)\n\n\n\n\n\n\n\n\nmake_lattice 関数によって、格子グラフを作成することができます。\n\ng_lat &lt;- make_lattice(c(3, 3))\nplot(g_lat, vertex.size = 25)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>igraph</span>"
    ]
  },
  {
    "objectID": "articles/igraph.html#プロットのレイアウトを調整する",
    "href": "articles/igraph.html#プロットのレイアウトを調整する",
    "title": "igraph",
    "section": "プロットのレイアウトを調整する",
    "text": "プロットのレイアウトを調整する\nplot.igraph（igraph オブジェクトに plot を適用したときに呼び出される関数）では、layout を指定することで、グラフのレイアウトを指定することができます。レイアウトを作成するアルゴリズムは複数あり、igraph パッケージには layout_with_kk 関数や layout_with_fr 関数などが実装されています。layout_nicely 関数を用いると、グラフの特性に適したアルゴリズムによるレイアウトを出力することができます。\n\nplot(g_tree, layout = layout_nicely(g_tree), vertex.size = 25)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>igraph</span>"
    ]
  },
  {
    "objectID": "articles/iml.html",
    "href": "articles/iml.html",
    "title": "iml",
    "section": "",
    "text": "パッケージの概要\nimlパッケージは、予測モデルを解釈するために開発された可視化手法を統一的な記法で実行するためのパッケージです。imlパッケージでは、予測モデルにPredictor$new関数を適用することで、Predictorオブジェクトが作成されます。このPredictorオブジェクトに対して、imlパッケージのさまざまな関数を適用することで、PDP、ICE、SHAPなどのプロットを簡単に作成することができます。\nここに挙げた機械学習の解釈手法については、パッケージの作者 Christoph Molnar 氏によるウェブ書籍 Interpretable Machine Learning（邦訳） をご参照ください。\nここでは、非常に簡単な例として、treesデータセットのVolumeを、GirthとHeightという二つの特徴量から予測するモデルを作成し、そのモデルに対して解釈手法を適用してみましょう。\n# treesデータセットを読み込み、ランダムフォレストモデルを作成する。\ndata(trees)\nmodel_rf &lt;- ranger::ranger(Volume~., trees)\nX &lt;- dplyr::select(trees, -Volume)",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>iml</span>"
    ]
  },
  {
    "objectID": "articles/iml.html#モデル解釈のためのpredictorオブジェクトを生成する",
    "href": "articles/iml.html#モデル解釈のためのpredictorオブジェクトを生成する",
    "title": "iml",
    "section": "モデル解釈のためのPredictorオブジェクトを生成する",
    "text": "モデル解釈のためのPredictorオブジェクトを生成する\nPredictor$new()関数は、さまざまなパッケージのもとで作成された予測モデルを、imlパッケージの他のクラスや関数に対応するように加工するための関数です。加工後の予測モデルは、Predictorオブジェクトと呼ばれます。\n\nlibrary(iml)\npredictor &lt;- (\n  (Predictor$new(model_rf, data = X, y = trees$Volume))\n  )",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>iml</span>"
    ]
  },
  {
    "objectID": "articles/iml.html#特徴量と予測値の関係を解釈する",
    "href": "articles/iml.html#特徴量と予測値の関係を解釈する",
    "title": "iml",
    "section": "特徴量と予測値の関係を解釈する",
    "text": "特徴量と予測値の関係を解釈する\nPredictorオブジェクトにFeatureEffect$new()関数を適用すると、ICEプロット、PDプロット、ALEプロットなどを作図することができます。\n\nICE（Individual Conditional Expectation）プロットは、注目している特徴量の値だけが違っていた場合に予測値がどのように変化するかを、個々の予測ごとに可視化するものです。\nPD（Partial Dependence）プロットは、データ全体のICEプロットを平均したものにほかならず、注目している特徴量の値が変化したときに予測値が平均的にどのように変化するかを表していると解釈できます。\nALE（Accumulated Local Effects）プロットは、PDと同様、注目している特徴量の変化に対応する予測値の変化を表します。PDでは、注目している特徴量の値 x によらずにデータ全体の分布を使って予測値の変化を平均しますが、ALEでは、特徴量の値が x であるときの効果を、データの中でその特徴量の値が x に近いものだけを抽出した部分データにおける予測値の変化を平均することによって計算します。\n\n\nlibrary(dplyr) # データ処理に利用\nlibrary(ggplot2) # プロットの作成や調整に利用\n\n# iceプロットを作図する。\nice &lt;- predictor %&gt;%\n  FeatureEffect$new(feature = \"Girth\", method = \"ice\")\nice$plot() + labs(title = \"ICE plot\")\n\n\n\n\n\n\n\n# pdプロットを作図する。\npdp &lt;- predictor %&gt;%\n  FeatureEffect$new(feature = \"Girth\", method = \"pdp+ice\")\npdp$plot() + labs(title = \"PDP plot\")\n\n\n\n\n\n\n\n# aleプロットを作図する。\nale &lt;- predictor %&gt;%\n  FeatureEffect$new(feature = \"Girth\", method = \"ale\")\nale$plot() + labs(title = \"ALE plot\")",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>iml</span>"
    ]
  },
  {
    "objectID": "articles/iml.html#個別の予測における特徴量の寄与を解釈する",
    "href": "articles/iml.html#個別の予測における特徴量の寄与を解釈する",
    "title": "iml",
    "section": "個別の予測における特徴量の寄与を解釈する",
    "text": "個別の予測における特徴量の寄与を解釈する\nPredictorオブジェクトにShapley$new()関数を適用すると、SHAP（SHapley Additive exPlanation）プロットを作図することができます。SHAP は、個別の予測値と平均的な予測値との差を、ゲーム理論的手法によって特徴量ごとの寄与に分解したものです。ここでは、5 番目のインスタンスに対する予測への特徴量ごとの寄与を表示してみます。HAPプロットを作図することができます。\n\nshap &lt;- predictor %&gt;%\n  Shapley$new(x.interest = X[5, ])\nshap$plot() + theme_light() + theme(legend.position='null')",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>iml</span>"
    ]
  },
  {
    "objectID": "articles/iml.html#モデルにおける特徴量の重要度を解釈する",
    "href": "articles/iml.html#モデルにおける特徴量の重要度を解釈する",
    "title": "iml",
    "section": "モデルにおける特徴量の重要度を解釈する",
    "text": "モデルにおける特徴量の重要度を解釈する\nPredictorオブジェクトにFeatureImp$new()関数を適用すると、PFI（Permutation Feature Importance）プロットを作図することができます。PFI は、「データの中で特定の特徴量だけをランダムに並び替えたときに、予測精度がどの程度低下するか」をその特徴量の重要度として解釈するものです。\n\npfi &lt;- predictor %&gt;% FeatureImp$new(loss=\"rmse\")\n\nWarning: package 'renv' was built under R version 4.5.1\n\nplot(pfi) + theme_light()",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>iml</span>"
    ]
  },
  {
    "objectID": "articles/iml.html#サロゲートモデルを構築する",
    "href": "articles/iml.html#サロゲートモデルを構築する",
    "title": "iml",
    "section": "サロゲートモデルを構築する",
    "text": "サロゲートモデルを構築する\nPredictorオブジェクトにTreeSurrogate$new()関数を適用すると、決定木によるサロゲートモデルを構築することができます。サロゲートモデルとは、予測モデルを理解しやすいシンプルな関数で近似することによって得られる予測モデルのことで、ここでは、深さ2の決定木という非常に簡単な予測モデルに変換しています。\n\ntree &lt;- predictor %&gt;% TreeSurrogate$new(maxdepth = 2)\n\nLoading required package: partykit\n\n\nWarning: package 'partykit' was built under R version 4.5.1\n\n\nLoading required package: libcoin\n\n\nWarning: package 'libcoin' was built under R version 4.5.1\n\n\nLoading required package: mvtnorm\n\n\nWarning: package 'mvtnorm' was built under R version 4.5.1\n\nplot(tree)\n\n\n\n\n\n\n\n\nPredictorオブジェクトにLocalModel$new()関数を適用すると、特定の予測に関して LIME（Local Interpretable Model-agnostic Explanations）による局所的なサロゲートモデルを構築し、各特徴量の寄与を分解することができます。なお、LIMEは、注目するデータとその近傍データ（または近いデータを重視する加重データ）を用いて、正則化回帰などによって解釈しやすい予測モデルを構築する手法です。\n\nlime &lt;- predictor %&gt;% LocalModel$new(x.interest = X[5, ], k = 2)\n\nLoading required package: glmnet\n\n\nLoaded glmnet 4.1-8\n\n\nLoading required package: gower\n\nlime$results[, c(\"feature.value\", \"beta\", \"effect\")]\n\n       feature.value      beta   effect\nGirth     Girth=10.7 4.0613365 43.45630\nHeight     Height=81 0.3714726 30.08928\n\nplot(lime)",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>iml</span>"
    ]
  },
  {
    "objectID": "articles/iml.html#参考資料",
    "href": "articles/iml.html#参考資料",
    "title": "iml",
    "section": "参考資料",
    "text": "参考資料\niml パッケージには、ここで紹介した手法以外にもさまざまな手法が実装されています。以下のウェブ書籍は、機械学習の解釈手法を紹介するものとして非常に有名です。 Christoph Molnar, Interpretable Machine Learning, A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book/（邦訳）",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>iml</span>"
    ]
  },
  {
    "objectID": "articles/ismev.html",
    "href": "articles/ismev.html",
    "title": "ismev",
    "section": "",
    "text": "パッケージの概要\nismevは、極値理論に基づく統計解析を行うためのパッケージです。 Stuart Colesによる極値理論を紹介する著書『An Introduction to Statistical Modeling of Extreme Values』に登場する計算をサポートする関数が含まれています。 極値理論は、分布から大きく外れるような極端な事象をモデル化するための手法であり、保険分野においても、再保険や巨大自然災害リスクの分析、リスク管理でのテイル評価等に利用されています。",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ismev</span>"
    ]
  },
  {
    "objectID": "articles/ismev.html#ブロック最大値モデル",
    "href": "articles/ismev.html#ブロック最大値モデル",
    "title": "ismev",
    "section": "ブロック最大値モデル",
    "text": "ブロック最大値モデル\nブロック最大値モデルは、同一分布からの独立な標本の最大値の確率的性質を扱うモデルです。 ここでは、ismevパッケージに入っているportpirieのデータセットを使って、一般化極値分布（GEV）のフィッティングを行います。 portpirieは、65行2列で構成されるデータセットであり、1923年から1987年までの南オーストラリアのポートピリーで記録された年間最大海面水位を示しています。1列目は対応する年を示しています。年ごとの最大値のデータとなっているため、このままGEVのフィッティングを行います。フィッティングにはgev.fit関数を使います。\n\n# データセット\"portpirie\"を取得\ndata(portpirie)\nhead(portpirie)\n\n  Year SeaLevel\n1 1923     4.03\n2 1924     3.83\n3 1925     3.65\n4 1926     3.88\n5 1927     4.01\n6 1928     4.08\n\n# ヒストグラムを表示\nhist(portpirie$SeaLevel)\n\n\n\n\n\n\n\n# \"portpirie\"の年間最大海面水位についてGEVフィッティング\nfit_portpirie &lt;- gev.fit(portpirie$SeaLevel)\n\n$conv\n[1] 0\n\n$nllh\n[1] -4.339058\n\n$mle\n[1]  3.87474692  0.19804120 -0.05008773\n\n$se\n[1] 0.02793211 0.02024610 0.09825633\n\n\ngev.fit関数によるフィッティング結果の見方は次のとおりです。\n\n$conv\n\n収束のステータスを表しています。0の場合は収束したことを意味し、最適なパラメータが見つかり、アルゴリズムが正常終了したことを示します。\n\n$nllh\n\n負の対数尤度です。GEV分布のパラメータを推定する際に、尤度関数を最大化しますが、この値はその最大化した尤度関数の対数を負にしたものです。この値が小さいほど、データに適合したモデルであることを示します。\n\n$mle\n\nGEV分布の推定されたパラメータ（最尤推定値、MLE）です。順番に、位置パラメータ（location）\\mu、尺度パラメータ（scale）\\sigma、形状パラメータ（shape）\\xiの値を示しています。\n\n$se\n\n各パラメータの標準誤差です。最尤推定値の不確実性を示しており、値が小さいほど、推定値がより正確であることを意味します。\n\n\n次に、gev.diag関数を用いて、先ほど作成したGEVモデルについて、データに適合しているかどうかを視覚的および統計的に評価します。 gev.diag関数により、P-Pプロット、Q-Qプロット、再現レベルプロット、データのヒストグラムと適合した密度を出力することができます。\n\ngev.diag(fit_portpirie)\n\n\n\n\n\n\n\n\n次に、指数分布に従う乱数から作成した最大値のデータに対して、GEVフィッティングを行います。理論的には、最大値M_nを正規化した値Z_nはグンベル分布に法則収束するはずです。\n\n# シード値を設定\nset.seed(1234)\n\n# λ = 2 の指数分布に従う乱数を100万個生成\nN &lt;- 1000000\nlambda &lt;- 2\nrandom_exp &lt;- rexp(N, rate = lambda)\n\n# ヒストグラムを表示\nhist(random_exp)\n\n\n\n\n\n\n\n# 1000個ずつのブロックに分けて、各ブロックの最大値を計算\nn &lt;- 1000\nmax_exp &lt;- tapply(random_exp, (seq_along(random_exp) - 1) %/% n + 1, max)\n\n# 各ブロックの最大値M_nをデータフレームに格納\ndf_max_exp &lt;- data.frame(Block = 1:length(max_exp), M_n = max_exp)\n\n# M_nを正規化した値Z_nをデータフレームに格納\nd_n &lt;- log(n) / lambda\nc_n &lt;- 1 / lambda\ndf_max_exp$Z_n &lt;- (df_max_exp$M_n - d_n) / c_n\n\n# データフレームの最初の数行を表示\nhead(df_max_exp)\n\n  Block      M_n        Z_n\n1     1 3.633542  0.3593291\n2     2 3.730824  0.5538925\n3     3 3.123590 -0.6605744\n4     4 3.188098 -0.5315598\n5     5 3.621496  0.3352366\n6     6 3.186516 -0.5347237\n\n# 生成したデータフレームについてGEVフィッティング\nfit_max_exp &lt;- gev.fit(df_max_exp$Z_n)\n\n$conv\n[1] 0\n\n$nllh\n[1] 1578.983\n\n$mle\n[1] -0.043063270  0.996804717  0.006797281\n\n$se\n[1] 0.03540953 0.02575965 0.02287590\n\n# フィッティング結果を評価\ngev.diag(fit_max_exp)\n\n\n\n\n\n\n\n\n\\mu \\fallingdotseq 0、\\sigma\\fallingdotseq 1、\\xi\\fallingdotseq 0となり、グンベル分布が概ね再現できました。",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ismev</span>"
    ]
  },
  {
    "objectID": "articles/ismev.html#閾値超過モデル",
    "href": "articles/ismev.html#閾値超過モデル",
    "title": "ismev",
    "section": "閾値超過モデル",
    "text": "閾値超過モデル\n閾値超過モデルはある閾値を超過したデータを対象とするモデルです。 先ほど生成した指数分布に従う乱数データを用いて、閾値超過モデルである一般化パレート分布（GPD）のフィッティングを行います。\n\n# 乱数データをデータフレームに格納\ndf_random_exp &lt;- data.frame(X = random_exp)\n\n# 結果を確認\nhead(df_random_exp)\n\n            X\n1 1.250879302\n2 0.123379442\n3 0.003290978\n4 0.871373045\n5 0.193591292\n6 0.044974836\n\n# 閾値を設定\nu &lt;- 3\n\n# 生成したデータフレームについてGPDフィッティング\nfit_excess_exp &lt;- gpd.fit(df_random_exp$X, u)\n\n$threshold\n[1] 3\n\n$nexc\n[1] 2378\n\n$conv\n[1] 0\n\n$nllh\n[1] 740.2906\n\n$mle\n[1]  0.504669635 -0.004868447\n\n$rate\n[1] 0.002378\n\n$se\n[1] 0.01463252 0.02049923\n\n\ngpd.fit関数によるフィッティング結果の見方は、gev.fit関数と共通する箇所もありますが、次のとおりです。形状パラメータ\\xiが0に近い数値となっており、想定どおり指数分布の形状を示す結果となっています。\n\n$threshold\n\n引数にて設定した閾値です。\n\n$nexc\n\n閾値を超えたデータの件数を示します。閾値を超えたデータのみがGPDのフィットに使用されます。\n\n$conv\n\ngev.fit関数と同じく収束のステータスを表しています。0の場合は収束したことを意味します。\n\n$nllh\n\ngev.fit関数と同じく負の対数尤度であり、この値が小さいほど、データに適合したモデルであることを示します。\n\n$mle\n\nGPDの推定されたパラメータ（最尤推定値、MLE）です。順番に、尺度パラメータ（scale）\\sigma、形状パラメータ（shape）\\xiの値を示しています。GEVと異なり、位置パラメータはありません。\n\n$rate\n\n閾値を超えたデータの割合です。\n\n$se\n\ngev.fit関数と同じく各パラメータの標準誤差であり、値が小さいほど、推定値がより正確であることを意味します。\n\n\nGPDにおいても、次のとおりgpd.diag関数により作成したモデルについて評価することが可能です。出力内容はgev.diagと同じです。\n\n# フィッティング結果を評価\ngpd.diag(fit_excess_exp)\n\n\n\n\n\n\n\n\ngpd.fit関数では閾値を自分で設定する必要があります。適切な閾値を選択する方法はいくつか考えられますが、gpd.fitrange関数を使えば、異なる閾値に対してGPDを適合させ、パラメータの推定値の変化を調べることができます。2つのパラメータが安定するように閾値を設定することでより信頼性の高いフィッティングが可能になります。\n\n# 閾値が2～5の場合のパラメータ推定値を出力\ngpd.fitrange(df_random_exp$X, 2, 5)\n\n\n\n\n\n\n\n\n上記の結果では、閾値が4に差し掛かる辺りから信頼区間が急激に広がり不安定になることがわかります。このため、3～3.5の辺りで閾値を設定することが考えられます。",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ismev</span>"
    ]
  },
  {
    "objectID": "articles/ismev.html#参考資料",
    "href": "articles/ismev.html#参考資料",
    "title": "ismev",
    "section": "参考資料",
    "text": "参考資料\n[1] Stuart Coles. An Introduction to Statistical Modeling of Extreme Values.\n[2] 日本アクチュアリー会. 損保数理.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ismev</span>"
    ]
  },
  {
    "objectID": "articles/lifecontingencies.html",
    "href": "articles/lifecontingencies.html",
    "title": "lifecontingencies",
    "section": "",
    "text": "パッケージの概要\nlifecontingenciesは、ファイナンスや人口統計、保険数理の標準的な計算を実行することができるパッケージです。生命保険のリスク評価を実行するための包括的なツールセットとなっています。\nlibrary(lifecontingencies)",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>lifecontingencies</span>"
    ]
  },
  {
    "objectID": "articles/lifecontingencies.html#データの取得",
    "href": "articles/lifecontingencies.html#データの取得",
    "title": "lifecontingencies",
    "section": "データの取得",
    "text": "データの取得\nlifecontingenciesにはたくさんのデータセットが含まれています。以下にパッケージに含まれるデータセットの一覧を出力します。\n\n# データセット一覧を取得\ndatasets_info &lt;- as.data.frame(data(package=\"lifecontingencies\")$results[,c(\"Item\",\"Title\")])\n# 各データセットの型を取得し、一覧に追加\ndatasets_info$Class &lt;- sapply(datasets_info$Item, function(x) class(get(x)))\n# データセット一覧を出力\ncat(apply(datasets_info, 1, function(x) paste(x, collapse = \" : \")), sep = \"\\n\")\n\nAF92Lt : Uk AM AF 92 life tables : lifetable\nAM92Lt : Uk AM AF 92 life tables : lifetable\nSoAISTdata : SoA illustrative service table : data.frame\nde_angelis_di_falco : Italian Health Insurance Data : list\ndemoCanada : Canada Mortality Rates for UP94 Series : data.frame\ndemoChina : China Mortality Rates for life table construction : data.frame\ndemoFrance : French population life tables : data.frame\ndemoGermany : German population life tables : data.frame\ndemoIta : Italian population life tables for males and females : data.frame\ndemoJapan : Japan Mortality Rates for life table construction : data.frame\ndemoUk : UK life tables : data.frame\ndemoUsa : United States Social Security life tables : data.frame\nsoa08 : Society of Actuaries Illustrative Life Table object. : lifetable\nsoa08Act : Society of Actuaries Illustrative Life Table with interest rate at 6 : actuarialtable\nsoaLt : Society of Actuaries life table : data.frame\n\n\n例えば、soaLtは古典的書籍『Actuarial Mathematics (Second Edition)』から参照された、年齢xと生存数I_xの2列から成るデータフレームです。\n\nhead(soaLt)\n\n  x       Ix\n1 0 10000000\n2 1  9949901\n3 2  9899801\n4 3  9849702\n5 4  9799602\n6 5  9749503\n\ntail(soaLt)\n\n      x   Ix\n106 105 1668\n107 106  727\n108 107  292\n109 108  108\n110 109   36\n111 110   11",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>lifecontingencies</span>"
    ]
  },
  {
    "objectID": "articles/lifecontingencies.html#lifetable-class",
    "href": "articles/lifecontingencies.html#lifetable-class",
    "title": "lifecontingencies",
    "section": "lifetable-class",
    "text": "lifetable-class\nlifetable型のオブジェクトは、年齢と各年齢に対応する生存数があれば作成でき、生存確率p_xと平均余命e_xを保持しています。\ndata.frame型であるsoaLtのデータセットから、with関数およびnew関数を利用してlifetable型のオブジェクトを作成し、このオブジェクトをもう一度data.frame型に変換し直すと、生存確率p_xと平均余命e_xの情報が追加されていることがわかります。\nなお、lifecontingenciesにはsoa08というlifetable型のデータセットが含まれていますが、こちらは『Actuarial Mathematics (Second Edition)』のAppendix 2Aに掲載されているIllustrative life tableより作成されたもので、soaLtとは数値が異なります。\n\n# soaLtの先頭行を出力。年齢xと生存数Ixの情報のみ。\nhead(soaLt)\n\n  x       Ix\n1 0 10000000\n2 1  9949901\n3 2  9899801\n4 3  9849702\n5 4  9799602\n6 5  9749503\n\n# soaLtからlifetable型のオブジェクトsoaLt_lifetableを作成。\nsoaLt_lifetable &lt;- with(soaLt, new(\"lifetable\",x=x,lx=Ix,name=\"lifetable\"))\nstr(soaLt_lifetable)\n\nFormal class 'lifetable' [package \"lifecontingencies\"] with 3 slots\n  ..@ x   : int [1:111] 0 1 2 3 4 5 6 7 8 9 ...\n  ..@ lx  : num [1:111] 10000000 9949901 9899801 9849702 9799602 ...\n  ..@ name: chr \"lifetable\"\n\n# soaLt_lifetableをdata.frame型に変換（head関数を使用できるようにするため）。\nsoaLt_df &lt;- as(soaLt_lifetable, \"data.frame\")\n\n# 再び先頭行を出力。lifetable型のオブジェクトを作成したことで生存確率pxと平均余命exが取得できた。\nhead(soaLt_df)\n\n  x       lx        px       ex\n1 0 10000000 0.9949901 71.34692\n2 1  9949901 0.9949648 70.70616\n3 2  9899801 0.9949394 70.06398\n4 3  9849702 0.9949136 69.42035\n5 4  9799602 0.9948876 68.77526\n6 5  9749503 0.9990991 68.12867",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>lifecontingencies</span>"
    ]
  },
  {
    "objectID": "articles/lifecontingencies.html#actuarialtable-class",
    "href": "articles/lifecontingencies.html#actuarialtable-class",
    "title": "lifecontingencies",
    "section": "actuarialtable-class",
    "text": "actuarialtable-class\nactuarialtable型のオブジェクトは、年齢と各年齢に対応する生存数に加えて予定利率を設定すれば作成でき、計算基数を保持しています。\n上述のlifetable型と同様の方法で、soaLtのデータセットからactuarialtable型のオブジェクトを作成し、計算基数の情報が追加されることを確認します。なお、ここでは予定利率を2.0%とします。\n\n# soaLtの先頭行を出力。年齢xと生存数Ixの情報のみ。\nhead(soaLt)\n\n  x       Ix\n1 0 10000000\n2 1  9949901\n3 2  9899801\n4 3  9849702\n5 4  9799602\n6 5  9749503\n\n# soaLtからactuarialtable型のオブジェクトsoaLt_actuarialtableを作成。\nsoaLt_actuarialtable &lt;- with(soaLt, new(\"actuarialtable\",interest=0.02,\n                                        x=x,lx=Ix,name=\"actuarialtable\"))\nstr(soaLt_actuarialtable)\n\nFormal class 'actuarialtable' [package \"lifecontingencies\"] with 4 slots\n  ..@ interest: num 0.02\n  ..@ x       : int [1:111] 0 1 2 3 4 5 6 7 8 9 ...\n  ..@ lx      : num [1:111] 10000000 9949901 9899801 9849702 9799602 ...\n  ..@ name    : chr \"actuarialtable\"\n\n# soaLt_actuarialtableをdata.frame型に変換（head関数を使用できるようにするため）。\nsoaLt_df2 &lt;- as(soaLt_actuarialtable, \"data.frame\")\n\n# 再び先頭行を出力。actuarialtable型のオブジェクトを作成したことで計算基数が取得できた。\nhead(soaLt_df2)\n\n  x       lx       Dx        Nx        Cx      Mx        Rx\n1 0 10000000 10000000 376393655 49117.059 2619732 161974034\n2 1  9949901  9754805 366393655 48153.979 2570615 159354301\n3 2  9899801  9515380 356638851 47209.784 2522461 156783686\n4 3  9849702  9281594 347123471 46284.102 2475251 154261225\n5 4  9799602  9053318 337841877 45376.570 2428967 151785974\n6 5  9749503  8830425 328788559  7799.053 2383591 149357006",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>lifecontingencies</span>"
    ]
  },
  {
    "objectID": "articles/lifecontingencies.html#生命年金の年金現価",
    "href": "articles/lifecontingencies.html#生命年金の年金現価",
    "title": "lifecontingencies",
    "section": "生命年金の年金現価",
    "text": "生命年金の年金現価\nlifecontingenciesに含まれているactuarialtable型のデータセットであるsoa08Actを用いて、様々な生命年金の年金現価を計算してみます。このデータセットはsoaLtと同じ生存数によるものですが、予定利率は6.0%です。なお、予定利率はaxn関数の引数にて新たに設定することも可能ですが、ここでは6.0%のままとします（以下に紹介する関数も同様）。\n\n# 70歳開始の期始払終身年金の年金現価\naxn(soa08Act, x=70, payment=\"advance\")\n\n[1] 8.569251\n\n# 60歳開始の期末払有期年金（10年）の年金現価\naxn(soa08Act, x=60, n=10, payment=\"arrears\")\n\n[1] 6.730136\n\n# 65歳開始（5年据置して70歳に初回支払）の期始払有期年金（15年）の年金現価\naxn(soa08Act, x=65, n=15, m=5, payment=\"advance\")\n\n[1] 5.164831\n\n\nまた、x、n、mの引数はベクトルで設定することも可能です。\n\n# 60～70歳開始の期始払終身年金の年金現価\naxn(soa08Act, x=c(60:70), payment=\"advance\")\n\n [1] 11.145352 10.904118 10.658363 10.408367 10.154442  9.896928  9.636190\n [8]  9.372622  9.106643  8.838698  8.569251\n\n# 60歳開始の期末払有期年金（5,10,15,20年）の年金現価\naxn(soa08Act, x=60, n=c(5,10,15,20), payment=\"arrears\")\n\n[1] 4.028154 6.730136 8.435766 9.414205",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>lifecontingencies</span>"
    ]
  },
  {
    "objectID": "articles/lifecontingencies.html#生命保険の一時払保険料",
    "href": "articles/lifecontingencies.html#生命保険の一時払保険料",
    "title": "lifecontingencies",
    "section": "生命保険の一時払保険料",
    "text": "生命保険の一時払保険料\n同様に、soa08Actを用いて、様々な生命保険の一時払保険料を計算してみます。\n\n# 30歳加入、保険期間10年の定期保険の一時払保険料\nAxn(soa08Act, x=30, n=10)\n\n[1] 0.01418541\n\n# 60歳開始、保険期間5年の養老保険の一時払保険料\nAExn(soa08Act, x=60, n=5)\n\n[1] 0.7543062\n\n\n生命年金と同様に、引数はベクトルで設定することも可能です。\n\n# 30歳加入、保険期間1～10年の定期保険の一時払保険料\nAxn(soa08Act, x=30, n=c(1:10))\n\n [1] 0.001442374 0.002872079 0.004292064 0.005705164 0.007114111 0.008521538\n [7] 0.009929980 0.011341881 0.012759600 0.014185406",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>lifecontingencies</span>"
    ]
  },
  {
    "objectID": "articles/lifecontingencies.html#生命保険の責任準備金",
    "href": "articles/lifecontingencies.html#生命保険の責任準備金",
    "title": "lifecontingencies",
    "section": "生命保険の責任準備金",
    "text": "生命保険の責任準備金\n上記で紹介した関数を利用して、定期保険の純保険料式責任準備金を経過別に算出してみましょう。\n\n# 40歳加入、保険期間20年の定期保険の年払純保険料\nP &lt;- Axn(soa08Act, x=40, n=20) / axn(soa08Act, x=40, n=20, payment=\"advance\")\nprint(P)\n\n[1] 0.005112706\n\n# 40歳加入、保険期間20年の定期保険の純保険料式責任準備金\nV &lt;- data.frame(t = numeric(21), Reserve = numeric(21))\nfor (t in 0:20){\n  V[t+1, 1] &lt;- t\n  V[t+1, 2] &lt;- Axn(soa08Act, x=40+t, n=20-t) - P * axn(soa08Act, x=40+t, n=20-t, payment=\"advance\")\n}\nprint(V)\n\n    t     Reserve\n1   0 0.000000000\n2   1 0.002645617\n3   2 0.005257705\n4   3 0.007815974\n5   4 0.010297115\n6   5 0.012674419\n7   6 0.014917344\n8   7 0.016991027\n9   8 0.018855721\n10  9 0.020466160\n11 10 0.021770822\n12 11 0.022711085\n13 12 0.023220254\n14 13 0.023222423\n15 14 0.022631158\n16 15 0.021347950\n17 16 0.019260396\n18 17 0.016240051\n19 18 0.012139873\n20 19 0.006791181\n21 20 0.000000000\n\n# ggplotを使って責任準備金をプロット\nlibrary(ggplot2)\n\nggplot(V, aes(x = t, y = Reserve)) +\n  geom_line() +\n  labs(x = \"t\", y = \"Reserve\")",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>lifecontingencies</span>"
    ]
  },
  {
    "objectID": "articles/makedummies.html",
    "href": "articles/makedummies.html",
    "title": "makedummies",
    "section": "",
    "text": "パッケージの概要\nmakedummiesパッケージは、データフレームのfactor型変数(カテゴリ変数)をダミー変数化する関数を提供します。 簡単な指定で一度に複数の列を加工することが出来る点が特徴です。",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>makedummies</span>"
    ]
  },
  {
    "objectID": "articles/makedummies.html#準備",
    "href": "articles/makedummies.html#準備",
    "title": "makedummies",
    "section": "準備",
    "text": "準備\n\nパッケージの読み込み\n\nlibrary(makedummies)\nlibrary(carData) #データセット\n\n\n\nデータセットの読み込み\nJohn Fox and Sanford Weisberg (2018) で使用されたデータセット等をまとめたパッケージcarDataに含まれる、 WVSというデータセットを使用します。\n1995～1997年にオーストラリア、ノルウェー、スウェーデン、アメリカにて実施されたアンケート調査で、 貧困層への政策に対する評価と、年齢、性別などの回答者の属性からなる6つの列で構成されたデータセットです。 サンプル数は5,381件です。\n\ndata(\"WVS\")\ndf_all &lt;- WVS\nstr(df_all)\n\n'data.frame':   5381 obs. of  6 variables:\n $ poverty : Ord.factor w/ 3 levels \"Too Little\"&lt;\"About Right\"&lt;..: 1 2 1 3 1 2 3 1 1 1 ...\n $ religion: Factor w/ 2 levels \"no\",\"yes\": 2 2 2 2 2 2 2 2 2 2 ...\n $ degree  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 2 2 1 1 1 1 1 ...\n $ country : Factor w/ 4 levels \"Australia\",\"Norway\",..: 4 4 4 4 4 4 4 4 4 4 ...\n $ age     : int  44 40 36 25 39 80 48 32 74 30 ...\n $ gender  : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 1 1 2 1 2 ...\n\n\n各列の内容は次のとおりで、age以外はfactor型変数(カテゴリ変数)であることがわかります。\n\n\n\n\n\n\n\n\n列名\n型\n説明\n\n\n\n\npoverty\n順序付きfactor型\nこの国の政府が貧困層に対して行っていることは、適切だと思うか、多すぎるか、少なすぎるか？という問いへの答え : Too Little &lt; About Right &lt; Too Much の3通り\n\n\nreligion\nfactor型\n宗教の信者かどうか : no または yes\n\n\ndegree\nfactor型\n大学の学位を取得しているかどうか : no または yes\n\n\ncountry\nfactor型\n国 : Austraila, Norway, Sweden, USA の4通り\n\n\nage\n整数型\n年齢(歳)\n\n\ngender\nfactor型\n性別 : male または female\n\n\n\n冒頭6行のレコード、およびサマリーを確認すると次のとおり。\n\nhead(df_all)\n\n\n  \n\n\nsummary(df_all)\n\n        poverty     religion   degree          country          age       \n Too Little :2708   no : 786   no :4238   Australia:1874   Min.   :18.00  \n About Right:1862   yes:4595   yes:1143   Norway   :1127   1st Qu.:31.00  \n Too Much   : 811                         Sweden   :1003   Median :43.00  \n                                          USA      :1377   Mean   :45.04  \n                                                           3rd Qu.:58.00  \n                                                           Max.   :92.00  \n    gender    \n female:2725  \n male  :2656  \n              \n              \n              \n              \n\n\nデータセットのさらなる詳細については ?WVS を実行することで確認できます。",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>makedummies</span>"
    ]
  },
  {
    "objectID": "articles/makedummies.html#使用方法",
    "href": "articles/makedummies.html#使用方法",
    "title": "makedummies",
    "section": "使用方法",
    "text": "使用方法\n\n基本的な使用方法\nmakedummies関数により、すべてのfactor型変数(順序付きを含む)をダミー変数に変換することが出来ます。\n\ndf_all_dummy &lt;- makedummies(df_all)\nhead(df_all_dummy)\n\n\n  \n\n\n\nfactor型変数だった列が、(カテゴリの数 - 1)個の列に変換されています。\n基準となるカテゴリ(例えば、poverty列なら\"Too Little\"、religion列なら\"no\")に対応する列は作られず、 それ以外のカテゴリに対して、「対応するカテゴリに所属しているなら1」というダミー変数が生成されています。\nカテゴリが3個以上ある場合、列名が「元の列名_カテゴリ名」のように加工されます。 一方、カテゴリが2個しかない場合は、元の列名のままとなっています。\nfactor型ではない列(age)はそのまま残されていることがわかります。\n\n\n引数 basal_level: 基準カテゴリに対応する列を作る\n引数 basal_level に TRUE を与えることで、基準となるカテゴリに対応する列も作られるようになります。 なお、このような変換をワンホットエンコーディングといいます。\n\ndf_all_dummy &lt;- makedummies(df_all, basal_level = TRUE)\nhead(df_all_dummy)\n\n\n  \n\n\n\n\n\n引数 col: 特定の列のみ出力する\n引数 col に列名を与えることで、その列の処理結果だけが出力されるようになります。\n\ndf_all_dummy &lt;- makedummies(df_all, col = \"poverty\")\nhead(df_all_dummy)\n\n  poverty_About Right poverty_Too Much\n1                   0                0\n2                   1                0\n3                   0                0\n4                   0                1\n5                   0                0\n6                   1                0\n\n\n次のように、複数の列を出力対象にすることもできます。また、数値型のような処理されない列を含めることも可能です。\n\ndf_all_dummy &lt;- makedummies(df_all, col = c(\"poverty\", \"age\", \"gender\"))\nhead(df_all_dummy)\n\n\n  \n\n\n\n\n\n引数 numerical: ダミー変数化ではなく単純に整数に変換する列を指定\n引数 numerical に列名を与えることで、その列は単純に1, 2, 3, …とカテゴリ別の整数に変換するように変更できます。\n\ndf_all_dummy &lt;- makedummies(df_all, numerical =  c(\"poverty\", \"gender\"))\nhead(df_all_dummy)\n\n\n  \n\n\n\n\n\n引数 as.is: 変換しない列を指定\n引数 as.is に列名を与えることで、その列は処理の対象から除外されます。\n\ndf_all_dummy &lt;- makedummies(df_all, as.is =  c(\"poverty\", \"country\"))\nhead(df_all_dummy)",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>makedummies</span>"
    ]
  },
  {
    "objectID": "articles/makedummies.html#補足",
    "href": "articles/makedummies.html#補足",
    "title": "makedummies",
    "section": "補足",
    "text": "補足\n\n存在しないカテゴリに対する動作\nfactor型の定義にはあるものの、実際には存在しないカテゴリについては、ダミー変数化の対象から除かれます。\nたとえば上から3行のみを抽出したとします。\n\ndf_minimum &lt;- df_all[1:3,c(\"poverty\", \"gender\")]\ndf_minimum\n\n\n  \n\n\n\npoverty列には\"Too Much\"のカテゴリとなるデータがありません。\n\nsummary(df_minimum)\n\n        poverty     gender \n Too Little :2   female:2  \n About Right:1   male  :1  \n Too Much   :0             \n\n\nこの場合、makedummies関数による処理では\"Too Much\"に対応するダミー変数は作られません。\n\nmakedummies(df_minimum)\n\n  poverty gender\n1       0      1\n2       1      0\n3       0      0\n\n\nなお、単一のカテゴリしかない列が存在するとエラーとなります。 このような列はモデル構築においても悪影響を及ぼしうるため、前処理の中で別途取り除いておくことが考えられます。\n\ndf_minimum &lt;- df_all[1:3,c(\"country\", \"gender\")]\nsummary(df_minimum) #countryはUSAのみ\n\n      country     gender \n Australia:0   female:2  \n Norway   :0   male  :1  \n Sweden   :0             \n USA      :3             \n\nmakedummies(df_minimum) #res[cbind(seq.int(m), dat)] &lt;- 1L でエラー: 添え字が許される範囲外です\n\nError in res[cbind(seq.int(m), dat)] &lt;- 1L: subscript out of bounds\n\n\n\n\nNA値に対する動作\nNA値(欠損値)が含まれている場合、ダミー変数化後もNA値のままとなります。\n\n#NAの混ざったデータを生成\nN &lt;- 6\ndf_NA &lt;- df_all[1:N, c(\"poverty\",\"gender\")]\nset.seed(2024)\ndf_NA &lt;- data.frame(lapply(df_NA, function(x) {\n    x[[floor(runif(1, min=1, max=N+1))]] &lt;- NA\n    x\n  }))\ndf_NA\n\n\n  \n\n\n\n\nmakedummies(df_NA)\n\n  poverty_About Right poverty_Too Much gender\n1                   0                0      1\n2                   1                0     NA\n3                   0                0      0\n4                   0                1      0\n5                   0                0      1\n6                  NA               NA      0\n\n\n\n\nfactor型以外の取り扱い\n本パッケージはfactor型以外の列の加工には対応していません。\n実質的にはカテゴリ変数であるものの、数値や文字列として格納されている場合には、factor型に変換してからmakedummies関数を使用します。\n例えば性別が文字列で、学歴が数値で格納されているとしましょう。当然これらはfactor型ではないので、makedummies関数では加工できません。\n\ndf_tmp &lt;- data.frame(\"性別\" = c(\"女\", \"男\", \"女\", \"男\"), \"学歴\" = c(1, 1, 3, 2))\ndf_tmp\n\n  性別 学歴\n1   女    1\n2   男    1\n3   女    3\n4   男    2\n\nmakedummies(df_tmp)\n\n  性別 学歴\n1   女    1\n2   男    1\n3   女    3\n4   男    2\n\n\n以下のように、factor関数等を用いてfactor型に変換する必要があります。\n\ndf_tmp$性別 &lt;- factor(df_tmp$性別, levels = c(\"男\", \"女\"))\ndf_tmp$学歴 &lt;- factor(df_tmp$学歴, levels = c(1, 2, 3), labels = c(\"高卒\", \"大卒\", \"大学院卒\"), ordered = TRUE)\nstr(df_tmp)\n\n'data.frame':   4 obs. of  2 variables:\n $ 性別: Factor w/ 2 levels \"男\",\"女\": 2 1 2 1\n $ 学歴: Ord.factor w/ 3 levels \"高卒\"&lt;\"大卒\"&lt;..: 1 1 3 2\n\nmakedummies(df_tmp)\n\n  性別 学歴_大卒 学歴_大学院卒\n1    1         0             0\n2    0         0             0\n3    1         0             1\n4    0         1             0\n\n\n\n\n基準カテゴリの変更\n本パッケージでは基準カテゴリを指定する機能は無いため、事前にrelevel関数などで変更しておく必要があります。\n\ndf_tmp &lt;- df_all\ndf_tmp$country &lt;- relevel(df_tmp$country, \"USA\")#アメリカを基準カテゴリに変更\nhead(makedummies(df_tmp))\n\n\n  \n\n\n\nreorder関数で数が多いカテゴリを基準カテゴリにするなど、機械的に基準カテゴリを変更しておくことも考えられます。\n\ndf_tmp &lt;- df_all\ntable(df_tmp$country) #カテゴリごとの件数を表示\n\n\nAustralia    Norway    Sweden       USA \n     1874      1127      1003      1377 \n\ndf_tmp$country &lt;- reorder(df_tmp$country, df_tmp$country, length, decreasing = TRUE) #件数が多い順にカテゴリを並べ替える\nhead(makedummies(df_tmp)) #最も件数が多かったAustraliaが基準カテゴリになっている\n\n\n  \n\n\n\n\n\n順序付きfactor型\n順序付きfactor型とはカテゴリ変数を取り扱うfactor型の中でも特殊なもので、 カテゴリ間に大小関係のような順序関係があること(質的変数のなかでも順序尺度であること)を表しています。\nclass関数で型を確認してみるとorderedという型が付与されており、順序付きでないfactor型とは区別されていることがわかります。\n\nlapply(df_all, class)\n\n$poverty\n[1] \"ordered\" \"factor\" \n\n$religion\n[1] \"factor\"\n\n$degree\n[1] \"factor\"\n\n$country\n[1] \"factor\"\n\n$age\n[1] \"integer\"\n\n$gender\n[1] \"factor\"\n\n\n順序付きfactor型変数は、factor関数で引数orderedにTRUEを与える(またはordered関数を使用する)ことで生成することができます。\n\ndf_tmp &lt;- factor(c(4,2,4,2,3,2,3), levels = c(1,2,3,4), \n          labels = c(\"まったくそう思わない\", \"あまりそう思わない\", \"ややそう思う\", \"とてもそう思う\"),\n          ordered = TRUE)\ndf_tmp\n\n[1] とてもそう思う     あまりそう思わない とてもそう思う     あまりそう思わない\n[5] ややそう思う       あまりそう思わない ややそう思う      \n4 Levels: まったくそう思わない &lt; あまりそう思わない &lt; ... &lt; とてもそう思う\n\nclass(df_tmp)\n\n[1] \"ordered\" \"factor\" \n\n\n\n\ntibbleに対する動作\ndata.frame型を拡張したものにtibbleがありますが、makedummies関数がtibbleを加工した場合はtibbleのまま返します。\ndata.frame型を加工すると当然data.frame型のままです1が…\n\nclass(df_all)\n\n[1] \"data.frame\"\n\nclass(makedummies(df_all))\n\n[1] \"data.frame\"\n\n\ntibbleを加工したものはtibbleになります。\n\nlibrary(tibble)\ndf_all_tb &lt;- as_tibble(df_all)\nclass(df_all_tb)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nclass(makedummies(df_all_tb))\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nなお、tibbleはtidyverseに含まれるもののうちの一つで、 例えば以下のような記法で列を選択した時にベクトルにならない(データフレームのままである)等の点で R標準のdata.frameとは違いがあります。\ntibbleを用いている場合、加工後にtibbleとしての機能が失われないことは利便性の観点で重要です。\n\nhead(df_all[,\"poverty\"])\n\n[1] Too Little  About Right Too Little  Too Much    Too Little  About Right\nLevels: Too Little &lt; About Right &lt; Too Much\n\nhead(df_all_tb[,\"poverty\"])\n\n\n  \n\n\n\n\n\n他の関数・パッケージとの比較\n\nmodel.matrix関数\n数値型への変換にあたってはR標準のmodel.matrix関数が使われることがあります。\nこちらは予測モデルに入力する計画行列を生成するための関数で、交互作用項の追加等、ダミー変数化以外の機能もあります。\n出力は行列(matrix型)となるため注意してください。\n\nhead(model.matrix(~ . , data = df_all)) #切片項が生成される　順序付きfactor型に対しては多項式対比を使用\n\n  (Intercept)     poverty.L  poverty.Q religionyes degreeyes countryNorway\n1           1 -7.071068e-01  0.4082483           1         0             0\n2           1 -7.850462e-17 -0.8164966           1         0             0\n3           1 -7.071068e-01  0.4082483           1         0             0\n4           1  7.071068e-01  0.4082483           1         1             0\n5           1 -7.071068e-01  0.4082483           1         1             0\n6           1 -7.850462e-17 -0.8164966           1         0             0\n  countrySweden countryUSA age gendermale\n1             0          1  44          1\n2             0          1  40          0\n3             0          1  36          0\n4             0          1  25          0\n5             0          1  39          1\n6             0          1  80          0\n\nhead(model.matrix(~ . +0, data = df_all)) #切片項が生成されない\n\n  povertyToo Little povertyAbout Right povertyToo Much religionyes degreeyes\n1                 1                  0               0           1         0\n2                 0                  1               0           1         0\n3                 1                  0               0           1         0\n4                 0                  0               1           1         1\n5                 1                  0               0           1         1\n6                 0                  1               0           1         0\n  countryNorway countrySweden countryUSA age gendermale\n1             0             0          1  44          1\n2             0             0          1  40          0\n3             0             0          1  36          0\n4             0             0          1  25          0\n5             0             0          1  39          1\n6             0             0          1  80          0\n\n\n\n\nfastDummiesパッケージ\n本パッケージと非常に類似するパッケージとしてfastDummiesパッケージがあります。 名前のとおり、model.matrix関数よりも処理が高速だとされています。\ndummy_cols関数によりダミー変数化が可能です。 ただし、makedummies関数とはデフォルトの処理が若干異なっています。 たとえば順序付きfactor型は処理対象としないほか、ダミー変数化前の変数が残る、 基準カテゴリに対応するダミー変数も作成する、といった点が異なります。\n\nlibrary(fastDummies)\n\nWarning: package 'fastDummies' was built under R version 4.5.1\n\nhead(dummy_cols(df_all))\n\n\n  \n\n\n\n引数により処理をカスタマイズすることができます。\n例えば引数select_columnsにより処理対象の列を指定することが出来ます。 ここでは順序付きfactor型だけでなく、数値型をも処理の対象とすることが出来ます。\n\nhead(dummy_cols(df_all,\n                #処理対象列の指定\n                select_columns = c(\"poverty\",\"religion\",\"degree\",\"county\"),\n                #元の列を残さない\n                remove_selected_columns = TRUE,\n                #基準カテゴリに対応するダミー変数を作らない\n                remove_first_dummy = TRUE \n                ))\n\nWarning in dummy_cols(df_all, select_columns = c(\"poverty\", \"religion\", : NOTE: The following select_columns input(s) is not a column in data.\n    \n\n\n\n  \n\n\n\nなお、NA値に対してはデフォルトで個別のダミー変数を作成する挙動となっていますが、 引数ignore_naでこれを制御することが出来ます。\n\nhead(dummy_cols(df_NA, remove_selected_columns = TRUE, ignore_na = FALSE))\n\n\n  \n\n\nhead(dummy_cols(df_NA, remove_selected_columns = TRUE, ignore_na = TRUE))\n\n\n  \n\n\n\nその他、最も大きなカテゴリを基準カテゴリとする引数remove_most_frequent_dummy、 複数のカテゴリ名が単一の文字列に含まれる際の処理を行う引数split、 元の列名をダミー変数に引き継がない引数omit_colname_prefixといった機能があります。\nまた、factor型の列に関してすべてのカテゴリ（の組み合わせ）が現れるように行を補完するdummy_rows関数も提供されます。\n\n\nonehotパッケージ\nonehotパッケージはシンプルにワンホットエンコーディングを行う機能だけを提供するものです。\nまずonehot関数によりエンコーダーを生成し、predict関数により実際にデータを変換します。\n出力は行列(matrix型)となるため注意してください。\n\nlibrary(onehot)\ndf_tmp &lt;- df_all\n#順序付きfactor型があるとエラーとなるため普通のfactor型に変換\ndf_tmp$poverty &lt;- factor(df_tmp$poverty, ordered = FALSE)\n#ここでのdf_tmpはデータの形式を指定するためのもので、実際に変換したいデータと同一データである必要はない\nencoder &lt;- onehot(df_tmp) \nhead(predict(encoder, df_tmp))\n\n     poverty=Too Little poverty=About Right poverty=Too Much religion=no\n[1,]                  1                   0                0           0\n[2,]                  0                   1                0           0\n[3,]                  1                   0                0           0\n[4,]                  0                   0                1           0\n[5,]                  1                   0                0           0\n[6,]                  0                   1                0           0\n     religion=yes degree=no degree=yes country=Australia country=Norway\n[1,]            1         1          0                 0              0\n[2,]            1         1          0                 0              0\n[3,]            1         1          0                 0              0\n[4,]            1         0          1                 0              0\n[5,]            1         0          1                 0              0\n[6,]            1         1          0                 0              0\n     country=Sweden country=USA age gender=female gender=male\n[1,]              0           1  44             0           1\n[2,]              0           1  40             1           0\n[3,]              0           1  36             1           0\n[4,]              0           1  25             1           0\n[5,]              0           1  39             0           1\n[6,]              0           1  80             1           0\n\n\n\n\nrecipesパッケージ\nrecipesパッケージはtidymodelsに含まれ、ダミー変数化を含む様々な前処理機能を提供するパッケージです。\n前処理手順のオブジェクト化などtidyな(整然とした)コーディングが可能になるほか、 他のtidymodelsのパッケージとの連携も特徴です。 本パッケージはダミー変数化のためだけに導入するようなものではありませんが、 そのようなコーディングを指向する場合にはこちらを用いるべきかもしれません。\nダミー変数化を行う機能を提供するのはstep_dummy関数で、例えば次のように使用します。 なお、順序付きfactor型変数に対しては多項式対比を用いた変換が行われます。\n\nlibrary(recipes)\nrec &lt;- recipe(df_all, formula = ) %&gt;%\n  step_dummy(all_factor(), all_ordered())\nrec %&gt;% prep() %&gt;% bake(new_data = NULL) %&gt;% head()\n\n\n  \n\n\n\n\n\nその他のパッケージ\ncaretパッケージのdummyVars関数、mltoolsパッケージのone_hot関数でも同様の処理は可能です。 これらのパッケージを利用している場合は、それらの関数を用いてもよいでしょう。 ただし、これらはより広範な機能を提供するパッケージのため、ダミー変数化のためだけに導入するようなものではありません。",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>makedummies</span>"
    ]
  },
  {
    "objectID": "articles/makedummies.html#参考文献",
    "href": "articles/makedummies.html#参考文献",
    "title": "makedummies",
    "section": "参考文献",
    "text": "参考文献\n\n\nJohn Fox, and Sanford Weisberg. 2018. An R Companion to Applied Regression, Third Edition. Springer.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>makedummies</span>"
    ]
  },
  {
    "objectID": "articles/makedummies.html#footnotes",
    "href": "articles/makedummies.html#footnotes",
    "title": "makedummies",
    "section": "",
    "text": "意図された動作かは不明なものの、すべての列が変換対象となったときは行列(matrix型)になります。↩︎",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>makedummies</span>"
    ]
  },
  {
    "objectID": "articles/mlbench.html",
    "href": "articles/mlbench.html",
    "title": "mlbench",
    "section": "",
    "text": "パッケージの概要\nmlbenchには、機械学習モデルの性能を比較・評価するベンチマーク課題用のデータセット、およびデータを生成するための関数が用意されています。\nデータセットは、例えばBostonHousing等の有名なデータセットや、UCIリポジトリ[1] (カリフォルニア大学アーバイン校が公開している機械学習データセットリポジトリ)の一部のデータセット等が含まれています。\nまた、ベンチマーク課題用のテストデータを生成する関数には、例えば複数クラスの2次元正規分布を生成するmlbench.2dnormals関数や、螺旋データを生成するmlbench.spiral関数等、様々な種類の関数が用意されています。\nなお、mlbenchに用意されているこれらのデータ生成用の関数は、mlbench.[データを示す名称]という形式の関数名になっています。一方、mlbenchに格納されているデータセットそのものは、BostonHousingやZooのように、mlbench.がついておらず大文字から始まる名称なので、データセットとデータ生成関数は名前で区別することができます。",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>mlbench</span>"
    ]
  },
  {
    "objectID": "articles/mlbench.html#パッケージの使用例",
    "href": "articles/mlbench.html#パッケージの使用例",
    "title": "mlbench",
    "section": "パッケージの使用例",
    "text": "パッケージの使用例\n\nデータセット\nmlbenchには、様々なベンチマーク課題用のデータセットが用意されています。 データセットは以下のように、データセット名を指定してdata.frameとしてロードすることができます。\n\nlibrary(mlbench)\ndata(\"BostonHousing\", package = \"mlbench\")\nstr(BostonHousing)\n\n'data.frame':   506 obs. of  14 variables:\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : num  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ b      : num  397 397 393 395 397 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n\n\n以下は格納されているデータセットの一覧です。(バージョン2.1.5時点です。)\nクラス分類用のデータや回帰用のデータ、特徴量が連続変数のデータやカテゴリ変数のデータ、クラスに偏りがあるデータなど、様々な種類の機械学習のベンチマーク課題に使用できるようなデータが格納されています。 各データのカラムやデータソース等の詳細は、mlbenchのリファレンスをご参照ください。\n\n\n\n表 24.1\n\n\n\n\n\n\n\n\n\n\n\n\nデータセット名\n概要\n課題の種別\nレコード数\nカラム数\n\n\n\n\nBostonHousing\n1970年調査によるボストン506区画の住宅価格データ\n回帰\n506\n14\n\n\nBreastCancer\n乳がんの事例に関するデータから良性／悪性に分類\n2クラス分類\n699\n11\n\n\nDNA\nDNAのスプライス・ジャンクションの塩基配列データを、エクソン-イントロン／イントロン―エクソン／それ以外に分類\n多クラス分類\n3,186\n181\n\n\nGlass\n7種類のガラスに対する化学分析のデータ\n多クラス分類\n214\n10\n\n\nHouseVotes84\n1984年のアメリカ合衆国議会の16の投票記録から民主党／共和党を分類\n2クラス分類\n435\n17\n\n\nIonosphere\n電離層のレーダー・リターンデータをgood／badに分類\n2クラス分類\n351\n35\n\n\nLetterRecognition\nアルファベットの画像から計算された特徴量をもとにA-Zを分類\n多クラス分類\n20,000\n17\n\n\nOzone\n1976年のロサンゼルスのオゾン汚染データ\n回帰\n366\n13\n\n\nPimaIndiansDiabetes\nピマ・インディアン女性のデータから糖尿病発症あり／なしを分類\n2クラス分類\n768\n9\n\n\nSatellite\nランドサット衛星の衛星画像データを土壌、植生等のクラスに分類\n多クラス分類\n6,435\n37\n\n\nServo\nサーボシステムの応答時間の予測課題(ただし、mlbenchでは応答時間が順序尺度に変換されている)\n多クラス分類\n167\n5\n\n\nShuttle\nスペースシャトルの飛行中の状態を7種類に分類\n多クラス分類\n58,000\n10\n\n\nSonar\nソナー信号が岩石からの反射か、金属円筒からの反射かを識別\n2クラス分類\n208\n61\n\n\nSoybean\n19クラスのダイズの病害を分類\n多クラス分類\n683\n36\n\n\nVehicle\n車のシルエットに関する特徴量から4車種に分類\n多クラス分類\n846\n19\n\n\nVowel\nイギリス英語の11種類の母音を分類\n多クラス分類\n990\n11\n\n\nZoo\n毛の有無、羽の有無等、主に論理値の特徴量から動物の種類を分類\n多クラス分類\n101\n17\n\n\n\n\n\n\n\n\nデータ生成関数\nmlbenchには前項で紹介したデータセットに加え、ベンチマーク課題用のデータを自ら生成するための関数も用意されています。\n以下は、指定したクラス数のクラスごとに、2次元空間上で正規分布するデータを生成する、mlbench.2dnormalsの使用例です。 これらの生成関数によるデータは、R標準のS3 classという機能を用いて、as.data.frame関数とplot関数が適切に動作するように実装されています。 すなわち、List形式で返される生成データを引数として、as.data.frame関数によるデータフレームへ変換、およびplot関数による生成データの種類に応じたグラフのプロットができるようになっています。\n\n# 乱数シードの設定\nset.seed(123)\n\n# 2次元正規分布データの生成\n## cl=2が2クラスを意味する\n## デフォルトでは原点を中心に、半径r=(cl)^(1/2)の円上に各クラスの中心を配置する\np &lt;- mlbench.2dnormals(n=1000, cl=2, sd=1)\n\n# 生成したデータはmlbench.2dnormals classのList形式\nstr(p)\n\nList of 2\n $ x      : num [1:1000, 1:2] 0.3981 0.0063 2.0268 -0.2489 -0.5092 ...\n $ classes: Factor w/ 2 levels \"1\",\"2\": 1 1 1 2 1 2 2 2 1 1 ...\n - attr(*, \"class\")= chr [1:2] \"mlbench.2dnormals\" \"mlbench\"\n\n# data.frameへの変換\nstr(as.data.frame(p))\n\n'data.frame':   1000 obs. of  3 variables:\n $ x.1    : num  0.3981 0.0063 2.0268 -0.2489 -0.5092 ...\n $ x.2    : num  0.179 0.6927 0.0979 -0.3729 2.1204 ...\n $ classes: Factor w/ 2 levels \"1\",\"2\": 1 1 1 2 1 2 2 2 1 1 ...\n\n# mlbench.2dnormals classのplot\nplot(p)\n\n\n\n\n\n\n\n\nさらに、mlbenchの生成データに対して適用可能なベイズ識別器、bayesclass関数が用意されています。内部的には、引数として渡す生成データの種類によって、適切な識別関数の実装が呼び出されています。\n以下は、mlbench.2dnormalsで生成した2クラス分類用のデータ対する、bayesclass関数による分類結果をプロットしています。 mlbench.2dnormalsの生成データの場合、いずれのクラスも同分散の2次元正規分布に従うため、個々のデータは、各クラスの正規分布の中心(期待値)のうち、最も距離の近い中心を持つクラスへと分類されることになります。 そのため、上のグラフで示した真のクラスと、下図の分類結果とを比較すると、グラフ中央付近の両クラスのデータが重なる領域では、誤分類が発生していることがわかります。\n\nplot(p$x, col=as.numeric(bayesclass(p)))\n\n\n\n\n\n\n\n\n上で紹介したmlbench.2dnormals以外にも、mlbenchでは様々なデータ生成用の関数が用意されています。 以下ではいくつかの例をplotしています。 これらの例に挙げたもの以外を含む、関数の一覧や各関数の詳細については、mlbenchのリファレンスをご参照ください。\n\nSpiral\n\nplot(mlbench.spirals(n=200, sd=0.05))\n\n\n\n\n\n\n\n\n\n\nCircle\n\nplot(mlbench.circle(n=1000))\n\n\n\n\n\n\n\n\n\n\nShapes\n\nplot(mlbench.shapes(n=1000))\n\n\n\n\n\n\n\n\n\n\nSmiley\n\nplot(mlbench.smiley(n=1000))",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>mlbench</span>"
    ]
  },
  {
    "objectID": "articles/mlbench.html#参考資料",
    "href": "articles/mlbench.html#参考資料",
    "title": "mlbench",
    "section": "参考資料",
    "text": "参考資料\n[1] UC Irvine Machine Learning Repository.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>mlbench</span>"
    ]
  },
  {
    "objectID": "articles/modeldata.html",
    "href": "articles/modeldata.html",
    "title": "modeldata",
    "section": "",
    "text": "パッケージの概要\nmodeldataには、Rにおけるモデリングや機械学習のためのツール群として有名なtidymodelsパッケージ[1]のドキュメントやテストに使用されたデータセットが含まれており、これらは様々な機械学習や予測モデリングのサンプルデータとしても活用することができます。 また、データセットそのものに加え、分類や回帰課題のためのシミュレーションデータを生成する関数も含まれています。",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>modeldata</span>"
    ]
  },
  {
    "objectID": "articles/modeldata.html#パッケージの使用例",
    "href": "articles/modeldata.html#パッケージの使用例",
    "title": "modeldata",
    "section": "パッケージの使用例",
    "text": "パッケージの使用例\n\nデータセット\nmodeldataには様々なデータセットが用意されています。 データセットは以下のように、データセット名を指定してロードすることができます。\n\nlibrary(modeldata)\n\n\nAttaching package: 'modeldata'\n\n\nThe following object is masked from 'package:datasets':\n\n    penguins\n\ndata(\"Chicago\", package = \"modeldata\")\nstr(Chicago)\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   5698 obs. of  50 variables:\n $ ridership       : num  15.7 15.8 15.9 15.9 15.4 ...\n $ Austin          : num  1.46 1.5 1.52 1.49 1.5 ...\n $ Quincy_Wells    : num  8.37 8.35 8.36 7.85 7.62 ...\n $ Belmont         : num  4.6 4.72 4.68 4.77 4.72 ...\n $ Archer_35th     : num  2.01 2.09 2.11 2.17 2.06 ...\n $ Oak_Park        : num  1.42 1.43 1.49 1.45 1.42 ...\n $ Western         : num  3.32 3.34 3.36 3.36 3.27 ...\n $ Clark_Lake      : num  15.6 15.7 15.6 15.7 15.6 ...\n $ Clinton         : num  2.4 2.4 2.37 2.42 2.42 ...\n $ Merchandise_Mart: num  6.48 6.48 6.41 6.49 5.8 ...\n $ Irving_Park     : num  3.74 3.85 3.86 3.84 3.88 ...\n $ Washington_Wells: num  7.56 7.58 7.62 7.36 7.09 ...\n $ Harlem          : num  2.65 2.76 2.79 2.81 2.73 ...\n $ Monroe          : num  5.67 6.01 5.79 5.96 5.77 ...\n $ Polk            : num  2.48 2.44 2.53 2.45 2.57 ...\n $ Ashland         : num  1.32 1.31 1.32 1.35 1.35 ...\n $ Kedzie          : num  3.01 3.02 2.98 3.01 3.08 ...\n $ Addison         : num  2.5 2.57 2.59 2.53 2.56 ...\n $ Jefferson_Park  : num  6.59 6.75 6.97 7.01 6.92 ...\n $ Montrose        : num  1.84 1.92 1.98 1.98 1.95 ...\n $ California      : num  0.756 0.781 0.812 0.776 0.789 0.37 0.274 0.473 0.844 0.835 ...\n $ temp_min        : num  15.1 25 19 15.1 21 19 15.1 26.6 34 33.1 ...\n $ temp            : num  19.4 30.4 25 22.4 27 ...\n $ temp_max        : num  30 36 28.9 27 32 30 28.9 41 43 36 ...\n $ temp_change     : num  14.9 11 9.9 11.9 11 11 13.8 14.4 9 2.9 ...\n $ dew             : num  13.4 25 18 10.9 21.9 ...\n $ humidity        : num  78 79 81 66.5 84 71 74 93 93 89 ...\n $ pressure        : num  30.4 30.2 30.2 30.4 29.9 ...\n $ pressure_change : num  0.12 0.18 0.23 0.16 0.65 ...\n $ wind            : num  5.2 8.1 10.4 9.8 12.7 12.7 8.1 8.1 9.2 11.5 ...\n $ wind_max        : num  10.4 11.5 19.6 16.1 19.6 17.3 13.8 17.3 23 16.1 ...\n $ gust            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ gust_max        : num  0 0 0 0 25.3 26.5 0 26.5 31.1 0 ...\n $ percip          : num  0 0 0 0 0 0 0 0 0 0 ...\n $ percip_max      : num  0 0 0 0 0 0 0 0.07 0.11 0.01 ...\n $ weather_rain    : num  0 0 0 0 0 ...\n $ weather_snow    : num  0 0 0.214 0 0.516 ...\n $ weather_cloud   : num  0.708 1 0.357 0.292 0.452 ...\n $ weather_storm   : num  0 0.2083 0.0714 0.0417 0.4516 ...\n $ Blackhawks_Away : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Blackhawks_Home : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Bulls_Away      : num  0 0 1 0 0 0 0 0 1 0 ...\n $ Bulls_Home      : num  0 1 0 0 0 1 0 0 0 0 ...\n $ Bears_Away      : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Bears_Home      : num  0 0 0 0 0 0 0 0 0 0 ...\n $ WhiteSox_Away   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ WhiteSox_Home   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Cubs_Away       : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Cubs_Home       : num  0 0 0 0 0 0 0 0 0 0 ...\n $ date            : Date, format: \"2001-01-22\" \"2001-01-23\" ...\n\n\n以下は格納されているデータセットの一覧です。(バージョン1.4.0時点です。) 各データのカラムやデータソース等の詳細は、modeldataのリファレンス[2]をご参照ください。\n\n\n\n\n\n\n\n\n\nデータセット名\n概要\nレコード数\nカラム数\n\n\n\n\nChicago stations\nシカゴの鉄道乗客数データ\n   5698\n     20\n    50\n     1\n\n\nSacramento\nサクラメント市（カリフォルニア州）の住宅価格\n    932\n     9\n\n\nSmithsonian\nスミソニアン博物館のジオコードデータ\n     20\n     3\n\n\nad_data\nアルツハイマー病に関する333患者のデータ\n    333\n   131\n\n\names\nエイムズ市（アイオワ州）の住宅データ\n   2930\n    74\n\n\nattrition\n従業員の離職に関するデータ\n   1470\n    31\n\n\nbiomass\nバイオマス燃料のデータ\n    536\n     8\n\n\nbivariate_test bivariate_train bivariate_val\n二値分類用データの例\n    710\n   1009\n    300\n     3\n     3\n     3\n\n\ncar_prices\nケリー・ブルー・ブック社によるGM車のリセールデータ\n    804\n    18\n\n\ncat_adoption\n保護猫の譲渡に関するデータ\n   2257\n    20\n\n\ncells\n細胞体の分類\n   2019\n    58\n\n\ncheck_times\nrパッケージの実行時間のデータ\n  13626\n    25\n\n\nchem_proc_yield\n化学製品の整合工程に関するデータセット\n    176\n    58\n\n\nconcrete\nコンクリート混合物の圧縮強度\n   1030\n     9\n\n\ncovers\n森林の主要な樹木に関する未加工のテキストデータ\n     40\n     1\n\n\ncredit_data\n信用リスクの分類用データ\n   4454\n    14\n\n\ncrickets\nコオロギが鳴く頻度と気温の関係\n     31\n     3\n\n\ndeliveries\nフード・デリバリーの配送時間のデータ\n  10012\n    31\n\n\ndrinks\nアルコール類の売上の時系列データ\n    309\n     2\n\n\ngrants_2008 grants_other grants_test\n学術助成金の採択／非採択のデータ\n   6633\n   8190\n    518\n     1\n  1503\n  1503\n\n\nhepatic_injury_qsar\n肝臓損傷の分類用データ\n    281\n   377\n\n\nhotel_rates\nリスボンにおけるホテル料金のデータ\n  15402\n    28\n\n\nhpc_cv\n高性能計算(HPC)環境における実行時間のクラス確率の予測\n   3467\n     7\n\n\nhpc_data\n高性能計算(HPC)環境における実行時間のクラス分類用データ\n   4331\n     8\n\n\nischemic_stroke\n虚血性脳卒中の有無を予測するためのデータ\n    126\n    29\n\n\nleaf_id_flavia\n葉のイメージデータの特徴量から植物の種を特定するデータ\n   1907\n    59\n\n\nlending_club\nレンディング・クラブ・サービスのローンデータ\n   9857\n    23\n\n\nmeats\n肉サンプルの脂肪含有量、水分含有量、タンパク質含有量\n    215\n   103\n\n\nmlc_churn\n顧客の解約に関するデータ\n   5000\n    20\n\n\noils\n市販の油の脂肪酸濃度\n     96\n     8\n\n\nparabolic\n放物線状の境界を持つ二値分類データ\n    500\n     3\n\n\npathology\n肝臓の病理データ\n    344\n     2\n\n\npd_speech\nパーキンソン病患者の発話の分類データ\n    252\n   752\n\n\npenguins\nパーマー基地のペンギンのデータ\n    344\n     7\n\n\npermeability_qsar\n分子構造に関する化学情報から透過性を予測するデータ\n    165\n  1108\n\n\nscat\n動物の糞の形態計測データ\n    110\n    19\n\n\nsolubility_test\n多変量適応的回帰スプライン(MARS)モデルによる溶解度の予測\n    316\n     2\n\n\nstackoverflow\nStack Overflowによる年次サーベイのデータ\n   5594\n    21\n\n\nsteroidogenic_toxicity\nステロイド生成毒性を評価するためのin vitroアッセイデータ\n    162\n    13\n\n\ntate_text\nテート・ギャラリーの近現代美術作品のメタデータ\n   4284\n     5\n\n\ntaxi\nシカゴのタクシーのデータセット\n  10000\n     7\n\n\ntesting_data training_data\nAmazonの食品レビューのデータ\n   1000\n   4000\n     3\n     3\n\n\ntwo_class_dat\n2クラス分類用に人工的に作成したデータ\n    791\n     3\n\n\ntwo_class_example\n2クラス分類の予測値\n    500\n     4\n\n\nwa_churn\nIBMワトソンのサイトから取得した顧客解約データ\n   7043\n    20\n\n\n\n\n\nデータ生成関数\nmodeldataには前項で紹介したデータセットに加え、データを生成するための関数も用意されています。\n\nsim_classification\nsim_classification関数は、二値分類用のシミュレーションデータを生成します。\n以下はデータ生成の実行例です。 引数methodには生成方法を指定しますが、バージョン1.4.0時点では”caret”が唯一のオプションとなっており、caretパッケージ[3]のcaret::twoClassSim()関数と同じ生成方法が実装されています。\n\nset.seed(1234)\n\nd1 &lt;- sim_classification(\n  num_samples = 100,  # 生成するレコード数(デフォルトは100)\n  method = \"caret\",   # 生成方法(デフォルトは\"caret\")\n  intercept = -5,     # クラスの偏りをコントロールする(デフォルトは-5)\n  num_linear = 2,     # クラス分類に無相関の変数linear_nの数(デフォルトは10)\n  keep_truth = TRUE   # class_1のクラス確率の出力有無(デフォルトはFALSE)\n)\n\nstr(d1)\n\ntibble [100 × 9] (S3: tbl_df/tbl/data.frame)\n $ class       : Factor w/ 2 levels \"class_1\",\"class_2\": 1 2 1 1 2 2 2 2 2 2 ...\n $ two_factor_1: num [1:100] -1.796 0.637 1.354 -2.716 1.04 ...\n $ two_factor_2: num [1:100] -1.3053 0.0755 1.432 -3.3104 0.0626 ...\n $ non_linear_1: num [1:100] 0.372 -0.167 0.514 0.552 0.147 ...\n $ non_linear_2: num [1:100] 0.6689 0.801 0.8555 0.0501 0.6744 ...\n $ non_linear_3: num [1:100] 0.281 0.174 0.17 0.561 0.429 ...\n $ linear_1    : num [1:100] -2.316 0.562 -0.784 -0.226 -1.587 ...\n $ linear_2    : num [1:100] 0.363 1.411 1.368 -0.407 0.763 ...\n $ .truth      : num [1:100] 0.99145 0.00313 0.84924 1 0.00423 ...\n\n\n“class”が、当該レコードが属するクラスを示すファクター変数で、class_1とclass_2のいずれかのレベルが格納されています。\n他の変数はclass_1とclass_2のうちどちらに属するかを予測するための説明変数(の候補)ですが、実際にクラス分類に使用できるのはtwo_factor_1とtwo_factor_2で、他はクラスと無相関という設定になっています。 具体的には、two_factor_1とtwo_factor_2の主効果と交互作用および引数に指定したinterceptによるバイアス(intercept - 4 * two_factor_1 + 4 * two_factor_2 + 2 * two_factor_1 * two_factor_2)を主な要素として計算した値を、ロジスティック関数でclass_1確率に変換し、当該確率をもとにランダムでクラスを割り当てています。\n以下では生成したデータの分析イメージの参考例として、two_factor_1とtwo_factor2を使用して2クラスを分離する線形識別関数を求めています。 w_0+w_1*two\\_factor\\_1 + w_2*two\\_factor\\_2\\geq0であればクラス1に、w_0+w_1*two\\_factor\\_1 + w_2*two\\_factor\\_2&lt;0であればクラス2に分類するモデルを考えます。 教師データを、クラス1なら+1、クラス2なら-1として、最小二乗誤差基準により係数を推定しています。\n\n# 線形識別関数のパラメータ推定\ny &lt;- ifelse(as.numeric(d1$class) &gt; 1, -1, 1)  # 教師データをclass_1=1, class_2=-1に変換\nX &lt;- cbind(1, as.matrix(d1[, c(\"two_factor_1\", \"two_factor_2\")])) # パターン行列\nw_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y     # 正規方程式で最小二乗法の解を求める\n\n# 結果のplot\nplot(x=d1$two_factor_1, y=d1$two_factor_2, \n     pch=ifelse(as.numeric(d1$class)==1, 1, 3), \n     col=d1$class)\nlegend(\"topleft\", legend=levels(d1$class), pch=c(1, 3), col=c(1, 2))\nabline(a=w_hat[1]/w_hat[3], b=-w_hat[2]/w_hat[3], lty=2)\n\n\n\n\n\n\n\n\n\n\nsim_regression\nsim_regression関数は、回帰用のシミュレーションデータを生成します。 以下の実行例の通り、目的変数outcomeとその数値の算出に使用した説明変数(predictor)が出力されます。\n\nd2 &lt;- sim_regression(\n  num_samples = 100,      # 生成するレコード数(デフォルトは100)\n  method = \"sapp_2014_1\", # 生成方法(デフォルトは\"sapp_2014_1\")\n  std_dev = NULL,         # 平均0の正規分布によるエラー項の標準偏差(デフォルトはNULLで、NULLの場合methodごとに決まった値が使用される)\n  factors = FALSE,        # predictorに二値変数が含まれる場合のfactor化有無(デフォルトはFALSE)\n  keep_truth = TRUE       # エラー項を含まない予測値の出力有無(デフォルトはFALSE)\n)\n\nstr(d2)\n\ntibble [100 × 22] (S3: tbl_df/tbl/data.frame)\n $ outcome     : num [1:100] 86.16 3.85 2.92 1.63 -29.98 ...\n $ predictor_01: num [1:100] 0.9404 1.8374 -5.0732 2.3541 0.0358 ...\n $ predictor_02: num [1:100] 1.365 4.276 -0.627 -3.694 -1.082 ...\n $ predictor_03: num [1:100] -3.072 -4.163 -0.148 5.433 -0.299 ...\n $ predictor_04: num [1:100] -8.62 -1.28 -4.46 1.77 -1.33 ...\n $ predictor_05: num [1:100] -3.616 0.904 -4.617 1.906 2.109 ...\n $ predictor_06: num [1:100] 0.34 -1.695 0.242 -2.087 -3.289 ...\n $ predictor_07: num [1:100] -4.21 7.15 2.62 -4.61 3.4 ...\n $ predictor_08: num [1:100] 3.15 1.1 5.92 1.34 -1.47 ...\n $ predictor_09: num [1:100] -6.019 -4.679 5.888 -0.535 4.073 ...\n $ predictor_10: num [1:100] 5.38 -4.09 -2.12 -1.67 -0.93 ...\n $ predictor_11: num [1:100] -0.748 -3.666 -0.218 4.095 -1.373 ...\n $ predictor_12: num [1:100] -1.35 4.332 -0.466 -3.065 4.615 ...\n $ predictor_13: num [1:100] -1.597 -1.496 -3.45 0.379 1.419 ...\n $ predictor_14: num [1:100] 3.372 2.093 0.543 -0.509 -0.704 ...\n $ predictor_15: num [1:100] -2.921 -0.299 -0.332 3.577 -4.968 ...\n $ predictor_16: num [1:100] -0.806 0.353 -2.567 6.526 -3.973 ...\n $ predictor_17: num [1:100] 1.634 1.589 -2.139 0.177 -2.733 ...\n $ predictor_18: num [1:100] -0.44 2.02 2.43 5.27 -3.65 ...\n $ predictor_19: num [1:100] -2.093 0.054 -0.538 -1.95 -5.075 ...\n $ predictor_20: num [1:100] -4.099 1.618 -3.966 -0.844 -6.315 ...\n $ .truth      : num [1:100] 80.258 5.973 4.227 0.864 -26.845 ...\n\n\npredictorの数や生成方法、およびそれらのpredictorから目的変数outcomeを計算する算式は、method引数の指定により異なります。\n上の実行例で指定した”sapp_2014_1”では、平均0、分散9の正規乱数を独立に20個生成してpredictorとし、以下コードに示す計算式の算出結果に正規分布に従うエラー項を加算してoutcomeを作成しています。\n\ny &lt;- with(d2,\n     # \"sapp_2014_1\"の算式\n       predictor_01 + sin(predictor_02) + log(abs(predictor_03)) +\n       predictor_04^2 + predictor_05 * predictor_06 +\n       ifelse(predictor_07 * predictor_08 * predictor_09 &lt; 0, 1, 0) +\n       ifelse(predictor_10 &gt; 0, 1, 0) + predictor_11 * ifelse(predictor_11 &gt; 0, 1, 0) +\n       sqrt(abs(predictor_12)) + cos(predictor_13) + 2 * predictor_14 + abs(predictor_15) +\n       ifelse(predictor_16 &lt; -1, 1, 0) + predictor_17 * ifelse(predictor_17 &lt; -1, 1, 0) -\n       2 * predictor_18 - predictor_19 * predictor_20\n)\n\n# パッケージの計算結果との比較\nhead(data.frame(list(\n  .truth = d2$.truth,\n  check = y\n)))\n\n       .truth       check\n1  80.2583408  80.2583408\n2   5.9726707   5.9726707\n3   4.2265011   4.2265011\n4   0.8642132   0.8642132\n5 -26.8451499 -26.8451499\n6  -7.8300851  -7.8300851\n\n\nバージョン1.4.0時点では、生成方法を指定するmethod引数として、“sapp_2014_1”以外にも”sapp_2014_2”、“van_der_laan_2007_1”および”van_der_laan_2007_2”が選択可能です。 それぞれの生成方法の詳細はリファレンス[2]をご参照ください。\n\n\nsim_noise\nsim_noise関数は平均0、分散1の正規分布に従う乱数を生成します。\n\nd3 &lt;- sim_noise(\n  num_samples = 10,           # 生成するレコード数\n  num_vars = 3,               # 生成するカラム(正規乱数)の数\n  cov_type = \"exchangeable\",  # 分散共分散行列の設定(デフォルトは\"exchangeable\")\n  outcome = \"none\",           # outcome変数の出力有無(デフォルトは\"none\")\n  num_classes = 2,            # outcome=\"classification\"の場合のクラス数(デフォルトは2)\n  cov_param = 0               # 変数間の共分散に関するパラメータ(デフォルトは0)\n)\n\nstr(d3)\n\ntibble [10 × 3] (S3: tbl_df/tbl/data.frame)\n $ noise_1: num [1:10] 0.6842 0.5628 -0.0469 -1.1456 0.1879 ...\n $ noise_2: num [1:10] 0.961 0.306 -0.15 0.465 -0.471 ...\n $ noise_3: num [1:10] 2.11547 0.80998 -0.00872 0.16157 -0.91603 ...\n\n\n各レコードはnoise_iとしてnum_varsで指定した数だけ変数が生成されていますが、これらは多変量正規分布に従っています。 この多変量正規分布は、前述のとおり平均は0、分散共分散行列の対角成分(分散)は1に固定されていますが、変数間の共分散は引数cov_typeとcov_paramで制御します。 cov_type=“exchangeable”を指定すると、どの(異なる)変数間の共分散もcov_paramで設定した値になります。 一方、cov_type=“toeplitz”では、cov_paramで指定した値から、変数間のインデックスが離れるにつれて共分散が指数的に変化するような行列となります。(例えば、noise_1とnoise_2間の共分散がcov_param=0.5とすると、noise_1とnoise_3間の共分散は0.5^2=0.25、noise_1とnoise_4間の共分散は0.5^3=0.125等となります。)\n引数outcomeに”regression”または”classification”を指定すると、それぞれ数値変数及びクラスを示すファクター変数が出力されますが、いずれも変数noise_iとは関連のない、ランダムな値が設定されます。\n\n\nsim_logistic、sim_multinomial\nsim_logistic関数及びsim_multinomial関数は、それぞれ2クラス分類および3クラス分類用のデータを生成します。 平均0、分散1、相関係数が引数correlationに従う正規乱数AおよびBの2変数を説明変数として使用しますが、クラス確率を計算するためのAとBの算式を、柔軟に設定できる点が特徴です。\n以下はsim_logistic関数による実行例です。 引数eqnには”~“から始まる2変数AとBの式、すなわち右辺のみのformulaオブジェクトを指定します。 eqnで指定した数式で計算した値をロジスティック関数によりクラス1確率に変換し、当該確率をもとにランダムにクラスを割り当てています。\n\nd4 &lt;- sim_logistic(\n  num_samples = 1000,        # 生成するレコード数\n  eqn = ~ 2 * A + B^2 + 1.2, # 2つの正規乱数AとBにより予測子を計算する算式\n  #eqn = rlang::expr(2 * A + B^2 + 1.2),           # 参考：expressionで指定することも可能\n  #eqn = base::expression(2 * A + B^2 + 1.2)[[1]], # 参考：expression listは指定できないため注意\n  correlation = 0,           # 変数AとBの相関係数(デフォルトは0)\n  keep_truth = TRUE          # クラス1確率の出力有無(デフォルトはFALSE)\n)\n\nstr(d4)\n\ntibble [1,000 × 5] (S3: tbl_df/tbl/data.frame)\n $ A           : num [1:1000] 0.104 1.375 -2.597 0.526 -1.029 ...\n $ B           : num [1:1000] -0.858 -1.359 0.397 0.504 1.75 ...\n $ .linear_pred: num [1:1000] 2.15 5.8 -3.84 2.5 2.21 ...\n $ .truth      : num [1:1000] 0.8953 0.997 0.0211 0.9245 0.9008 ...\n $ class       : Factor w/ 2 levels \"one\",\"two\": 1 1 2 1 1 1 1 2 1 1 ...\n\n\n\n# クラス確率が指定した式に従って算出されていることの確認\np &lt;- 1 / (1 + exp(-(2*d4$A + d4$B^2 + 1.2)))\nhead(data.frame(list(\n  .truth = d4$.truth,\n  check = p\n)))\n\n     .truth     check\n1 0.8952554 0.8952554\n2 0.9969767 0.9969767\n3 0.0211242 0.0211242\n4 0.9244752 0.9244752\n5 0.9008286 0.9008286\n6 0.9437250 0.9437250\n\n\nsim_multinomial関数は、3クラス分類用のデータを生成します。\n説明変数はsim_logistic関数と同様、正規分布に従う乱数AとBの二つですが、AとBの算式として、三つのクラスに対応する三つの式eqn_1～eqn_3を指定します。 これら三つの式の算出結果を入力とするソフトマックス関数P(class=k)=exp(eqn\\_k)/\\Sigma_{i}{exp(eqn\\_i)}により各クラスのクラス確率を計算し、当該確率をもとに、ランダムに3クラスのいずれかを割り当てています。\n\nd5 &lt;- sim_multinomial(\n  num_samples = 10,      # 生成するレコード数\n  eqn_1 = ~ A + B,       # クラス1に対応する式\n  eqn_2 = ~ 2 * A * B,   # クラス2に対応する式\n  eqn_3 = ~ log(abs(B)), # クラス3に対応する式\n  correlation = 0,       # 変数AとBの相関係数(デフォルトは0)\n  keep_truth = TRUE      # 各クラスのクラス確率の出力有無(デフォルトはFALSE)\n)\n\nstr(d5)\n\ntibble [10 × 6] (S3: tbl_df/tbl/data.frame)\n $ A           : num [1:10] -1.236 0.668 -1.119 0.173 0.375 ...\n $ B           : num [1:10] 0.184 1.404 0.44 0.216 -0.84 ...\n $ class       : Factor w/ 3 levels \"one\",\"two\",\"three\": 1 1 1 2 1 3 2 2 1 2\n $ .truth_one  : num [1:10] 0.299 0.5 0.384 0.533 0.314 ...\n $ .truth_two  : num [1:10] 0.543 0.411 0.283 0.389 0.266 ...\n $ .truth_three: num [1:10] 0.158 0.0885 0.3331 0.0779 0.4196 ...\n\n\n\n# クラス確率が指定した式に従って算出されていることの確認\n\n## 引数で指定した三つの算式\ny1 &lt;- d5$A + d5$B\ny2 &lt;- 2 * d5$A * d5$B\ny3 &lt;- log(abs(d5$B))\n\n## ソフトマックス関数によるクラス確率\np1 &lt;- exp(y1) / (exp(y1) + exp(y2) + exp(y3))\np2 &lt;- exp(y2) / (exp(y1) + exp(y2) + exp(y3))\np3 &lt;- exp(y3) / (exp(y1) + exp(y2) + exp(y3))\n\n## パッケージによる計算結果と比較\nhead(data.frame(list(\n  .truth_one = d5$.truth_one,\n  check_one = p1,\n  .truth_two = d5$.truth_two,\n  check_two = p2,\n  .truth_three = d5$.truth_three,\n  check_three = p3\n)))\n\n  .truth_one check_one .truth_two check_two .truth_three check_three\n1  0.2991672 0.2991672  0.5428603 0.5428603   0.15797249  0.15797249\n2  0.5003462 0.5003462  0.4111912 0.4111912   0.08846260  0.08846260\n3  0.3839573 0.3839573  0.2829503 0.2829503   0.33309237  0.33309237\n4  0.5328413 0.5328413  0.3892420 0.3892420   0.07791673  0.07791673\n5  0.3140786 0.3140786  0.2662960 0.2662960   0.41962537  0.41962537\n6  0.5666486 0.5666486  0.3759692 0.3759692   0.05738220  0.05738220",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>modeldata</span>"
    ]
  },
  {
    "objectID": "articles/modeldata.html#参考資料",
    "href": "articles/modeldata.html#参考資料",
    "title": "modeldata",
    "section": "参考資料",
    "text": "参考資料\n[1] Kuhn et al., (2020). Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org.\n[2] Kuhn M (2024). modeldata: Data Sets Useful for Modeling Examples. R package version 1.4.0, https://github.com/tidymodels/modeldata, https://modeldata.tidymodels.org.\n[3] Kuhn M (2019). The caret Package.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>modeldata</span>"
    ]
  },
  {
    "objectID": "articles/palmerpenguins.html",
    "href": "articles/palmerpenguins.html",
    "title": "palmerpenguins",
    "section": "",
    "text": "パッケージの概要\npalmerpenguins パッケージは、南極大陸のパルマ―群島に生息する三種類のペンギン（Gentoo：ジェンツーペンギン、Adelie：アデリーペンギン、Chinstrap：ヒゲペンギン）の体長や体重などに関する penguins データセットなどを収録したパッケージです。特に、penguins データセットは、探索的データ分析や可視化の練習で定番となっている iris データセットに代わる選択肢となることを目指して作成されています。",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>palmerpenguins</span>"
    ]
  },
  {
    "objectID": "articles/palmerpenguins.html#penguins-データセット",
    "href": "articles/palmerpenguins.html#penguins-データセット",
    "title": "palmerpenguins",
    "section": "penguins データセット",
    "text": "penguins データセット\n\n\n\n変数名\nデータ型\n概要\n\n\n\n\nspecies\nfactor\nペンギンの種類\n\n\nisland\nfactor\n生息している島の名前\n\n\nbill_length_mm\ndouble\nくちばしの長さ [mm]\n\n\nbill_depth_mm\ndouble\nくちばしの太さ [mm]\n\n\nflipper_length_mm\ninteger\nフリッパー（翼）の長さ [mm]\n\n\nbody_mass_g\ninteger\n体重 [g]\n\n\nsex\nfactor\n性別\n\n\nyear\ninteger\n測定年\n\n\n\n\nlibrary(palmerpenguins)\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\nlibrary(ggplot2)\n\nDT::datatable(penguins)",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>palmerpenguins</span>"
    ]
  },
  {
    "objectID": "articles/palmerpenguins.html#ggplot2-パッケージによる可視化の例",
    "href": "articles/palmerpenguins.html#ggplot2-パッケージによる可視化の例",
    "title": "palmerpenguins",
    "section": "ggplot2 パッケージによる可視化の例",
    "text": "ggplot2 パッケージによる可視化の例\n\n# カラーパレットの定義\ncolors &lt;- c(\"darkorange\",\"darkred\",\"darkcyan\")\n\n# 可視化の例\nggplot(na.omit(penguins),\n       aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species),\n             size = 2.5, alpha = 0.7) +\n  scale_color_manual(values = colors) +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"フリッパー（翼）の長さ [mm]\",\n       y = \"体重 [g]\",\n       color = \"種類\",\n       shape = \"種類\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n種類と生息地の関係\n\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(alpha = 0.8) + # 棒グラフを指定\n  scale_fill_manual(values = colors, guide = \"none\") + # 塗り分け方を指定\n  facet_wrap(~species, ncol = 1) + # 種類ごとにグラフを分ける\n  coord_flip() + # x軸とy軸を入れ替えて水平にする\n  labs(y=\"データに含まれる個体数\")\n\n\n\n\n\n\n\n\nアデリーペンギンはすべての島で見つかっていますが、ヒゲペンギンは Dream 島、ジェンツーペンギンは Biscoe 島だけで生活しているようです。\n\nくちばしの長さと太さの関係\n\n\nggplot(na.omit(penguins),\n       aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(shape = species),\n             color = \"grey\", size = 2.5, alpha = 0.7) +\n  geom_smooth(method = \"lm\") +\n  scale_color_manual(values = colors) +\n  labs(x = \"くちばちの長さ [mm]\", y = \"くちばしの太さ [mm]\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nペンギンたちのくちばしの長さと太さについて散布図を描いてみると、「くちばしは長いほど細い」という相関関係があることがわかります。\n\nggplot(na.omit(penguins),\n       aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species, shape = species),\n             size = 2.5, alpha = 0.7) +\n  geom_smooth(method = \"lm\",\n              aes(color = species, group = species)) +\n  scale_color_manual(values = colors) +\n  labs(x = \"くちばちの長さ [mm]\", y = \"くちばしの太さ [mm]\",\n       color = \"種類\", shape = \"種類\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nしかし、散布図をペンギンの種類ごとに色分けして観察すると、それぞれの種類の中では「くちばしは長いほど太い」という相関関係があることがわかります。\npalmerpenguins の公式ページには、これらのほかにggplot2 パッケージを用いた可視化の例が豊富に示されています。",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>palmerpenguins</span>"
    ]
  },
  {
    "objectID": "articles/parsnip.html",
    "href": "articles/parsnip.html",
    "title": "parsnip",
    "section": "",
    "text": "パッケージの概要\nparsnipは、機械学習や予測モデリングを行うパッケージ群tidymodelsに含まれるパッケージで、様々なモデルを統一的な記法で使用するためのインターフェースを提供します。 一つのアルゴリズム、例えばランダムフォレストのモデルを構築しようとしたときに、パッケージはranger、randomForest等複数の選択肢が存在し、その実行方法やパラメータの名称、設定方法は微妙に異なっていることもあるかもしれません。 parsnipは、これらのパッケージ間の実行方法の違いを吸収してくれるパッケージです。\nparsnipの概要を示す例として、ランダムフォレストによる回帰モデルを作成します。 サンプルデータとして、modeldataパッケージに含まれているカリフォルニア州サクラメント市の住宅価格のデータSacramentoを用います。\nlibrary(parsnip)\nlibrary(modeldata)\nlibrary(dplyr)\n\nset.seed(1234)\n\ndata(\"Sacramento\", package=\"modeldata\") \n# 今回はカーディナリティの高い(レコード数に対して取り得る値が多い)特徴量は除外\nd &lt;- Sacramento[, !(colnames(Sacramento) %in% \n                      c(\"city\", \"zip\", \"latitude\", \"longitude\"))] \nstr(d)\n\ntibble [932 × 5] (S3: tbl_df/tbl/data.frame)\n $ beds : int [1:932] 2 3 2 2 2 3 3 3 2 3 ...\n $ baths: num [1:932] 1 1 1 1 1 1 2 1 2 2 ...\n $ sqft : int [1:932] 836 1167 796 852 797 1122 1104 1177 941 1146 ...\n $ type : Factor w/ 3 levels \"Condo\",\"Multi_Family\",..: 3 3 3 3 3 1 3 3 1 3 ...\n $ price: int [1:932] 59222 68212 68880 69307 81900 89921 90895 91002 94905 98937 ...\nまずはrandomForestパッケージを使用します。 ここでは各パッケージをそのまま使用する場合とparsnipを使用する場合との違いを示すことが目的ですので詳細は割愛しますが、Sacramentoデータのpriceを目的変数として、他の変数で回帰するランダムフォレストモデルを構築しています。\n# From randomForest\nlibrary(randomForest)\nrf_1 &lt;- randomForest(\n  price ~ ., \n  data = d, \n  mtry = 3, \n  ntree = 200, \n  nodesize = 3,\n  importance = TRUE\n)\nrf_1\n\n\nCall:\n randomForest(formula = price ~ ., data = d, mtry = 3, ntree = 200,      nodesize = 3, importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 200\nNo. of variables tried at each split: 3\n\n          Mean of squared residuals: 7646314367\n                    % Var explained: 55.48\n次にrangerパッケージで同じ内容のランダムフォレストモデルを構築します。 この二つのパッケージの例では実行方法に大きな違いはありませんが、それでもなお、指定するハイパーパラメータの変数名が一部異なっていることがわかります。\n# From ranger\nlibrary(ranger)\nrf_2 &lt;- ranger(\n  price ~ ., \n  data = d, \n  mtry = 3, \n  num.trees = 200, \n  min.node.size = 3,\n  importance = \"impurity\"\n)\nrf_2\n\nRanger result\n\nCall:\n ranger(price ~ ., data = d, mtry = 3, num.trees = 200, min.node.size = 3,      importance = \"impurity\") \n\nType:                             Regression \nNumber of trees:                  200 \nSample size:                      932 \nNumber of independent variables:  4 \nMtry:                             3 \nTarget node size:                 3 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       7674868879 \nR squared (OOB):                  0.5536376\n今度はparsnipを通して両パッケージのランダムフォレストを実行します。 二つの実行例のコードを見比べるとわかるように、parsnipを通して実行することで多くのパラメータが同じ変数名で指定できるようになります。\n# From randomForest\nrand_forest(mtry = 3, trees = 200, min_n = 3) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"randomForest\", importance = TRUE) %&gt;%\n  fit(price ~ ., data = d)\n\nparsnip model object\n\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, ntree = ~200, mtry = min_cols(~3,      x), nodesize = min_rows(~3, x), importance = ~TRUE) \n               Type of random forest: regression\n                     Number of trees: 200\nNo. of variables tried at each split: 3\n\n          Mean of squared residuals: 7633083891\n                    % Var explained: 55.56\n# From ranger\nrand_forest(mtry = 3, trees = 200, min_n = 3) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  fit(price ~ ., data = d)\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~3,      x), num.trees = ~200, min.node.size = min_rows(~3, x), importance = ~\"impurity\",      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Regression \nNumber of trees:                  200 \nSample size:                      932 \nNumber of independent variables:  4 \nMtry:                             3 \nTarget node size:                 3 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       7655884183 \nR squared (OOB):                  0.5547417\nこの例のように、ランダムフォレストという同じモデルの同じハイパーパラメータであっても、パッケージによって変数名は異なっている場合があります。 また、パッケージによっては、目的変数と説明変数をformula形式(y ~ xのような形式)ではなく、それぞれを引数で与える方式を採用しているかもしれません。 parsnipはそのような差異を吸収し、統一的な記法でモデルを構築する仕組みを提供するパッケージです。",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>parsnip</span>"
    ]
  },
  {
    "objectID": "articles/parsnip.html#基本的な使用例１---回帰モデル",
    "href": "articles/parsnip.html#基本的な使用例１---回帰モデル",
    "title": "parsnip",
    "section": "基本的な使用例１ - 回帰モデル",
    "text": "基本的な使用例１ - 回帰モデル\n前項で示したランダムフォレストの例について、１ステップごとに解説しながら、parsnipの基本的な使用方法を紹介します。\n(1) モデルの生成\nまずは作成したいモデルの種類(例えば線形回帰、SVM、ランダムフォレスト等。parsnipではこれをmodel typeと呼ぶ。)に応じた関数を呼び出して、モデルを生成します。 これらの関数を呼び出すと、model_specという形式のデータでモデルが生成されます。 model_specはモデルに関する各種の情報を保持するparsnipのデータ形式で、モデルの種類に加えて、後述するモード、エンジン、パラメータといった情報を付加していきます。\nランダムフォレストの場合、モデル生成関数はrand_forest関数を使用します。 次のコードで、rand_forest関数にハイパーパラメータを与えて実行し、ランダムフォレストのmodel_specオブジェクトが作成されています。\n\nrf_spec &lt;- rand_forest(mtry = 3, trees = 200)\nrf_spec\n\nRandom Forest Model Specification (unknown mode)\n\nMain Arguments:\n  mtry = 3\n  trees = 200\n\nComputational engine: ranger \n\n\nparsnipでは、モデルに与える引数は大きく、メイン引数(main arguments)とエンジン引数(engine arguments)に分けられます。 メイン引数は、ある特定のモデルにおいて、パッケージの種類によらずどのパッケージでも指定する必要があるような基本的なモデルのパラメータを指し、モデルの種類(およびその生成関数)によって決められています。\nメイン引数は、モデルの生成関数を呼び出す際に引数として渡します。 上の例ではランダムフォレストの生成関数であるrand_forest関数に、メイン引数として二つの引数(一つ一つの決定木を作成する際に用いる特徴量の数”mtry”、および決定木の数”trees”)を渡しています。 この二つのパラメータは、パッケージによって変数名は異なる場合があるものの、ランダムフォレストを実行するためにはどのパッケージでも指定しなければならないような基本的なパラメータです。(省略した場合、デフォルトの値が設定されます。)\nこれに対してエンジン引数は、エンジンに指定したパッケージで使用できる引数のうち、メイン引数以外のものを指しています。 エンジン引数はモデルのパッケージ(エンジン)を指定する際に合わせて引数として指定するので、「エンジンの設定」の項で解説します。\n(2) モードの設定\nモデルをどのような課題に対して使用するかをset_mode関数で指定しますが、parsnipではこれをモードと呼びます。 今回はランダムフォレストを回帰モデルとして使用するため、モードとして”regression”を指定します。\n\nrf_spec &lt;- set_mode(rf_spec, mode = \"regression\")\nrf_spec\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 3\n  trees = 200\n\nComputational engine: ranger \n\n\n実行結果を見ると、1行目の(unknown)となっていた箇所が、(regression)に更新されたことが確認できます。\n指定できるモードの種類はモデルの種類ごとに決まっています。 ランダムフォレストでは、回帰に用いる場合の”regression”以外にも、分類に用いる場合の”classification”および生存時間分析に用いる場合の“censored regression”が指定できます。 また、モデルの種類によっては、教師なし学習のモデルに用いる”clustering”、生存時間分析のモデルに用いる“risk regression”といったモードが用意されています。\n(3) エンジンの設定\n生成したモデルの具体的な実装について、set_engine関数でパッケージ(エンジン)を指定します。 ランダムフォレストのパッケージとして、例えば”randomForest”を指定します。\n\nrf_spec &lt;- set_engine(rf_spec, engine = \"randomForest\", importance = TRUE)\nrf_spec\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 3\n  trees = 200\n\nEngine-Specific Arguments:\n  importance = TRUE\n\nComputational engine: randomForest \n\n\n実行結果を見ると、最終行に表示されているエンジンの指定が、デフォルトの”ranger”から”randomForest”に更新されたことが確認できます。\n上の例では、set_engine関数に”importance”という引数も渡しています。 このimportance引数は、randomForestパッケージでランダムフォレストを実行する際に使用する引数で、特徴量重要度を算出するかどうかを制御するためのものです。 randomForestパッケージで使用されている引数でありながら、parsnipのrand_forest関数ではメイン引数となっていない引数なので、先に述べた「エンジン引数」に該当します。 このようにエンジン引数は、使用するエンジンの指定と併せて、エンジンとして使用するパッケージ内での引数名のまま、set_engine関数に引き渡して使用することができます。\n(4) モデルのフィッティング\nここまででモデルの設定が完了したので、データによる学習(フィッティング)を行います。 モード、エンジンおよびパラメータを設定済みのmodel_spec、学習用のデータ、および目的変数と説明変数の関係を表す式(formula。R標準のlm等で使用するものと同じです。)をfit関数に渡します。 fit関数を実行すると、学習済のparsnipモデルとしてmodel_fitという形式のデータが返ります。\n\n# train dataとtest dataに分割\nn_train &lt;- floor(nrow(d) * 0.75)\nd_train &lt;- d[1:n_train,]\nd_test &lt;- d[(n_train+1):nrow(d),]\n\n# モデルのフィッティング\nrf_fit &lt;- fit(rf_spec, price ~ ., data = d_train)\nrf_fit\n\nparsnip model object\n\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, ntree = ~200, mtry = min_cols(~3,      x), importance = ~TRUE) \n               Type of random forest: regression\n                     Number of trees: 200\nNo. of variables tried at each split: 3\n\n          Mean of squared residuals: 7699846058\n                    % Var explained: 51.66\n\n\nなお、説明変数と目的変数の関係をformula形式で指定する方法以外にも、説明変数と目的変数をそれぞれ引数xとyで直接指定する方式のfit_xy関数が用意されています。\n\nrf_fit &lt;- fit_xy(rf_spec, \n                 x = d_train[, names(d_train)[names(d_train) != \"price\"]], \n                 y = d_train$price)\nrf_fit\n\nparsnip model object\n\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, ntree = ~200, mtry = min_cols(~3,      x), importance = ~TRUE) \n               Type of random forest: regression\n                     Number of trees: 200\nNo. of variables tried at each split: 3\n\n          Mean of squared residuals: 7671604694\n                    % Var explained: 51.83\n\n\nさて、(1)から(4)では解説のために、モデルの生成から学習までの一連の処理を、一つ一つのステップへと分解して実行してきました。 実際に使用する際は、以下のようにパイプ演算子でつなぎながら実行することで流れがわかりやすくなります。\n\nrf_fit &lt;- rand_forest(mtry = 3, trees = 200) %&gt;%\n  set_mode(mode = \"regression\") %&gt;%\n  set_engine(engine = \"randomForest\", importance = TRUE) %&gt;%\n  fit(price ~ ., data = d_train)\n\n(補足) モデルスペックの様々な設定方法\nモデルスペックの設定について、ここまでの例で示したものと異なる設定方法を紹介します。\nまずは、モデルのモードとエンジンはそれぞれset_mode関数およびset_engine関数を使用して設定しましたが、モデル生成関数の引数に指定して設定することもできます。\n\n# モデル生成用の関数(この例ではrand_forest)でモードとエンジンを設定する方式\nrf_spec &lt;- rand_forest(\n  mode = \"regression\",\n  engine = \"ranger\",\n  mtry = 3, \n  trees = 200\n)\n\n続いて、一度作成したmodel_specのメイン引数(ハイパーパラメータ)を更新するupdate関数を紹介します。 次のコードは、上の例で作成したランダムフォレストのmodel_specについて、mtryの更新および設定していなかったmin_nを新たに設定しています。\n\nrf_spec &lt;- update(rf_spec, mtry = 5, min_n = 3, fresh = FALSE) \n# fresh=TRUEにするとパラメータ全体を入れ替える\n# (この場合、update関数で指定していないtreesパラメータの設定は初期化される)\n\n最後に、model_specのモードとパラメータを維持したまま他のエンジンに変換する、translate関数を紹介します。 次の例では、上で生成したランダムフォレストのmodel_specを、rangerエンジンからrandomForestエンジンに変換しています。 なお、エンジン引数を設定している場合、エンジン引数もそのまま引き継がれますが、前述のようにエンジン引数は本来的にパッケージごとに異なるパラメータですので、変換後のパッケージでは使用できないこともある点に注意が必要です。\n\ntranslate(rf_spec, engine = \"randomForest\")\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 5\n  trees = 200\n  min_n = 3\n\nComputational engine: randomForest \n\nModel fit template:\nrandomForest::randomForest(x = missing_arg(), y = missing_arg(), \n    mtry = min_cols(~5, x), ntree = 200, nodesize = min_rows(~3, \n        x))\n\n\n(5) モデルによる予測\n学習済のモデルをテストデータに適用して、テストデータに対する予測値を算出します。 R標準のlm等のモデルと同じように、predict関数を使用することができます。\n\ny_pred = predict(rf_fit, d_test)[[1]]\ny_true = d_test$price\nplot(x = y_pred, y = y_true)\nabline(a=0, b=1, col=\"red\")\n\n\n\n\n\n\n\n\nなお予測値を取得する関数として、predict関数以外にも、parsnipではaugment関数が用意されています。 augment関数は学習済のモデルと予測対象のデータを引数にとり、データに対して予測値と残差のカラムを追加したデータセットを返します。\n\naugment(rf_fit, d_test)[1:10,]\n\n# A tibble: 10 × 7\n     .pred   .resid  beds baths  sqft type         price\n     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;        &lt;int&gt;\n 1 189186.   70814.     3   2    1196 Residential 260000\n 2 302809.  -42809.     3   2    1621 Residential 260000\n 3 303272.  -39772.     3   2    1811 Residential 263500\n 4 249206.   17304.     3   2    1540 Residential 266510\n 5 390607. -122857.     4   2.5  2647 Residential 267750\n 6 393610. -123610.     2   2    2750 Residential 270000\n 7 363463.  -92463.     3   2.5  1910 Residential 271000\n 8 261279.   11421.     4   2.5  1846 Residential 272700\n 9 244198.   30802.     3   2    1543 Residential 275000\n10 299032.  -23032.     5   3    2494 Residential 276000",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>parsnip</span>"
    ]
  },
  {
    "objectID": "articles/parsnip.html#基本的な使用例２---分類モデル",
    "href": "articles/parsnip.html#基本的な使用例２---分類モデル",
    "title": "parsnip",
    "section": "基本的な使用例２ - 分類モデル",
    "text": "基本的な使用例２ - 分類モデル\nここまでは、ランダムフォレストの回帰モデルを構築する例を通して、parsnipの基本的な使用方法を解説してきました。 本項では、parsnipの別の使用例として、勾配ブースティング決定木(GBM)を分類問題に適用する例を紹介します。\nこの使用例を通じて、モデルの種類やモードが異なる場合でも、parsnipでは同じ流れでモデル構築ができることを示します。 また、学習済のparsnipモデルから情報を取り出して、更なる分析に使用する方法を紹介します。\nサンプルデータとして、modeldataパッケージに含まれるcredit_dataを使用します。 顧客の信用度を示すStatus変数について、goodとbadを分類します。\n\nd2 &lt;- na.omit(credit_data)\n\n# factor型の説明変数は数値に変換(ラベルエンコーディング)する。\n# ※factor型のままでも自動的にone-hotエンコーディングされるためモデルの構築は可能。\n# 　今回は後続の説明上の理由からラベルエンコーディングしている。\nfct_vars &lt;- c('Home', 'Marital', 'Records', 'Job')\nd2[, fct_vars] &lt;- (sapply(d2[, fct_vars], as.integer))\n\nstr(d2)\n\n'data.frame':   4039 obs. of  14 variables:\n $ Status   : Factor w/ 2 levels \"bad\",\"good\": 2 2 1 2 2 2 2 2 2 1 ...\n $ Seniority: int  9 17 10 0 0 1 29 9 0 0 ...\n $ Home     : int  6 6 3 6 6 3 3 4 3 4 ...\n $ Time     : int  60 60 36 60 36 60 60 12 60 48 ...\n $ Age      : int  30 58 46 24 26 36 44 27 32 41 ...\n $ Marital  : int  2 5 2 4 4 2 2 4 2 2 ...\n $ Records  : int  1 1 2 1 1 1 1 1 1 1 ...\n $ Job      : int  2 1 2 1 1 1 1 1 2 4 ...\n $ Expenses : int  73 48 90 63 46 75 75 35 90 90 ...\n $ Income   : int  129 131 200 182 107 214 125 80 107 80 ...\n $ Assets   : int  0 0 3000 2500 0 3500 10000 0 15000 0 ...\n $ Debt     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Amount   : int  800 1000 2000 900 310 650 1600 200 1200 1200 ...\n $ Price    : int  846 1658 2985 1325 910 1645 1800 1093 1957 1468 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:415] 30 114 144 153 158 177 195 206 240 241 ...\n  ..- attr(*, \"names\")= chr [1:415] \"30\" \"114\" \"144\" \"153\" ...\n\n\n(1) モデルの生成～学習\nparsnipを通してGBMによる分類モデルを構築します。 次の例では、GBM用のモデル生成関数boost_treeを用いて、モードには分類用の”classification”、使用するGBMパッケージとして”xgboost”をエンジンに指定しています。 ここではGBMのハイパーパラメータ等の詳細は割愛しますが、モデルの生成、モードとエンジンの設定、学習という流れ自体は、ランダムフォレストと同じであることがわかると思います。\n\n# train dataとtest dataに分割\nn_train &lt;- floor(nrow(d2) * 0.75)\nd_train &lt;- d2[1:n_train,]\nd_test &lt;- d2[(n_train+1):nrow(d2),]\n\n# モデルの生成～フィッティング\ngbm_fit &lt;- \n  boost_tree(\n    mtry = 0.8,        # 木ごとに特徴量をサンプリングする割合\n    trees = 2000,      # 作成する木の本数\n    min_n = 1,         # 葉を分岐するために必要な最小のサンプル数\n    tree_depth = 5,    # 木ごとの最大の深さ\n    learn_rate = 0.05, # 学習率\n    sample_size = 0.8, # 木ごとにデータをサンプリングする割合\n    stop_iter = 300    # アーリーストッピング(一定のラウンド数で精度が上がらなければ打ち切る)を判定するラウンド数\n  ) %&gt;%\n  set_mode(mode = \"classification\") %&gt;%\n  set_engine(engine = \"xgboost\",       \n             eval_metric = \"logloss\",  # 損失関数の種類を指定\n             counts = FALSE,           # mtryを割合で指定するオプション\n             validation = 0.25) %&gt;%    # アーリーストッピングの判定に用いるデータの割合\n  fit(Status ~ ., data = d_train)\ngbm_fit\n\nparsnip model object\n\n##### xgb.Booster\nraw: 799.2 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.05, max_depth = 5, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 0.8, min_child_weight = 1, \n    subsample = 0.8), data = x$data, nrounds = 2000, watchlist = x$watchlist, \n    verbose = 0, early_stopping_rounds = 300, eval_metric = \"logloss\", \n    nthread = 1, objective = \"binary:logistic\")\nparams (as set within xgb.train):\n  eta = \"0.05\", max_depth = \"5\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"0.8\", min_child_weight = \"1\", subsample = \"0.8\", eval_metric = \"logloss\", nthread = \"1\", objective = \"binary:logistic\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  best_iteration, best_msg, best_ntreelimit, best_score, niter\ncallbacks:\n  cb.evaluation.log()\n  cb.early.stop(stopping_rounds = early_stopping_rounds, maximize = maximize, \n    verbose = verbose)\n# of features: 13 \nniter: 386\nbest_iteration : 86 \nbest_ntreelimit : 86 \nbest_score : 0.4452538 \nbest_msg : [86] validation-logloss:0.445254 \nnfeatures : 13 \nevaluation_log:\n  iter validation_logloss\n &lt;num&gt;              &lt;num&gt;\n     1          0.6733157\n     2          0.6546558\n   ---                ---\n   385          0.4888858\n   386          0.4891175\n\n\n学習済のモデルをテストデータに適用し、テストデータに対する分類結果とその正解率を算出します。\n\ny_pred &lt;- predict(gbm_fit, d_test)[[1]]\ny_true &lt;- d_test$Status\nsprintf(\"accuracy : %.4f\", sum(y_pred==y_true)/length(y_true))\n\n[1] \"accuracy : 0.7921\"\n\n\n(2) モデル情報の利用\nparsnipを通して学習させたモデルからより多くの情報を取得するため、parsnipのオブジェクトとしてではなく、エンジンに指定したパッケージのオブジェクトとして扱いたい場合があるかもしれません。 例えば、xgboostパッケージの学習済モデルであれば、xgboostのxgb.importance関数で特徴量重要度が計算できるので、parsnipで学習させたモデルをxgboostのモデルとして扱うことができれば都合がいいです。\nparsnipではextract_fit_engine関数が、そのような機能を提供します。 次の例では、上で学習させたparsnipモデルから、extract_fit_engine関数でxgboostモデルとしての情報を取り出し、xgb.importance関数に渡しています。\n\nimportance_gbm &lt;- gbm_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  xgb.importance(model = .)\nimportance_gbm\n\n      Feature        Gain      Cover  Frequency\n       &lt;char&gt;       &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1:    Income 0.162361809 0.15308392 0.15144738\n 2:     Price 0.152862530 0.19189145 0.18225867\n 3:    Amount 0.134619300 0.13369218 0.13318425\n 4: Seniority 0.118221478 0.10297003 0.08957634\n 5:       Age 0.081321036 0.08125753 0.11454839\n 6:   Records 0.077957485 0.05251855 0.02633868\n 7:    Assets 0.066155159 0.06963897 0.07764940\n 8:  Expenses 0.062748185 0.05693741 0.06994658\n 9:       Job 0.041249101 0.03177091 0.03093552\n10:      Time 0.036138501 0.03563342 0.04149584\n11:      Home 0.032315613 0.03410136 0.03640204\n12:      Debt 0.024199277 0.03237239 0.02932041\n13:   Marital 0.009850526 0.02413188 0.01689651\n\n\nこの特徴量重要度の情報を利用して、重要度が上位の変数のみを使用した一般化線形モデル(ロジスティック回帰)によるモデルを構築することを考えます。 特徴量重要度(Gain)の数値が、ある一定の値を超えている変数のみを使用して、parsnipを通したロジスティック回帰モデルを作成します。 ロジスティック回帰はlogistic_reg関数を使用しますが、モデル構築から予測までの流れはやはり、ランダムフォレストやGBMと同じです。\n\nd3 &lt;- na.omit(credit_data)\nd_train &lt;- d3[1:n_train,]\nd_test &lt;- d3[(n_train+1):nrow(d3),]\n\nglm_fit &lt;- \n  logistic_reg() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"glm\") %&gt;%  # エンジンとしてR標準のglmを使用\n  fit_xy(x=d_train[, importance_gbm[importance_gbm$Gain &gt; 0.05,][[\"Feature\"]]], \n         y=d_train$Status)\nglm_fit\n\nparsnip model object\n\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)       Income        Price       Amount    Seniority          Age  \n  1.094e+00    7.647e-03    1.310e-03   -2.288e-03    1.075e-01   -6.543e-03  \n Recordsyes       Assets     Expenses  \n -1.846e+00    2.478e-05   -1.219e-02  \n\nDegrees of Freedom: 3028 Total (i.e. Null);  3020 Residual\nNull Deviance:      3410 \nResidual Deviance: 2683     AIC: 2701\n\n\n\ny_pred &lt;- predict(glm_fit, d_test)[[1]]\ny_true &lt;- d_test$Status\nsprintf(\"accuracy : %.4f\", sum(y_pred==y_true)/length(y_true))\n\n[1] \"accuracy : 0.7752\"\n\n\nここではロジスティック回帰のエンジンとしてR標準のglmを使用していますが、glmのモデルは、summary関数で各説明変数の標準誤差等のより詳細な情報が確認できます。 そこで再度extract_fit_engine関数を使用して、parsnipモデルからglmモデルとしての情報を取り出し、summary関数に渡します。\n\nglm_fit %&gt;% \n  extract_fit_engine() %&gt;%\n  summary(.)\n\n\nCall:\nstats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.094e+00  2.274e-01   4.811 1.50e-06 ***\nIncome       7.647e-03  8.305e-04   9.207  &lt; 2e-16 ***\nPrice        1.310e-03  1.634e-04   8.021 1.05e-15 ***\nAmount      -2.288e-03  1.966e-04 -11.641  &lt; 2e-16 ***\nSeniority    1.075e-01  9.039e-03  11.890  &lt; 2e-16 ***\nAge         -6.543e-03  5.264e-03  -1.243  0.21388    \nRecordsyes  -1.846e+00  1.287e-01 -14.340  &lt; 2e-16 ***\nAssets       2.478e-05  7.969e-06   3.109  0.00188 ** \nExpenses    -1.219e-02  2.582e-03  -4.722 2.33e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3410.5  on 3028  degrees of freedom\nResidual deviance: 2682.8  on 3020  degrees of freedom\nAIC: 2700.8\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>parsnip</span>"
    ]
  },
  {
    "objectID": "articles/parsnip.html#parsnipで様々なモデルを使用する",
    "href": "articles/parsnip.html#parsnipで様々なモデルを使用する",
    "title": "parsnip",
    "section": "parsnipで様々なモデルを使用する",
    "text": "parsnipで様々なモデルを使用する\nここまでの例で示してきたように、parsnipではモデルの種類、エンジン、モードの組み合わせでモデルを特定します。 あるモデルの種類に関して、parsnipが対応しているパッケージ(エンジン)とモードの組み合わせを表示するshow_engines関数が用意されています。 例えばGBMについてこれらの情報を知りたい場合、show_engines関数に、GBMの生成関数の名称である”boost_tree”を引数として渡して実行します。\n\nshow_engines(\"boost_tree\")\n\n# A tibble: 5 × 2\n  engine  mode          \n  &lt;chr&gt;   &lt;chr&gt;         \n1 xgboost classification\n2 xgboost regression    \n3 C5.0    classification\n4 spark   classification\n5 spark   regression    \n\n\ntidymodelsの公式サイトではparsnipが対応しているモデル、エンジン、モードの全ての組み合わせが一覧化されているので、そちらもご参照ください。\n\nSearch parsnip models. https://www.tidymodels.org/find/parsnip/.\n\nまた、更に発展的な使用方法として、parsnipに用意されていないモデルを新たに登録し、parsnipのインターフェースで実行できるようにする方法も用意されています。 これにより、parsnipベースで記述したコードを再利用できたり、tidymodelsの他のパッケージとの連携が可能になるといった利点が考えられます。 モデルの追加はparsnipのディベロッパー・ツールとして用意されている関数を使用します。 次のコードは新たなモデルとそのモード、エンジンとなるパッケージを登録しています。\n\n# 架空のnewpkgパッケージをnew_modelの回帰モデルとして登録する\nset_new_model(\"new_model\")\nset_model_mode(model = \"new_model\", mode = \"regression\")\nset_model_engine(\n  model = \"new_model\",\n  mode = \"regression\",\n  eng = \"newpkg\"\n)\nset_dependency(\"new_model\", eng = \"newpkg\", pkg = \"newpkg\")\nshow_model_info(\"new_model\")\n\nInformation for `new_model`\n modes: unknown, regression \n\n engines: \n   regression: newpkgNA\n\n\n no registered arguments.\n\n no registered fit modules.\n\n no registered prediction modules.\n\n\n実際に使用するには更に、モデル生成関数の作成、引数の設定、fit関数及びpreditct関数に対する動作の設定等が必要になります。 内容はやや高度になりますので、興味がある方はtidymodels公式サイトの以下記事をご参照ください。\n\nHow to build a parsnip model. https://www.tidymodels.org/learn/develop/models/.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>parsnip</span>"
    ]
  },
  {
    "objectID": "articles/parsnip.html#参考資料",
    "href": "articles/parsnip.html#参考資料",
    "title": "parsnip",
    "section": "参考資料",
    "text": "参考資料\n[1] Kuhn et al., (2020). Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org.\n[2] Kuhn M, Vaughan D (2025). parsnip: A Common API to Modeling and Analysis Functions. R package version 1.3.1, https://parsnip.tidymodels.org/, https://github.com/tidymodels/parsnip.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>parsnip</span>"
    ]
  },
  {
    "objectID": "articles/partykit.html",
    "href": "articles/partykit.html",
    "title": "partykit",
    "section": "",
    "text": "パッケージの概要\n木構造の回帰・分類モデルを表現・要約し可視化するためのパッケージです。",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>partykit</span>"
    ]
  },
  {
    "objectID": "articles/partykit.html#使用例",
    "href": "articles/partykit.html#使用例",
    "title": "partykit",
    "section": "使用例",
    "text": "使用例\nrpartパッケージにて決定木モデルを構築し、その可視化を行います。データはKyphosisデータ（脊椎矯正手術を受けた子供のデータ）を使用します。\nまずはKyphosisデータの各種構造を確認します。\n\nKyphosis：absent(1)…手術後症状が消えた、present(2)…手術後症状が残った\nAge：手術をした子供の月齢\nNumber:手術に関与する脊椎の数\nStart：手術を受けた最上部の頸椎の番号\n\n\nstr(kyphosis)\n\n'data.frame':   81 obs. of  4 variables:\n $ Kyphosis: Factor w/ 2 levels \"absent\",\"present\": 1 1 2 1 1 1 1 1 1 2 ...\n $ Age     : int  71 158 128 2 1 1 61 37 113 59 ...\n $ Number  : int  3 3 4 5 4 2 2 3 2 6 ...\n $ Start   : int  5 14 5 1 15 16 17 16 16 12 ...\n\n\nrpartで決定木のモデルを生成します。\n\n(rp &lt;- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis))\n\nn= 81 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 81 17 absent (0.79012346 0.20987654)  \n   2) Start&gt;=8.5 62  6 absent (0.90322581 0.09677419)  \n     4) Start&gt;=14.5 29  0 absent (1.00000000 0.00000000) *\n     5) Start&lt; 14.5 33  6 absent (0.81818182 0.18181818)  \n      10) Age&lt; 55 12  0 absent (1.00000000 0.00000000) *\n      11) Age&gt;=55 21  6 absent (0.71428571 0.28571429)  \n        22) Age&gt;=111 14  2 absent (0.85714286 0.14285714) *\n        23) Age&lt; 111 7  3 present (0.42857143 0.57142857) *\n   3) Start&lt; 8.5 19  8 present (0.42105263 0.57894737) *\n\n\npartykitを使用して、rpartの決定木のモデルをより見やすくすることが出来ます。\n\nplot(as.party(rp))\n\n\n\n\n\n\n\n\n引数にてtype=’simple’と入力すると、より簡便的なモデル表示となります。\n\nplot(as.party(rp), type = 'simple')",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>partykit</span>"
    ]
  },
  {
    "objectID": "articles/pdp.html",
    "href": "articles/pdp.html",
    "title": "pdp",
    "section": "",
    "text": "パッケージの概要\npdpパッケージは、予測モデルとデータをもとに、PDP (Partial Dependence Plot、部分依存図) を作成する機能を実装したパッケージです。\nなお、PDPは、関心のある説明変数 X について、その値を x_1 に固定したときの予測値の期待値を考え、これをデータセットの全レコードについて X=x_1 と置き換えたときの予測値の平均で推定することによって、変数 X の影響を X に関する1変数関数として表現する手法です。\nlibrary(randomForest) # 予測モデルの構築に利用\n\nWarning: package 'randomForest' was built under R version 4.5.1\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\nlibrary(pdp) # bostonデータセットを利用\n\nWarning: package 'pdp' was built under R version 4.5.1\n\n# ランダムシードを固定する\nset.seed(42)\n\n# 予測モデルを構築する\nmodel.rf &lt;- randomForest(cmedv~., data = boston)",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>pdp</span>"
    ]
  },
  {
    "objectID": "articles/pdp.html#pd-プロットを作成する",
    "href": "articles/pdp.html#pd-プロットを作成する",
    "title": "pdp",
    "section": "PD プロットを作成する",
    "text": "PD プロットを作成する\n予測モデルに partial() 関数を適用することで、PD プロットを作成することができます。ただし 解釈しようとする予測モデルによっては、pred.fun（予測に使う関数）や train（学習用データ）などの引数を調整する必要があります。\n\npartial(model.rf, pred.var = \"lstat\", plot = TRUE, rug = TRUE)\n\n\n\n\n\n\n\n\nなお、randomForest パッケージには、PDプロットを作成するための独自の関数 partialPlot() が用意されています。\n\n# randomForest パッケージの partialPlot() 関数でも作成可能\npartialPlot(model.rf, pred.data = boston, x.var = \"lstat\")",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>pdp</span>"
    ]
  },
  {
    "objectID": "articles/pdp.html#ice-プロットを作成する",
    "href": "articles/pdp.html#ice-プロットを作成する",
    "title": "pdp",
    "section": "ICE プロットを作成する",
    "text": "ICE プロットを作成する\npartial() 関数では、ice = TRUE と指定することで、ICE プロットを描画することも可能です。\n\npartial(model.rf, pred.var = c(\"lstat\"), ice = TRUE, plot = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>pdp</span>"
    ]
  },
  {
    "objectID": "articles/plyr.html",
    "href": "articles/plyr.html",
    "title": "plyr",
    "section": "",
    "text": "パッケージの概要\nplyrパッケージはデータフレーム等のグループ化集計（グループごとに分割し、適用し、まとめる）に特化したパッケージです。 データフレーム操作パッケージとして名高いdplyrパッケージの前身にあたりますが、本パッケージは配列などのデータフレーム以外の操作にも対応しているのが特徴です。\n前述のようなグループ化集計にあたっては、R標準ではapply系関数を用いることになりますが、 その戻り値の型が場合によって異なること、多次元配列の操作のような高度な処理にはあまり向かないこと等の問題点がありました。 本パッケージではこれらの問題に対処したapply系関数の改良版として、**plyという名前1の関数を多数提供しています。\nplyrパッケージの名前は、その思想の根幹を成す**ply系関数から取られています2。",
    "crumbs": [
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>plyr</span>"
    ]
  },
  {
    "objectID": "articles/plyr.html#使用方法",
    "href": "articles/plyr.html#使用方法",
    "title": "plyr",
    "section": "使用方法",
    "text": "使用方法\n事前準備としてパッケージを読み込んでおきます。\n\nlibrary(plyr)\n\n\n**ply 系関数の基本\n**ply系関数は、Split-Apply-Combineというグループ化集計の流れを1つの関数で完結させるものです。\n\nSplit…データをグループごとに分割する\nApply…分割したデータそれぞれごとに、何らかの処理を適用する\nCombine…処理した結果を1つのデータにまとめる\n\nたとえば、irisデータセットにおいて、アヤメの種類ごとのデータの平均値を集計するには次のようにします。\n\n#Speciesの列にアヤメの種類が入っている\nhead(iris)\n\n\n  \n\n\n#第2引数にどのようにデータを分割するか、第3引数にどのような処理を適用するかを指定\ndaply(iris, .(Species), function(df){mean(df$Sepal.Length)})\n\n    setosa versicolor  virginica \n     5.006      5.936      6.588 \n\n\nここで、関数名のdaplyの最初の2文字は入出力データの型を表します。つまり、1文字目のdはデータフレームを入力とすること、2文字目のaは配列を出力とすることを表しています。\nこの命名規則のもと、**ply系関数には以下のようなものが存在しています3。\n\n\n\n\n\n\n\n\n\n\n入力型 ＼ 出力型\n配列(a)\nデータフレーム(d)\nリスト(l)\n出力なし(_)\n\n\n\n\n配列(a)\naaply\nadply\nalply\na_ply\n\n\nデータフレーム(d)\ndaply\nddply\ndlply\nd_ply\n\n\nリスト(l)\nlaply\nldply\nllply\nl_ply\n\n\n\n入力型の指定は、データの分割の指定方法が入力型によって異なる（後述）ことから設けられています。 また出力型の指定は、apply系関数では出力型が一定でなく扱いづらかったことへの対応として設けられたものです。\nなお、第4引数以降は第3引数の関数に引き継がれます。\n\ndaply(iris, .(Species), \n      function(df, offset){mean(df$Sepal.Length) + offset},\n      offset = 12345)\n\n    setosa versicolor  virginica \n  12350.01   12350.94   12351.59 \n\n\n以下では使用方法が特徴的なもののみを取り上げます。\n\nd*ply\nデータフレームを入力型とする場合、第2引数にはグループ化に使用する列を指定するのでした。\n\n#idはプレイヤーを表す文字列、teamはチーム名\nhead(plyr::baseball)\n\n\n  \n\n\n#比較的最近のデータのみを抽出\ndf_baseball &lt;- subset(baseball, year &gt;= 2003)\n\n#rbi(打点)の合計値を年度ごとに計算\n#summarise関数については後述\nddply(df_baseball, .(year), summarise, sum_rbi = sum(rbi))\n\n\n  \n\n\n\nこの第2引数には複数の列を指定することも可能です。\n\n#コンマで区切って複数指定\nhead(ddply(df_baseball, .(year, stint), summarise, sum_rbi = sum(rbi)))\n\n\n  \n\n\n#文字列ベクトルで指定することも可能\nhead(ddply(df_baseball, c(\"year\", \"stint\"), summarise, sum_rbi = sum(rbi)))\n\n\n  \n\n\n\n列名を用いた式を記述することも可能です。さらに、作成される列名を指定することもできます。\n\n#コンマで区切って複数指定\nddply(df_baseball, .(year &gt;= 2005, stint_0 = stint - 1), summarise, sum_rbi = sum(rbi))\n\n\n  \n\n\n\n\n\n*aply\n出力型が配列の場合は、グループ化に指定した列それぞれに配列の次元が対応する形になります。 たとえば、以下の例では2変数でグループ化しているため、2次元配列が出力されます。\n\ndaply(df_baseball, .(year, stint), summarise, sum_rbi = sum(rbi))\n\n      stint\nyear   1    2   3    4   \n  2003 4976 279 23   0   \n  2004 4490 161 3    NULL\n  2005 3359 50  4    NULL\n  2006 2332 204 12   NULL\n  2007 1689 43  NULL NULL\n\n\n\n\n*lply\n出力型がリストの場合は長さがグループ数のリストで出力されます。\n\nls &lt;- dlply(df_baseball, .(year, stint), function(df){sum(df$rbi)})\n#各グループがどのようなyearとstintの組み合わせであったかがdata.frameで記録されている\nls\n\n$`2003.1`\n[1] 4976\n\n$`2003.2`\n[1] 279\n\n$`2003.3`\n[1] 23\n\n$`2003.4`\n[1] 0\n\n$`2004.1`\n[1] 4490\n\n$`2004.2`\n[1] 161\n\n$`2004.3`\n[1] 3\n\n$`2005.1`\n[1] 3359\n\n$`2005.2`\n[1] 50\n\n$`2005.3`\n[1] 4\n\n$`2006.1`\n[1] 2332\n\n$`2006.2`\n[1] 204\n\n$`2006.3`\n[1] 12\n\n$`2007.1`\n[1] 1689\n\n$`2007.2`\n[1] 43\n\nattr(,\"split_type\")\n[1] \"data.frame\"\nattr(,\"split_labels\")\n   year stint\n1  2003     1\n2  2003     2\n3  2003     3\n4  2003     4\n5  2004     1\n6  2004     2\n7  2004     3\n8  2005     1\n9  2005     2\n10 2005     3\n11 2006     1\n12 2006     2\n13 2006     3\n14 2007     1\n15 2007     2\n\n\nこのとき、各要素がどのようなグループであったかが記録されており、 l*ply系関数に適用した場合にこの情報が参照されます。\n\n#直接daply関数を用いたときと同様の結果が得られる\nlaply(ls, function(x)x)\n\n      stint\nyear      1   2  3  4\n  2003 4976 279 23  0\n  2004 4490 161  3 NA\n  2005 3359  50  4 NA\n  2006 2332 204 12 NA\n  2007 1689  43 NA NA\n\n\n\n\na*ply\n配列を入力型とする場合、グループ化の指定方法がデータフレームの場合とは異なります。\nここではその動作を把握するため、ozoneという3次元配列を題材として使用します。\nこれは中央アメリカにおけるオゾン濃度を記録したデータで、 1つめの次元(lat)は緯度、2つめの次元(long)は経度、3つ目の次元(time)は観測時刻を表します。\n\nar_ozone &lt;- plyr::ozone\nstr(ar_ozone)\n\n num [1:24, 1:24, 1:72] 260 258 258 254 252 252 250 248 248 248 ...\n - attr(*, \"dimnames\")=List of 3\n  ..$ lat : chr [1:24] \"-21.2\" \"-18.7\" \"-16.2\" \"-13.7\" ...\n  ..$ long: chr [1:24] \"-113.8\" \"-111.3\" \"-108.8\" \"-106.3\" ...\n  ..$ time: chr [1:72] \"1\" \"2\" \"3\" \"4\" ...\n\n\nこのとき、a*ply関数の第2引数はグループ化に使用する次元を指定します。\n\nhead(adply(ar_ozone, 1, mean))\n\n\n  \n\n\nhead(adply(ar_ozone, 2, mean))\n\n\n  \n\n\nhead(adply(ar_ozone, 3, mean))\n\n\n  \n\n\n\n複数の次元を使用することも可能です。\n例えば各緯度・経度ごとの平均値を計算し、緯度・経度の次元を持つ配列に結果を格納するには次のようにします。\n\n#最初の5×5の地点のみ計算\naaply(ar_ozone[1:5, 1:5, ], 1:2, mean)\n\n       long\nlat       -113.8   -111.3   -108.8   -106.3   -103.8\n  -21.2 268.2500 268.9444 269.1389 269.4722 269.7500\n  -18.7 265.7500 265.7500 265.8611 266.2500 265.9444\n  -16.2 262.7778 263.0278 262.9444 263.3889 263.3611\n  -13.7 260.4722 260.8333 260.8611 260.8056 260.7778\n  -11.2 258.6667 259.0000 258.8611 258.8611 259.0556\n\n\n少し変わった用法ですが、データフレームに対して各行ごとに処理を行いたい場合に使用することもできます。\n\n#各行を単純に文字列結合したものを出力\nhead(adply(iris, 1, paste, collapse = \" \"))\n\n\n  \n\n\n\n\n\n\n基本形以外の **ply 系関数\n**ply系関数には前節で紹介した12種の基本形以外のものも存在します。4\n\nr*ply\nr*ply関数5は同じ処理を複数回繰り返し、その結果をデータフレームなどで出力するものです。\n\n#100個の[0, 1)一様乱数の平均をとる、という操作を20回繰り返す\nset.seed(42)\nrdply(20, mean(runif(100)))\n\n\n  \n\n\n\n\n\nm*ply\nm*ply関数6は同じ関数を異なる引数で繰り返すのに使用します。\n第1引数にはその関数に渡したい引数を行列かデータフレームで与え、第2引数には繰り返したい関数を指定します。\n\nmdply(cbind(x = c(1, 1, 2, 2, 3), y = c(1, 3, 2, 4, 5)), function(x, y){x*y})\n\n\n  \n\n\nmaply(expand.grid(x = 1:5, y = 1:5), function(x, y){x*y})\n\n   y\nx   1  2  3  4  5\n  1 1  2  3  4  5\n  2 2  4  6  8 10\n  3 3  6  9 12 15\n  4 4  8 12 16 20\n  5 5 10 15 20 25\n\n\n\n\n\n補助関数\n本パッケージには **ply 系関数と組み合わせて使用する補助関数が多数提供されています。\nそのうち代表的なものを紹介します。\n\nmutate, summarise（データフレームの列加工）\nmutate関数はデータフレームの列加工に用いられる関数です。\n\ndf &lt;- data.frame(x = c(1, 2, 3, 4),\n                 y = c(2, 3, 5, 7))\n#x + yを計算して新しい列zに格納\nmutate(df, z = x + y)\n\n\n  \n\n\n#列xを2倍\nmutate(df, x = 2 * x)\n\n\n  \n\n\n\nR標準のtransform関数と働きは同様ですが、 複数回の加工を一度に行う際、それまでの加工結果を引き継ぐという点が特徴です。\n\n#xにyを足しこんでから2倍する\nmutate(df, x = x + y, w = 2 * x)\n\n\n  \n\n\n#2*xの計算に使うxはもともとのx\ntransform(df, x = x + y, w = 2 * x)\n\n\n  \n\n\n#mutate中新たに作られる列zの結果を後続で使用可能\nmutate(df, z = x + y, w = 2 * z)\n\n\n  \n\n\n\n一方、summarise関数はmutate関数と同様の計算を行うものの、 もともとあった列が取り除かれることが特徴です。\n\n#xにyを足しこんでから2倍する\nsummarise(df, x = x + y, w = 2 * x)\n\n\n  \n\n\n\nこの性質は、ddply関数の第3引数に用いるときに便利です。\n（mutate関数を用いた場合はもともとある列が全て吐き出されてしまう）\n\nddply(iris, .(Species), summarise,\n      mean_Sepal.Length = mean(Sepal.Length))\n\n\n  \n\n\n\n\n\ncolwise（すべての列を処理する関数に変換）\ncolwise関数は同じ関数を全ての列に対して実行したい場合に用いられます。\n\nddply(iris, .(Species), colwise(mean))\n\n\n  \n\n\n\n\n\neach（同時に複数個の関数を実行する単一の関数を生成）\neach関数は同時に複数の関数を実行したい場合に用いられます。 例えば次のような例では、平均と分散の両方を同時に計算することができます。\n\naaply(ar_ozone[1:5, 1:5, ], 1:2, each(mean, var))\n\n, ,  = mean\n\n       long\nlat       -113.8   -111.3   -108.8   -106.3   -103.8\n  -21.2 268.2500 268.9444 269.1389 269.4722 269.7500\n  -18.7 265.7500 265.7500 265.8611 266.2500 265.9444\n  -16.2 262.7778 263.0278 262.9444 263.3889 263.3611\n  -13.7 260.4722 260.8333 260.8611 260.8056 260.7778\n  -11.2 258.6667 259.0000 258.8611 258.8611 259.0556\n\n, ,  = var\n\n       long\nlat        -113.8    -111.3    -108.8    -106.3    -103.8\n  -21.2 150.30282 156.39124 157.72692 162.02739 155.82394\n  -18.7 115.82394 115.82394 111.02269 115.37324 119.32081\n  -16.2  85.47105  82.81612  84.05321  84.24100  84.82551\n  -13.7  68.11189  62.50704  66.68466  69.48279  70.48513\n  -11.2  62.76056  61.97183  66.12128  63.98044  68.05321\n\n\n\n\nsplat（単一リストを引数にとる関数に変換）\nsplat関数は、複数引数をとる関数を単一リストを引数にとる関数に変換します。\nd*ply系関数で複数列を処理する場合に用いることができます。\n\nfn_mean_Sepal.Rate &lt;- function(Sepal.Length, Sepal.Width, ...){\n  mean(Sepal.Length / Sepal.Width)\n}\n\nddply(iris, .(Species), splat(fn_mean_Sepal.Rate))\n\n\n  \n\n\n\n\n\nfailwith（エラー処理）\nfailwith関数も関数を変換するものの一種で、 実行時にエラーとなってしまう場合にNA等の値を当てはめるようにします。\n第1引数にエラー時の値、第2引数に実行する関数を指定します。\n\nllply(-2:3, failwith(NA, \n                     function(n) seq(1, n, 1)))\n\nError in seq.default(1, n, 1) : wrong sign in 'by' argument\nError in seq.default(1, n, 1) : wrong sign in 'by' argument\nError in seq.default(1, n, 1) : wrong sign in 'by' argument\n\n\n[[1]]\n[1] NA\n\n[[2]]\n[1] NA\n\n[[3]]\n[1] NA\n\n[[4]]\n[1] 1\n\n[[5]]\n[1] 1 2\n\n[[6]]\n[1] 1 2 3\n\n#エラーメッセージを表示しない\nllply(-2:3, failwith(NA, \n                     function(n) seq(1, n, 1),\n                     quiet = T))\n\n[[1]]\n[1] NA\n\n[[2]]\n[1] NA\n\n[[3]]\n[1] NA\n\n[[4]]\n[1] 1\n\n[[5]]\n[1] 1 2\n\n[[6]]\n[1] 1 2 3\n\n\n\n\nas.data.frame.function\nas.data.frame.function関数も関数を変換するものの一種で、 関数の出力をデータフレームに変更します。\n\n#as.data.frame関数の引数に関数を指定すると、\n#as.data.frame.function関数が使用される\nldply(1:3, as.data.frame(function(n) 1:n))",
    "crumbs": [
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>plyr</span>"
    ]
  },
  {
    "objectID": "articles/plyr.html#plyrパッケージの長所短所",
    "href": "articles/plyr.html#plyrパッケージの長所短所",
    "title": "plyr",
    "section": "plyrパッケージの長所・短所",
    "text": "plyrパッケージの長所・短所\n\n長所\nplyrパッケージはグループ化集計を念頭に置きつつ、 R標準のapply系関数を拡張・改良する方向性でデザインされているため、 R標準機能になじみ深いユーザーにとっては受け入れやすいのが長所といえます。\n特に、多次元配列をapply系関数のように扱いたい場合に威力を発揮します。\nパッケージ開発者の論文(Wickham, Hadley 2011)で挙げられているozone（オゾン濃度データ）の例がわかりやすいため、 ここで紹介します。\n本データは12か月×6年間＝72か月分のデータがありますが、その濃度の増減には周期性がみられます。 ある1地点での時間変化をプロットすると次のとおり。\n\na_oz &lt;- ar_ozone[1, 1, ]\nplot(a_oz)\n\n\n\n\n\n\n\n\n1年周期の季節的な変動と考えられるので、 このデータに対して月を説明変数としたロバスト線形回帰モデルを構築し、 そのモデルの予測値を差し引くことで季節的な要因を取り除くことを考えます。\n\nmonth &lt;- ordered(rep(1:12, length = 72))\nmodel_rlm &lt;- MASS::rlm(a_oz ~ month - 1, maxit = 50)\nmodel_rlm\n\nCall:\nrlm(formula = a_oz ~ month - 1, maxit = 50)\nConverged in 9 iterations\n\nCoefficients:\n  month1   month2   month3   month4   month5   month6   month7   month8 \n264.3964 259.2036 255.0000 252.0052 258.5089 265.3387 274.0000 276.6724 \n  month9  month10  month11  month12 \n277.0000 285.0000 283.6036 273.1964 \n\nDegrees of freedom: 72 total; 60 residual\nScale estimate: 4.45 \n\n\n\n#実データと予測値を重ねたプロット\nplot(a_oz)\npoints(predict(model_rlm, month), pch = 3)\nlegend(\"bottomright\", NULL, c(\"raw\", \"rlm\"), pch = c(1,3))\n\n\n\n\n\n\n\n#残差のプロット\nplot(resid(model_rlm), pch = 4)\nlegend(\"bottomright\", NULL, c(\"residue = raw - rlm\"), pch = 4)\n\n\n\n\n\n\n\n\nただし、これはあくまで1地点における分析です。 このような季節要因を取り除くという計算を他の地点でも同様に行いたいとしましょう。\n最もわかりやすいのはfor文によるものです7。\n\n#季節要因を除いたデータをar_deseasに格納したい\nar_deseas &lt;- array(NA, c(24, 24, 72))\ndimnames(ar_deseas) &lt;- dimnames(ar_ozone)\n#季節要因を除くのに使用したモデルを保存しておきたい\nmodels &lt;- as.list(rep(NA, 24*24))\ndim(models) &lt;- c(24, 24)\n\nfor(i in 1:24){\n  for(j in 1:24){\n    a_oz &lt;- ar_ozone[i, j, ]\n    model &lt;- MASS::rlm(a_oz ~ month - 1, maxit = 50)\n    \n    models[[i, j]] &lt;- model\n    ar_deseas[i, j, ] &lt;- resid(model)\n  }\n}\n\nR言語ではfor文よりもapply系関数によるコーディングが好まれますが、 この例の場合は配列の次元の取り扱いに苦慮します。\n\nfn_fitmodel &lt;- function(a_oz){\n  MASS::rlm(a_oz ~ month - 1, maxit = 50)\n}\nmodels &lt;- apply(ar_ozone, 1:2, fn_fitmodel)\nls_deseas &lt;- lapply(models, resid)\nar_deseas &lt;- unlist(ls_deseas)\nstr(ar_deseas) #1次元の配列\n\n Named num [1:41472] -4.4 -5.2 -1 -8.01 -8.51 ...\n - attr(*, \"names\")= chr [1:41472] \"1\" \"2\" \"3\" \"4\" ...\n\n#3次元の配列に変更\ndim(ar_deseas) &lt;- c(72, 24, 24)\n#添え字が時間→場所の順になっているので、元に戻すために入れ替える\nar_deseas &lt;- aperm(ar_deseas, c(2, 3, 1))\ndimnames(ar_deseas) &lt;- dimnames(ozone)\n\nplyrパッケージの**ply系関数を使用すれば、 apply系関数によるコーディングの良さを残しつつも、コードをよりシンプルにすることが可能です。\n\nfn_fitmodel &lt;- function(a_oz){\n  MASS::rlm(a_oz ~ month - 1, maxit = 50)\n}\n#モデルは配列には代入できないため、aaplyではなくalplyとする\nmodels &lt;- alply(ar_ozone, 1:2, fn_fitmodel)\nar_deseas &lt;- laply(models, resid)\n\nなお参考までに実行結果を可視化すると以下のとおりで、論文の図を再現できていると考えられます。\n\n#季節要因を除く前の各地点での平均値\nar_mean &lt;- aaply(ar_ozone, 1:2, mean)\nar_mean[24:20, 1:5] #左上部分のみ数値を表示\n\n      long\nlat      -113.8   -111.3   -108.8   -106.3   -103.8\n  36.2 310.7778 313.9722 313.9722 306.1111 300.1389\n  33.7 307.0833 307.0833 298.8889 297.7222 299.6944\n  31.2 300.3056 300.3056 296.6944 298.2500 294.1667\n  28.7 290.3056 287.6944 287.6944 276.3611 288.5278\n  26.2 282.4167 281.1667 278.7778 278.7778 279.1389\n\n#ヒートマップ(高いところほど濃い色)\nheatmap(ar_mean, scale = \"none\",\n        Rowv = NA, Colv = NA)\n\n\n\n\n\n\n\n#季節要因を除いた後の各地点での標準偏差\nar_sd &lt;- aaply(ar_deseas, 1:2, sd)\nar_sd[24:20, 1:5] #左上部分のみ数値を表示\n\n      long\nlat       -113.8    -111.3    -108.8    -106.3    -103.8\n  36.2 15.005658 12.812271 12.812271 12.150841 10.137015\n  33.7 12.236271 12.236271  8.823250  9.270989 10.212678\n  31.2 10.311851 10.311851 10.156382  9.056539  9.032406\n  28.7  8.876504  8.854916  8.854916  9.176301  8.386308\n  26.2  7.285146  7.470320  7.503187  7.503187  8.058870\n\n#ヒートマップ(高いところほど濃い色)\nheatmap(ar_sd, scale = \"none\",\n        Rowv = NA, Colv = NA)\n\n\n\n\n\n\n\n\n\n\n短所\n一方、短所は後継となるdplyrパッケージやこれを含むtidyverse環境とは併存しづらいということです。\n基本的に、tidyverse環境を前提とするのであれば データフレームの操作はdplyrパッケージの方が扱いやすいケースが多く、 plyrパッケージが必要となる場面は限定的でしょう。\n\n#Sepal.LengthとSepal.Widthの比の平均値を種類ごとに計算\n#plyrパッケージの場合\ndf_iris &lt;- iris\ndf_iris_tmp &lt;- plyr::mutate(df_iris,\n                        Sepal.Rate = Sepal.Length / Sepal.Width)\nddply(df_iris_tmp, .(Species),\n      plyr::summarise,\n      mean_Sepal.Rate = mean(Sepal.Rate))\n\n\n  \n\n\n#dplyrパッケージの場合\nlibrary(dplyr)\ndf_iris &lt;- iris\ndf_iris %&gt;%\n  dplyr::mutate(Sepal.Rate = Sepal.Length / Sepal.Width) %&gt;%\n  dplyr::group_by(Species) %&gt;%\n  dplyr::summarise(mean_Sepal.Rate = mean(Sepal.Rate))\n\n\n  \n\n\n\nそれでも、配列に対する操作等のplyrパッケージ特有の機能を理由に、 dplyrパッケージとplyrパッケージを敢えて併存させることは考えられます。\nこの場合に問題となるのは、両者で関数名の競合が多くみられ、 どちらのパッケージの関数を用いているのかに気を配る必要がある点です。\n例えば以下のようにplyrパッケージをdplyrパッケージよりも後に読み込んだうえで、 関数のパッケージ名を明示せずに使用した場合、 意図しない動作を引き起こしてしまいます。\n\n#読み込んでいるパッケージを環境から取り外す（読み込む前の状態に戻る）\ndetach(\"package:dplyr\") \ndetach(\"package:plyr\")\n\n#dplyr→plyrの順で読み込んでしまうと、dplyrの関数群がplyrのものでマスクされる\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(plyr)\n\n------------------------------------------------------------------------------\n\n\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n\n\n------------------------------------------------------------------------------\n\n\n\nAttaching package: 'plyr'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n#以下のように書くとplyrパッケージのsummarise関数が実行されるため、\n#グループごとの集計に失敗\ndf_iris &lt;- iris\ndf_iris %&gt;%\n  mutate(Sepal.Rate = Sepal.Length / Sepal.Width) %&gt;%\n  group_by(Species) %&gt;%\n  summarise(mean_Sepal.Rate = mean(Sepal.Rate))\n\n\n  \n\n\n\nこのような事態を防ぐには、plyrパッケージを読み込んでからdplyrパッケージを読み込むか、 plyr::***のように逐次パッケージ名を明記するかのいずれかを選択することになります。 前者の方法をとる場合でも、dplyrパッケージ自体が他のパッケージの前提となっている（知らぬ間に読み込まれてしまっている）こともあり、十二分に注意が必要です。\nこのような煩雑さを避けるため、前節で挙げたようなケースでも敢えてR標準機能でしのぐことは考えられます。",
    "crumbs": [
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>plyr</span>"
    ]
  },
  {
    "objectID": "articles/plyr.html#参考文献",
    "href": "articles/plyr.html#参考文献",
    "title": "plyr",
    "section": "参考文献",
    "text": "参考文献\n\n\nWickham, Hadley. 2011. “The Split-Apply-Combine Strategy for Data Analysis.” https://doi.org/10.18637/jss.v040.i01.",
    "crumbs": [
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>plyr</span>"
    ]
  },
  {
    "objectID": "articles/plyr.html#footnotes",
    "href": "articles/plyr.html#footnotes",
    "title": "plyr",
    "section": "",
    "text": "このアスタリスク「*」は、その場所に何らかの文字が入ったものの総称として使用しています。たとえば l*ply は laply、ldply、llply、l_ply の総称です。↩︎\nなお、dplyrパッケージの名前は、データフレーム操作に特化したplyrパッケージの後継というところから取られています。↩︎\nこの命名規則に従う関数は他にも提供されていますが、これら12種とは若干趣が異なるもののため本表では省略しています。↩︎\nこれらは Split-Apply-Combine の Split 部分の考え方が異なるもののため、別扱いとしました。↩︎\nこのrは Replicate の頭文字を取ったものと考えられます。↩︎\nこのmはR標準の mapply 関数と同様、関数の繰り返しの際に複数引数を取り扱える(Multivariate)ことを意味していると考えられます。↩︎\n以下、これらのコードでは'rlm' failed to converge in 50 stepsという警告が複数回表示されますが、本稿では記載を省略しています。↩︎",
    "crumbs": [
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>plyr</span>"
    ]
  },
  {
    "objectID": "articles/pROC.html",
    "href": "articles/pROC.html",
    "title": "pROC",
    "section": "",
    "text": "パッケージの概要\npROCパッケージは、ROC曲線（Receiver Operating Characteristic Curve）の描画や分析に特化して作られたパッケージです。ROC曲線は、二値変数である目的変数（アウトカム変数）と、連続変数である説明変数の関係の強さを評価する手法であり、医療分野における診断マーカーや、より一般には二値分類モデルの精度指標として用いられています。",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>pROC</span>"
    ]
  },
  {
    "objectID": "articles/pROC.html#roc曲線を作成する",
    "href": "articles/pROC.html#roc曲線を作成する",
    "title": "pROC",
    "section": "ROC曲線を作成する",
    "text": "ROC曲線を作成する\nデータフレームにroc()関数を適用することで、rocオブジェクト（または、rocオブジェクトを要素とするリスト）を作成することができます。rocオブジェクトに対してplot()関数やggroc()関数を適用することで、ROC曲線を描画することができます。\n\nlibrary(ggplot2)\nlibrary(pROC)\n\nresponse &lt;- c(0, 0, 0, 1, 0, 1, 1, 1)\nprobability &lt;- 1:8 /10\n(sample.roc &lt;- roc(response, predictor = probability))\n\n\nCall:\nroc.default(response = response, predictor = probability)\n\nData: probability in 4 controls (response 0) &lt; 4 cases (response 1).\nArea under the curve: 0.9375\n\nggroc(sample.roc)\n\n\n\n\n\n\n\n\nなお、ROC曲線は、連続変数を分類に用いるときの「しきい値」を変化させたときの、感度と特異度の変化を平面上にプロットしたものです。\nたとえば、予測確率（10%~80%）の値がしきい値 t 以上であるときに response の値を 1 と予測することにすると、t = 10% （すべてを 1 と予測）のとき感度 = 1、特異度 = 0 となり、これはグラフの右上の点に対応しています。t を 20%、30%、…と変化させていったときの点を明示すれば、下のようになります。\n\ndf &lt;- data.frame(sensitivity = 1 - cumsum(c(0, response)) / 4,\n                 specificity = cumsum(c(0, 1 - response)) / 4,\n                 label = paste0(\"t=\", 1:9 * 10, \"%\"))\n\n# ggplot2 ベースでROCカーブを描画する\nggroc(sample.roc) +\n  geom_label(aes(specificity, sensitivity, label = label), df)",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>pROC</span>"
    ]
  },
  {
    "objectID": "articles/pROC.html#aucを計算する",
    "href": "articles/pROC.html#aucを計算する",
    "title": "pROC",
    "section": "AUCを計算する",
    "text": "AUCを計算する\nROC曲線のグラフにおいて、グラフの下の部分の面積をAUC（Area Under the Curve）と呼びます。AUCは、しばしば分類モデルの性能を評価する指標として用いられます。\n\ncat(auc(sample.roc))\n\n0.9375\n\n# 出力される数値は\"auc\"クラスのオブジェクトでもあり、表示方法が設定されている。\nauc(sample.roc)\n\nArea under the curve: 0.9375",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>pROC</span>"
    ]
  },
  {
    "objectID": "articles/pROC.html#asahデータセット",
    "href": "articles/pROC.html#asahデータセット",
    "title": "pROC",
    "section": "aSAHデータセット",
    "text": "aSAHデータセット\npROCパッケージに収録されているaSAHデータセットは、動脈瘤性くも膜下出血患者113名について、以下の情報を記録したものです。\n\n\n\n変数名\n概要\n\n\n\n\ngos6\n入院6か月後におけるGlasgow Outcome Scaleの値\n\n\noutcome\n予後（gos6が3以下かどうかで分類）\n\n\ngender\n性別\n\n\nage\n年齢\n\n\nwfns\n入院時のWFNS分類（くも膜下出血の重症度分類の一種）\n\n\ns100b\n入院後12時間以内に採取された静脈血中のS100β濃度\n\n\nndka\n静脈血中のNDKA（ヌクレオシド二リン酸キナーゼA）の濃度\n\n\n\n\n\n\ndata(aSAH, package = \"pROC\")\nDT::datatable(aSAH)\n\n\n\n\n\n合計113件のデータのうち、41件が予後良好（Good）、72件が予後不良（Poor）です。これを、データの他の指標から判定ないし予測できるかどうかを、ROC曲線で表現します。\n\nROCs &lt;- roc(outcome ~ age + s100b + ndka, data = aSAH)\n\n# plot()関数による描画\nplot(ROCs$s100b)\n\n\n\n\n\n\n\n# ggroc()関数によるggplot2ベースの描画\nggroc(ROCs, legacy.axes = FALSE)\n\n\n\n\n\n\n\n\n各 ROC 曲線について AUC（ROC-AUC）の値は以下の通りとなり、くも膜下出血の予後の指標としては s100b の精度が他の指標よりも良いことが読み取れます。\n\n# auc()関数をrocオブジェクトにまとめて適用する\nsapply(ROCs, auc)\n\n      age     s100b      ndka \n0.6150068 0.7313686 0.6119580",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>pROC</span>"
    ]
  },
  {
    "objectID": "articles/psych.html",
    "href": "articles/psych.html",
    "title": "psych",
    "section": "",
    "text": "packageのインストール\npsychパッケージをインストールする。\n# install.packages(\"psych\")\nlibrary(psych)",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>psych</span>"
    ]
  },
  {
    "objectID": "articles/psych.html#pairs.panels関数散布図相関ヒストグラム",
    "href": "articles/psych.html#pairs.panels関数散布図相関ヒストグラム",
    "title": "psych",
    "section": "pairs.panels関数（散布図・相関・ヒストグラム）",
    "text": "pairs.panels関数（散布図・相関・ヒストグラム）\npsych関数の代表的な関数pairs.panels()を使用すると、散布図・相関行・ヒストグラムを表示することができる。以下では、Rに標準で用意されているirisのデータセットを用いる。psychパッケージのdescribe関数でデータの統計量などを確認できる\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\ndescribe(iris)\n\n             vars   n mean   sd median trimmed  mad min max range  skew\nSepal.Length    1 150 5.84 0.83   5.80    5.81 1.04 4.3 7.9   3.6  0.31\nSepal.Width     2 150 3.06 0.44   3.00    3.04 0.44 2.0 4.4   2.4  0.31\nPetal.Length    3 150 3.76 1.77   4.35    3.76 1.85 1.0 6.9   5.9 -0.27\nPetal.Width     4 150 1.20 0.76   1.30    1.18 1.04 0.1 2.5   2.4 -0.10\nSpecies*        5 150 2.00 0.82   2.00    2.00 1.48 1.0 3.0   2.0  0.00\n             kurtosis   se\nSepal.Length    -0.61 0.07\nSepal.Width      0.14 0.04\nPetal.Length    -1.42 0.14\nPetal.Width     -1.36 0.06\nSpecies*        -1.52 0.07\n\npairs.panels(iris)\n\n\n\n\n\n\n\n\nSpeciesの値によって、散布図を色分けすることも可能\n\npairs.panels(iris[1:4],bg=c(\"red\",\"yellow\",\"blue\")[iris$Species],\n        pch=21,main=\"Fisher Iris data by Species\")\n\n\n\n\n\n\n\npairs.panels(iris[1:4],bg=c(\"red\",\"yellow\",\"blue\")[iris$Species],\n  pch=21+as.numeric(iris$Species),main=\"Fisher Iris data by Species\",hist.col=\"red\") \n\n\n\n\n\n\n\n\nshow.pointsの引数をFalseとすると散布図を表示しない。\n\npairs.panels(iris,show.points=FALSE)",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>psych</span>"
    ]
  },
  {
    "objectID": "articles/purrr.html",
    "href": "articles/purrr.html",
    "title": "purrr",
    "section": "",
    "text": "パッケージの概要\npurrrは、ベクトルやリストに対して関数を適用し、データの加工や操作を効果的に行うためのユーティリティ関数や操作を提供します。\n\nsuppressMessages(require(tidyverse))\nrequire(purrr)\n\n\n\nmap関数\nmap関数はリストの各要素に対して、関数を適用した結果をリストに格納します 以下の例では、リストの各要素であるベクトルに対して、それぞれのベクトルの平均値を計算（meanを適用）します\n\nx &lt;- list(1:2,1:3,1:4)\nmap(x,mean)\n\n[[1]]\n[1] 1.5\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 2.5\n\n\nmap関数はベクトルに対しても適用することが可能です。 その場合、関数の各要素に対して、単独で適用され、結果はリストに格納されます。 mean関数に適用した場合は、単純に各ベクトルの要素がリストの各要素に格納されるだけとなりますが、例えば、rnorm関数に適用すれば、ベクトルの要素の数だけ、rnorm関数を適用した結果をリストに格納することができます。\n\ny &lt;- 1:3\nmap(y, mean)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\nmap(y, rnorm, n=5)\n\n[[1]]\n[1]  1.9824686  0.2751347 -0.1263742  0.6456573  0.5864637\n\n[[2]]\n[1]  3.11605499 -0.53956619  0.74846584 -0.01126552  2.02890768\n\n[[3]]\n[1] 2.479666 1.852039 3.013718 2.581137 3.488995\n\n\n\n\nmap_dbl関数\nmap_dbl関数は、結果をリストではなく、数値ベクトルに格納します。 類似の関数として、map_int関数や、map_chr関数なども存在します。\n\nz &lt;- 1:5\nmap_dbl(z, mean)\n\n[1] 1 2 3 4 5\n\nzclass &lt;- map_dbl(z, mean)\ntypeof(zclass)\n\n[1] \"double\"\n\n\nmap_dbl関数を利用する場合、関数を適用した結果は、それぞれの結果は1つの数値である必要があります。 例えば、map関数をrnorm関数に適用し、nを5とした場合、各リストに格納される結果は要素数が5のベクトルになるため、map_dbl関数ではエラーになります。\n\nz &lt;- 1:3\nmap(z, rnorm, n=5)\n\n[[1]]\n[1] 1.451150 2.517198 1.351899 1.031781 1.654450\n\n[[2]]\n[1] 3.3132701 1.1952853 0.8001845 2.2724800 1.5609079\n\n[[3]]\n[1] 1.600831 3.792787 4.010662 2.413056 3.116548\n\nmap_dbl(z, rnorm, n=5)\n\nError in `map_dbl()`:\nℹ In index: 1.\nCaused by error:\n! Result must be length 1, not 5.\n\n\nなお、map関数とmap_dbl関数を組み合わせることで、rnorm関数で生成した結果の平均値を1つの数値ベクトルに格納することができます\n\nz |&gt;\n  map(rnorm, n=5) |&gt;\n  map_dbl(mean)\n\n[1] 0.9105756 2.4009039 3.2977725\n\n\n\n\n線形回帰モデルを実装する活用例\nRのデフォルトのデータセットであるmtcars（自動車の燃費などに関するデータセット）を用いた例について記載します。 車の燃費（mpg）と車の重量（wt）の関係について異なるシリンダー数（cyl）ごとに線形回帰モデルを適用し、各モデルの決定係数（R-squared）を取得する処理は、map関数、map_dbl関数を用いれば、以下のコードで実現可能です。\n\nmtcars |&gt;\n  split(mtcars$cyl) |&gt;\n  map(function(df) lm(mpg ~ wt, data = df)) |&gt;\n  map(summary) |&gt;\n  map_dbl(\"r.squared\")\n\n        4         6         8 \n0.5086326 0.4645102 0.4229655 \n\n\n複数のパイプ処理が連なっているため、各コードの実装結果を順番に確認します。 まず、以下のコードでは、mtcarsをシリンダー数（cyl）別に分解して、それぞれの結果をリストに格納しています。\n\nmtcars |&gt;\n  split(mtcars$cyl)\n\n$`4`\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n$`6`\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nValiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n\n$`8`\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n\n\n次に、以下のコードではmap関数を用いて、シリンダー数（cyl）別に分解したそれぞれのデータセットに対して、線形回帰モデルを作成し、結果をリストに格納しています。\n\nmtcars |&gt;\n  split(mtcars$cyl)|&gt;\n  map(function(df) lm(mpg ~ wt, data = df)) \n\n$`4`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     39.571       -5.647  \n\n\n$`6`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n      28.41        -2.78  \n\n\n$`8`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     23.868       -2.192  \n\n\n以下のコードでは、シリンダー数（cyl）別に作成した線形回帰モデルのサマリーをリストに格納しています。\n\nmtcars |&gt;\n  split(mtcars$cyl)|&gt;\n  map(function(df) lm(mpg ~ wt, data = df))  |&gt;\n  map(summary)\n\n$`4`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1513 -1.9795 -0.6272  1.9299  5.2523 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   39.571      4.347   9.104 7.77e-06 ***\nwt            -5.647      1.850  -3.052   0.0137 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.332 on 9 degrees of freedom\nMultiple R-squared:  0.5086,    Adjusted R-squared:  0.454 \nF-statistic: 9.316 on 1 and 9 DF,  p-value: 0.01374\n\n\n$`6`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nResiduals:\n     Mazda RX4  Mazda RX4 Wag Hornet 4 Drive        Valiant       Merc 280 \n       -0.1250         0.5840         1.9292        -0.6897         0.3547 \n     Merc 280C   Ferrari Dino \n       -1.0453        -1.0080 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   28.409      4.184   6.789  0.00105 **\nwt            -2.780      1.335  -2.083  0.09176 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.165 on 5 degrees of freedom\nMultiple R-squared:  0.4645,    Adjusted R-squared:  0.3574 \nF-statistic: 4.337 on 1 and 5 DF,  p-value: 0.09176\n\n\n$`8`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1491 -1.4664 -0.8458  1.5711  3.7619 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  23.8680     3.0055   7.942 4.05e-06 ***\nwt           -2.1924     0.7392  -2.966   0.0118 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.024 on 12 degrees of freedom\nMultiple R-squared:  0.423, Adjusted R-squared:  0.3749 \nF-statistic: 8.796 on 1 and 12 DF,  p-value: 0.01179\n\n\n最後に、以下のコードでシリンダー数（cyl）別に作成した線形回帰モデルのサマリーのうち、決定係数のみを取得し、結果を数値ベクトルに格納しています。\n\nmtcars |&gt;\n  split(mtcars$cyl) |&gt;\n  map(function(df) lm(mpg ~ wt, data = df)) |&gt;\n  map(summary) |&gt;\n  map_dbl(\"r.squared\")\n\n        4         6         8 \n0.5086326 0.4645102 0.4229655",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>purrr</span>"
    ]
  },
  {
    "objectID": "articles/randomForest.html",
    "href": "articles/randomForest.html",
    "title": "randomForest",
    "section": "",
    "text": "パッケージの概要\n機械学習におけるRandomForestモデルを構築できます。RandomForestモデルとは、多数の決定木を集めてそれらの予測値よりモデル全体の予測値を算出する、アンサンブルモデルの一種です。",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>randomForest</span>"
    ]
  },
  {
    "objectID": "articles/randomForest.html#参考url",
    "href": "articles/randomForest.html#参考url",
    "title": "randomForest",
    "section": "参考URL",
    "text": "参考URL\nhttps://momonoki2017.blogspot.com/2018/04/r007-riris.html",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>randomForest</span>"
    ]
  },
  {
    "objectID": "articles/randomForest.html#使用例irisデータの分類",
    "href": "articles/randomForest.html#使用例irisデータの分類",
    "title": "randomForest",
    "section": "使用例：irisデータの分類",
    "text": "使用例：irisデータの分類\nirisデータを用いて、がく弁・花弁の長さ・幅の情報からアヤメの種類を特定するRandomForestモデルをrandomForestパッケージを用いて構築します。\n\nirisデータセットを読み込む\nirisデータを読み込み、データの先頭を表示します。\n\nSepal.Length：がく弁の長さ\nSepal.Width：がく弁の幅\nPetal.Length：花弁の長さ\nPetal.Width：花弁の幅\n\nアヤメの種類はsetosa(1)、versicolor(2)、virginica(3)の3種類です。\n\ndata(iris)\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\n\nirisデータの構造\nirisデータの各種構造を確認します。\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nまた、データを散布図にプロットして確認します。\n\nplot(iris, col=c(2, 3, 4)[iris$Species])\n\n\n\n\n\n\n\n\n\n\nモデル構築1（全体データ）\nまずは全てのデータを使ってRandomForestモデルを構築してみます。\n\n(iris.rf &lt;- randomForest(Species ~ ., data = iris))\n\n\nCall:\n randomForest(formula = Species ~ ., data = iris) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 4%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         50          0         0        0.00\nversicolor      0         47         3        0.06\nvirginica       0          3        47        0.06\n\n\n\n重要度の確認\nデータの各特徴量の重要度を確認することが出来ます。irisデータの分類には花弁の長さ・花弁の幅の情報が重要であることが分かります。\n\nimportance(iris.rf)\n\n             MeanDecreaseGini\nSepal.Length         9.937619\nSepal.Width          2.440018\nPetal.Length        40.254780\nPetal.Width         46.644024\n\n\n\n\n\nモデル構築2（訓練データとテストデータに分割）\nirisデータをモデル生成のための訓練データと、モデル評価のためのテストデータに分割します。データ割合は訓練データを7割、テストデータを3割とします。確認のため、データサイズを出力します。\n\n# 再現性のためにシードを設定\nset.seed(123)  \n\n# データの分割\nsample_indices &lt;- sample(1:nrow(iris), 0.7 * nrow(iris))  \ndf.train &lt;- iris[sample_indices, ]\ndf.test &lt;- iris[-sample_indices, ]\n\n# データサイズの確認\nc(nrow(iris), nrow(df.train), nrow(df.test))\n\n[1] 150 105  45\n\n\n\nモデル生成\n訓練データを用いてRandomFOrestモデルを生成します。\n\n(model.rf &lt;- randomForest(Species ~ ., data = df.train))\n\n\nCall:\n randomForest(formula = Species ~ ., data = df.train) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 5.71%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         36          0         0  0.00000000\nversicolor      0         29         3  0.09375000\nvirginica       0          3        34  0.08108108\n\n\n\n\nモデル評価\nテストデータを使ってモデル評価を行います。まずはテストデータを元に生成したモデルを用いて予測結果を算出します。\n\nprediction &lt;- predict(model.rf, df.test)\n\n予測結果とテストデータのもともとのアヤメの分類とを比較します。おおむね正しく分類できていることが分かります。\n\n(result &lt;- table(prediction, df.test$Species))\n\n            \nprediction   setosa versicolor virginica\n  setosa         14          0         0\n  versicolor      0         17         0\n  virginica       0          1        13\n\n(accuracy_prediction &lt;- sum(diag(result)) / sum(result))\n\n[1] 0.9777778",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>randomForest</span>"
    ]
  },
  {
    "objectID": "articles/ranger.html",
    "href": "articles/ranger.html",
    "title": "ranger",
    "section": "",
    "text": "パッケージの概要\n機械学習におけるRandomForestモデルを構築できます。高速実装であり、特に高次元データに適しています。 分類木、回帰木、生存木、確率予測木のアンサンブルをサポートしています。",
    "crumbs": [
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>ranger</span>"
    ]
  },
  {
    "objectID": "articles/ranger.html#使用例irisデータの分類",
    "href": "articles/ranger.html#使用例irisデータの分類",
    "title": "ranger",
    "section": "使用例：irisデータの分類",
    "text": "使用例：irisデータの分類\nirisデータを用いて、がく弁・花弁の長さ・幅の情報からアヤメの種類を特定するRandomForestモデルをrangerパッケージを用いて構築します。\n\nirisデータセットを読み込む\nirisデータを読み込み、データの先頭を表示します。\n\nSepal.Length：がく弁の長さ\nSepal.Width：がく弁の幅\nPetal.Length：花弁の長さ\nPetal.Width：花弁の幅\n\nアヤメの種類はsetosa(1)、versicolor(2)、virginica(3)の3種類です。\n\ndata(iris)\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\n\nirisデータの構造\nirisデータの各種構造を確認します。\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nまた、データを散布図にプロットして確認します。\n\nplot(iris, col=c(2, 3, 4)[iris$Species])",
    "crumbs": [
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>ranger</span>"
    ]
  },
  {
    "objectID": "articles/ranger.html#モデル構築１全体データ",
    "href": "articles/ranger.html#モデル構築１全体データ",
    "title": "ranger",
    "section": "モデル構築１（全体データ）",
    "text": "モデル構築１（全体データ）\nまずは全てのデータを使ってRandomForestモデルを構築してみます。\n\nlibrary(ranger)\n\n# シードを設定\nset.seed(123)\n(model.all &lt;- ranger(Species ~ ., data = iris, importance = \"impurity\"))\n\nRanger result\n\nCall:\n ranger(Species ~ ., data = iris, importance = \"impurity\") \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      150 \nNumber of independent variables:  4 \nMtry:                             2 \nTarget node size:                 1 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error:             5.33 % \n\n\n分類木の構築においては、importance = “impurity”と設定することにより、結果にvariable.importanceを保持してくれます。この中身を確認することにより各変数の重要度を確認することが出来ます。irisデータの分類には花弁の長さ（Petal.Length）・花弁の幅（Petal.Width）の情報が重要であることが分かります。\n\nmodel.all$variable.importance\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    9.629622     2.453766    41.714390    45.460729 \n\n\n\nデータセットの準備\nirisデータをモデル生成のための訓練データと、モデル評価のためのテストデータに分割します。データ割合は訓練データを7割、テストデータを3割とします。確認のため、データサイズを出力します。\n\n# 再現性のためにシードを設定\nset.seed(123)  \n\n# データの分割\nsample_indices &lt;- sample(1:nrow(iris), 0.7 * nrow(iris))  \ntrain_data &lt;- iris[sample_indices, ]\ntest_data &lt;- iris[-sample_indices, ]\n\n# データサイズの確認\nc(nrow(iris), nrow(train_data), nrow(test_data))\n\n[1] 150 105  45\n\n\n\n\nモデルの生成・予測の実行\n訓練データを用いて分類木のモデルを生成します。モデルの生成結果は以下の通りです。\n\nset.seed(123)  \n(model &lt;- ranger(Species ~ ., data = train_data))\n\nRanger result\n\nCall:\n ranger(Species ~ ., data = train_data) \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      105 \nNumber of independent variables:  4 \nMtry:                             2 \nTarget node size:                 1 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error:             5.71 % \n\n\nテストデータを用いてモデルの評価をします。まずは、テストデータを先ほど構築した分類木モデルに適用させ、その予測結果をpredictionsに格納します。\n\npredictions &lt;- predict(model, data = test_data)$predictions\n\n予測結果とテストデータのもともとのアヤメの分類とを比較します。おおむね正しく分類できていることが分かります。\n\n(confusion_matrix &lt;- table(predictions, test_data$Species))\n\n            \npredictions  setosa versicolor virginica\n  setosa         14          0         0\n  versicolor      0         17         0\n  virginica       0          1        13",
    "crumbs": [
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>ranger</span>"
    ]
  },
  {
    "objectID": "articles/ranger.html#ハイパーパラメーターのチューニング",
    "href": "articles/ranger.html#ハイパーパラメーターのチューニング",
    "title": "ranger",
    "section": "ハイパーパラメーターのチューニング",
    "text": "ハイパーパラメーターのチューニング\nrangerのRandomForestモデルにおける主なハイパーパラメーターは以下の通りです。\n\n決定木を生成する際に使用するパラメータの数(mtry)\n生成する決定木の数(num.trees)\n\nこれらのハイパーパラメーターの最適な設定を探す作業がハイパーパラメーターのチューニングとなります。rangerのハイパーパラメーターのチューニング用にはtuneRanger等のパッケージがありますが、ここではnum.treesについて直接パラメータ設定を変更して精度比較を実施します。\nなお、rangerのRandomForestモデルではOOBError(Out-Of-bag Error)が算出されます。これはモデル構築時に一部データを学習に使用しない代わりにモデル検証に使用して誤差率を求めています。そのため、クロスバリデーションをしなくても、ある程度の汎化性能を測ることができます。\nnum.trees = 300としてモデル構築します。OOBErrorは5.71%です。\n\nset.seed(123)  \n(model.num.trees.300 &lt;- ranger(Species ~ ., data = train_data, num.trees = 300))\n\nRanger result\n\nCall:\n ranger(Species ~ ., data = train_data, num.trees = 300) \n\nType:                             Classification \nNumber of trees:                  300 \nSample size:                      105 \nNumber of independent variables:  4 \nMtry:                             2 \nTarget node size:                 1 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error:             5.71 % \n\n\nnum.trees = 500としてモデル構築します。OOBErrorは5.71%です。\n\nset.seed(123)  \n(model.num.trees.500 &lt;- ranger(Species ~ ., data = train_data, num.trees = 500))\n\nRanger result\n\nCall:\n ranger(Species ~ ., data = train_data, num.trees = 500) \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      105 \nNumber of independent variables:  4 \nMtry:                             2 \nTarget node size:                 1 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error:             5.71 % \n\n\nnum.trees = 700としてモデル構築します。OOBErrorは4.76%です。\n\nset.seed(123)  \n(model.num.trees.700 &lt;- ranger(Species ~ ., data = train_data, num.trees = 700))\n\nRanger result\n\nCall:\n ranger(Species ~ ., data = train_data, num.trees = 700) \n\nType:                             Classification \nNumber of trees:                  700 \nSample size:                      105 \nNumber of independent variables:  4 \nMtry:                             2 \nTarget node size:                 1 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error:             4.76 % \n\n\nnum.trees = 700のときにOOBErrorが最も小さくなったので、そのモデルにてテストデータで精度を測ってみます。もともと精度が高いため、結果は変わりませんでした。\n\npredictions.num.trees &lt;- predict(model.num.trees.700, data = test_data)$predictions\n(confusion_matrix &lt;- table(predictions.num.trees, test_data$Species))\n\n                     \npredictions.num.trees setosa versicolor virginica\n           setosa         14          0         0\n           versicolor      0         17         0\n           virginica       0          1        13",
    "crumbs": [
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>ranger</span>"
    ]
  },
  {
    "objectID": "articles/readxl.html",
    "href": "articles/readxl.html",
    "title": "readxl",
    "section": "",
    "text": "パッケージの概要\nreadxlはExcelブックをRで読み込むためのパッケージです。 列名やデータ型をある程度自動で判定し、テーブルの形で(data.frame型として)データを読み込むことが出来ます。 読み取りたい領域をセル番地等で指定することも可能です。\n※Excelブックの編集・書き込みは本パッケージではできないため、openxlsxパッケージなどの他のパッケージを使用してください。",
    "crumbs": [
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>readxl</span>"
    ]
  },
  {
    "objectID": "articles/readxl.html#実行前の準備",
    "href": "articles/readxl.html#実行前の準備",
    "title": "readxl",
    "section": "実行前の準備",
    "text": "実行前の準備\n\nlibrary(readxl)",
    "crumbs": [
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>readxl</span>"
    ]
  },
  {
    "objectID": "articles/readxl.html#データの準備",
    "href": "articles/readxl.html#データの準備",
    "title": "readxl",
    "section": "データの準備",
    "text": "データの準備\nreadxlパッケージには例となるExcelブックがいくつか用意されています。\nreadxl_example関数を引数無しで実行することでブックのリストが得られます。\n\nreadxl_example()\n\n [1] \"clippy.xls\"    \"clippy.xlsx\"   \"datasets.xls\"  \"datasets.xlsx\"\n [5] \"deaths.xls\"    \"deaths.xlsx\"   \"geometry.xls\"  \"geometry.xlsx\"\n [9] \"type-me.xls\"   \"type-me.xlsx\" \n\n\n引数にブック名を指定することでフルパスが得られます。 ここではirisデータセットなどが含まれるdatasets.xlsxを使用することにします。\n\npath_datasets &lt;- readxl_example(\"datasets.xlsx\")\n\nまた、日本アクチュアリー会のホームページで公開されている標準生命表のExcelファイルも例として使用します。\n\npath_seimeihyo &lt;- \"../data/seimeihyo960718.xlsx\"",
    "crumbs": [
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>readxl</span>"
    ]
  },
  {
    "objectID": "articles/readxl.html#基本的な使い方",
    "href": "articles/readxl.html#基本的な使い方",
    "title": "readxl",
    "section": "基本的な使い方",
    "text": "基本的な使い方\n\nシート名の取得\nexcel_sheets関数でブックに含まれるシートの一覧を取得することができます。\n\nexcel_sheets(path_datasets)\n\n[1] \"mtcars\"   \"chickwts\" \"quakes\"  \n\nexcel_sheets(path_seimeihyo)\n\n[1] \"標準生命表\"\n\n\n\n\nシートの読み取り\nread_excel関数で、指定したシートのデータをdata.frame(正確にはtibble)として読み込むことが出来ます。\n\ndf &lt;- read_excel(path_seimeihyo, sheet = \"標準生命表\")\nstr(df) #テーブルの構造を表示する\n\ntibble [133 × 15] (S3: tbl_df/tbl/data.frame)\n $ ...1                  : chr [1:133] \"年齢\" NA \"0\" \"1\" ...\n $ 標準生命表２０１８    : chr [1:133] \"生保標準生命表２０１８（死亡保険用）\" \"男性\" \"8.1000000000000006E-4\" \"5.6000000000000006E-4\" ...\n $ ...3                  : chr [1:133] NA \"女性\" \"7.7999999999999999E-4\" \"5.2999999999999998E-4\" ...\n $ ...4                  : chr [1:133] \"第三分野標準生命表２０１８\" \"男性\" \"5.2999999999999998E-4\" \"2.2000000000000001E-4\" ...\n $ ...5                  : chr [1:133] NA \"女性\" \"5.1999999999999995E-4\" \"2.0000000000000001E-4\" ...\n $ 標準生命表２００７    : chr [1:133] \"生保標準生命表２００７（死亡保険用）\" \"男性\" \"1.08E-3\" \"7.5000000000000002E-4\" ...\n $ ...7                  : chr [1:133] NA \"女性\" \"9.6000000000000002E-4\" \"6.6E-4\" ...\n $ ...8                  : chr [1:133] \"生保標準生命表２００７（年金開始後用）\" \"男性\" \"5.8E-4\" \"2.5999999999999998E-4\" ...\n $ ...9                  : chr [1:133] NA \"女性\" \"4.6999999999999999E-4\" \"2.2000000000000001E-4\" ...\n $ ...10                 : chr [1:133] \"第三分野標準生命表２００７\" \"男性\" \"5.8E-4\" \"4.0999999999999999E-4\" ...\n $ ...11                 : chr [1:133] NA \"女性\" \"5.1000000000000004E-4\" \"3.6000000000000002E-4\" ...\n $ 生保標準生命表１９９６: chr [1:133] \"生保標準生命表１９９６（死亡保険用）\" \"男性\" \"1.1000000000000001E-3\" \"7.6000000000000004E-4\" ...\n $ ...13                 : chr [1:133] NA \"女性\" \"9.3999999999999997E-4\" \"6.8999999999999997E-4\" ...\n $ ...14                 : chr [1:133] \"生保標準生命表１９９６（年金開始後用）\" \"男性\" NA NA ...\n $ ...15                 : chr [1:133] NA \"女性\" NA NA ...\n\n#$ 列名 : 型名 [1:行数] 最初の方のデータ…　という形式で表示\n\nデフォルトではデータがある領域全体を一つのテーブルとし、さらに1行目に列名があるものとみなして読み取られます。\n標準生命表のExcelファイルは列名が3行にわたっているため、1, 2行目のレコードにまで列名が入ってしまいました。 また、副次的にすべての列が文字列型(chr)で読み込まれてしまっているため、このままでは数値データとして取り扱うことができません。\n最もシンプルな解決策は読み取り領域を指定することです。\n\ndf_qx &lt;- read_excel(path_seimeihyo, sheet = \"標準生命表\", range = \"G4:H135\") #生保標準2007 死亡保険用\n#読み取り範囲をG4(列名として「男性」「女性」がある行)からとしていることに注意\nstr(df_qx)\n\ntibble [131 × 2] (S3: tbl_df/tbl/data.frame)\n $ 男性: num [1:131] 0.00108 0.00075 0.00049 0.00031 0.00021 0.00017 0.00016 0.00016 0.00016 0.00015 ...\n $ 女性: num [1:131] 0.00096 0.00066 0.00042 0.00026 0.00016 0.00012 0.00012 0.00012 0.00011 0.0001 ...\n\n\n列名がレコードに混じる現象が解消され、また型の自動判定により数値型(num)として読み込むことができました。\nこのままでも使えなくはありませんが、年齢の列が失われているのは使いづらいため、これを別途読み込んで追加してみます。\n引数col_namesに列名を与えることができます。この場合、1行目から列名ではなくレコードがあるものとして読み込みます。\n\ndf_x &lt;- read_excel(path_seimeihyo, sheet = \"標準生命表\", range = \"B5:B135\", col_names = c(\"年齢\"))\n#読み取り範囲をB4からではなくB5(データがある行)からとしていることに注意\nstr(df_x)\n\ntibble [131 × 1] (S3: tbl_df/tbl/data.frame)\n $ 年齢: num [1:131] 0 1 2 3 4 5 6 7 8 9 ...\n\n\ncbind関数で結合することにより、年齢と死亡率を1つのテーブルに格納することができました。\n\ndf &lt;- cbind(df_x, df_qx)\nstr(df)\n\n'data.frame':   131 obs. of  3 variables:\n $ 年齢: num  0 1 2 3 4 5 6 7 8 9 ...\n $ 男性: num  0.00108 0.00075 0.00049 0.00031 0.00021 0.00017 0.00016 0.00016 0.00016 0.00015 ...\n $ 女性: num  0.00096 0.00066 0.00042 0.00026 0.00016 0.00012 0.00012 0.00012 0.00011 0.0001 ...",
    "crumbs": [
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>readxl</span>"
    ]
  },
  {
    "objectID": "articles/readxl.html#補足情報",
    "href": "articles/readxl.html#補足情報",
    "title": "readxl",
    "section": "補足情報",
    "text": "補足情報\n\nグラフの確認\n本稿の主題からは逸れますが、折角生命表を読み込んだので折れ線グラフを描いて確認してみましょう。\n\nplot(0, 0, type = \"n\", main = \"標準生命表2007 (死亡保険用)\", xlab = \"x\", ylab = \"qx\", xlim = c(0,120), ylim = c(0,1))#plot関数で描画エリアを用意\n\ncols &lt;- c(rgb(0,0,1), rgb(1,0,0)) #色の指定を変数に格納 青, 赤の順で指定\nltys &lt;- c(\"dotted\", \"dashed\") #線の種類を変数に格納 点線、破線の順で指定\n\nlines(x = df$年齢, y = df$男性, col = cols[1], lty = ltys[1]) #折れ線を1つずつ追加\nlines(x = df$年齢, y = df$女性, col = cols[2], lty = ltys[2])\n\nlegend(\"left\", legend = c(\"男性\", \"女性\"), col = cols, lty = ltys) #凡例を左側に表示\n\n\n\n\n\n\n\n\n記法が独特なものの、ggplot2パッケージを用いる方法もあります。\n\nlibrary(ggplot2)\nlibrary(reshape2) #テーブルを縦長に変形するために使用\ndf_melt &lt;- reshape2::melt(df, id.vars = \"年齢\", measure.vars = c(\"男性\", \"女性\"),\n                          value.name = \"死亡率\", variable.name = \"性別\") #男性と女性のデータが縦に並ぶようにする\nstr(df_melt)\n\n'data.frame':   262 obs. of  3 variables:\n $ 年齢  : num  0 1 2 3 4 5 6 7 8 9 ...\n $ 性別  : Factor w/ 2 levels \"男性\",\"女性\": 1 1 1 1 1 1 1 1 1 1 ...\n $ 死亡率: num  0.00108 0.00075 0.00049 0.00031 0.00021 0.00017 0.00016 0.00016 0.00016 0.00015 ...\n\nggplot(data = df_melt) + geom_line(mapping = aes(x = 年齢, y = 死亡率, color = 性別, linetype = 性別)) +\n  scale_x_continuous(limits = c(0, 120)) + #表示範囲の調節\n  scale_color_manual(values = c(rgb(0,0,1), rgb(1,0,0))) + #折れ線の見た目を1つ前のグラフと同様に設定\n  scale_linetype_manual(values = c(\"dotted\", \"dashed\"))\n\n\n\n\n\n\n\n\nRによるデータ可視化方法についてはアクチュアリージャーナルにも記載があります。 詳しくは データサイエンス関連基礎調査WG (2020) を参照してください。\n\n\n読み取り範囲の指定方法\nセル番地での指定が最もわかりやすいですが、他の指定方法もあるため紹介します。\n\n引数sheetではシート名だけでなくシート番号で指定することもできます。\n引数rangeの指定には、行全体を表すcell_rowsや列全体を表すcell_colsを用いることができます。\n\n\ndf_ &lt;- read_excel(path_seimeihyo, sheet = 1, range = cell_cols(c(2:4))) #2～4列目全体を取得\nstr(df_)\n\ntibble [133 × 3] (S3: tbl_df/tbl/data.frame)\n $ ...1              : chr [1:133] \"年齢\" NA \"0\" \"1\" ...\n $ 標準生命表２０１８: chr [1:133] \"生保標準生命表２０１８（死亡保険用）\" \"男性\" \"8.1000000000000006E-4\" \"5.6000000000000006E-4\" ...\n $ ...3              : chr [1:133] NA \"女性\" \"7.7999999999999999E-4\" \"5.2999999999999998E-4\" ...\n\n\n\ndf_ &lt;- read_excel(path_seimeihyo, sheet = 1, range = cell_rows(c(4, NA))) #4行目以降を取得\nstr(df_)\n\ntibble [131 × 15] (S3: tbl_df/tbl/data.frame)\n $ ...1     : num [1:131] 0 1 2 3 4 5 6 7 8 9 ...\n $ 男性...2 : num [1:131] 0.00081 0.00056 0.00036 0.00022 0.00014 0.0001 0.00009 0.00009 0.00009 0.00009 ...\n $ 女性...3 : num [1:131] 0.00078 0.00053 0.00033 0.00019 0.00011 0.00008 0.00008 0.00008 0.00007 0.00007 ...\n $ 男性...4 : num [1:131] 0.00053 0.00022 0.00015 0.00011 0.00008 0.00006 0.00006 0.00006 0.00005 0.00005 ...\n $ 女性...5 : num [1:131] 0.00052 0.0002 0.00014 0.00009 0.00007 0.00006 0.00005 0.00005 0.00004 0.00004 ...\n $ 男性...6 : num [1:131] 0.00108 0.00075 0.00049 0.00031 0.00021 0.00017 0.00016 0.00016 0.00016 0.00015 ...\n $ 女性...7 : num [1:131] 0.00096 0.00066 0.00042 0.00026 0.00016 0.00012 0.00012 0.00012 0.00011 0.0001 ...\n $ 男性...8 : num [1:131] 0.00058 0.00026 0.0002 0.00014 0.00011 0.00009 0.00009 0.00007 0.00006 0.00006 ...\n $ 女性...9 : num [1:131] 0.00047 0.00022 0.00015 0.0001 0.00007 0.00006 0.00006 0.00005 0.00004 0.00004 ...\n $ 男性...10: num [1:131] 0.00058 0.00041 0.00026 0.00017 0.00011 0.00009 0.00009 0.00008 0.00008 0.00007 ...\n $ 女性...11: num [1:131] 0.00051 0.00036 0.00023 0.00014 0.00009 0.00007 0.00007 0.00006 0.00006 0.00006 ...\n $ 男性...12: num [1:131] 0.0011 0.00076 0.0005 0.00033 0.00024 0.00022 0.00022 0.00021 0.00019 0.00017 ...\n $ 女性...13: num [1:131] 0.00094 0.00069 0.00048 0.00031 0.0002 0.00014 0.00013 0.00013 0.00013 0.00012 ...\n $ 男性...14: num [1:131] NA NA NA NA NA NA NA NA NA NA ...\n $ 女性...15: num [1:131] NA NA NA NA NA NA NA NA NA NA ...\n\n\n\nシート名の指定を引数rangeに含めることもできます\n\n\ndf_ &lt;- read_excel(path_seimeihyo, range = \"標準生命表!B4:D135\")\nstr(df_)\n\ntibble [131 × 3] (S3: tbl_df/tbl/data.frame)\n $ ...1: num [1:131] 0 1 2 3 4 5 6 7 8 9 ...\n $ 男性: num [1:131] 0.00081 0.00056 0.00036 0.00022 0.00014 0.0001 0.00009 0.00009 0.00009 0.00009 ...\n $ 女性: num [1:131] 0.00078 0.00053 0.00033 0.00019 0.00011 0.00008 0.00008 0.00008 0.00007 0.00007 ...\n\n\n\n引数rangeでは、anchored(左上のセル番地, c(行数, 列数)) という指定が可能です\n\n\ndf_ &lt;- read_excel(path_seimeihyo, sheet = \"標準生命表\", range = anchored(\"B4\", c(121, 3)))\nstr(df_)\n\ntibble [120 × 3] (S3: tbl_df/tbl/data.frame)\n $ ...1: num [1:120] 0 1 2 3 4 5 6 7 8 9 ...\n $ 男性: num [1:120] 0.00081 0.00056 0.00036 0.00022 0.00014 0.0001 0.00009 0.00009 0.00009 0.00009 ...\n $ 女性: num [1:120] 0.00078 0.00053 0.00033 0.00019 0.00011 0.00008 0.00008 0.00008 0.00007 0.00007 ...\n\n\n\n引数rangeを指定する代わりに、冒頭の読み飛ばし行数skipと読み取る行数n_maxを指定することもできます。\n\n\ndf_ &lt;- read_excel(path_seimeihyo, sheet = \"標準生命表\", skip = 3, n_max = 30)\nstr(df_)\n\ntibble [30 × 15] (S3: tbl_df/tbl/data.frame)\n $ ...1     : num [1:30] 0 1 2 3 4 5 6 7 8 9 ...\n $ 男性...2 : num [1:30] 0.00081 0.00056 0.00036 0.00022 0.00014 0.0001 0.00009 0.00009 0.00009 0.00009 ...\n $ 女性...3 : num [1:30] 0.00078 0.00053 0.00033 0.00019 0.00011 0.00008 0.00008 0.00008 0.00007 0.00007 ...\n $ 男性...4 : num [1:30] 0.00053 0.00022 0.00015 0.00011 0.00008 0.00006 0.00006 0.00006 0.00005 0.00005 ...\n $ 女性...5 : num [1:30] 0.00052 0.0002 0.00014 0.00009 0.00007 0.00006 0.00005 0.00005 0.00004 0.00004 ...\n $ 男性...6 : num [1:30] 0.00108 0.00075 0.00049 0.00031 0.00021 0.00017 0.00016 0.00016 0.00016 0.00015 ...\n $ 女性...7 : num [1:30] 0.00096 0.00066 0.00042 0.00026 0.00016 0.00012 0.00012 0.00012 0.00011 0.0001 ...\n $ 男性...8 : num [1:30] 0.00058 0.00026 0.0002 0.00014 0.00011 0.00009 0.00009 0.00007 0.00006 0.00006 ...\n $ 女性...9 : num [1:30] 0.00047 0.00022 0.00015 0.0001 0.00007 0.00006 0.00006 0.00005 0.00004 0.00004 ...\n $ 男性...10: num [1:30] 0.00058 0.00041 0.00026 0.00017 0.00011 0.00009 0.00009 0.00008 0.00008 0.00007 ...\n $ 女性...11: num [1:30] 0.00051 0.00036 0.00023 0.00014 0.00009 0.00007 0.00007 0.00006 0.00006 0.00006 ...\n $ 男性...12: num [1:30] 0.0011 0.00076 0.0005 0.00033 0.00024 0.00022 0.00022 0.00021 0.00019 0.00017 ...\n $ 女性...13: num [1:30] 0.00094 0.00069 0.00048 0.00031 0.0002 0.00014 0.00013 0.00013 0.00013 0.00012 ...\n $ 男性...14: num [1:30] NA NA NA NA NA NA NA NA NA NA ...\n $ 女性...15: num [1:30] NA NA NA NA NA NA NA NA NA NA ...\n\n\n詳細は readxl authors (2023b) や readxl authors (2023c) を参照してください。\n\n\n列名が複数行にわたる場合\nExcelのデータでは今回の標準生命表のように、列名が複数行にわたって表示されることがよくあります。\n事前にExcelのデータを修正しておくことも考えられますが、読み込む側の工夫でもある程度対処は可能なため、 参考までに一例を記載します。\nreadxlパッケージの関数で一発でうまく読み取れるわけではないため、 まず列名がある箇所をいったんデータとして読み取り、Rで加工のうえ列名をリストに保持し、 改めてデータのある箇所を読み取るという流れで対処することになります。\nまず、列名がある部分(Excelシート上の3～4行目)をデータとして読み取ります。\n\n#col_names = FALSEとすることにより、1行目を(テーブルdf_colnamesの)列名ではなくデータとして読み取る\ndf_colnames &lt;- read_excel(path_seimeihyo, sheet = \"標準生命表\", range = cell_rows(c(3:4)), col_names = FALSE)\nstr(df_colnames)\n\ntibble [2 × 15] (S3: tbl_df/tbl/data.frame)\n $ ...1 : chr [1:2] \"年齢\" NA\n $ ...2 : chr [1:2] \"生保標準生命表２０１８（死亡保険用）\" \"男性\"\n $ ...3 : chr [1:2] NA \"女性\"\n $ ...4 : chr [1:2] \"第三分野標準生命表２０１８\" \"男性\"\n $ ...5 : chr [1:2] NA \"女性\"\n $ ...6 : chr [1:2] \"生保標準生命表２００７（死亡保険用）\" \"男性\"\n $ ...7 : chr [1:2] NA \"女性\"\n $ ...8 : chr [1:2] \"生保標準生命表２００７（年金開始後用）\" \"男性\"\n $ ...9 : chr [1:2] NA \"女性\"\n $ ...10: chr [1:2] \"第三分野標準生命表２００７\" \"男性\"\n $ ...11: chr [1:2] NA \"女性\"\n $ ...12: chr [1:2] \"生保標準生命表１９９６（死亡保険用）\" \"男性\"\n $ ...13: chr [1:2] NA \"女性\"\n $ ...14: chr [1:2] \"生保標準生命表１９９６（年金開始後用）\" \"男性\"\n $ ...15: chr [1:2] NA \"女性\"\n\n\n今回のデータでは1行目の「生保標準生命表２０１８（死亡保険用）」などがそれぞれ一番左の列（各「男性」の列）にしか入れられていないため、 残りの列（各「女性」の列）にもこれを補完します。\n\nfor(row in 1:(nrow(df_colnames))){\n  colname &lt;- NA #1つ左の列の列名を保持する変数\n  for(col in 1:(ncol(df_colnames))){\n    if(row &gt; 1 & col &gt; 1){\n      if(is.na(df_colnames[row-1, col-1]) != is.na(df_colnames[row-1, col]) \n         || ( !is.na(df_colnames[row-1, col-1]) && !is.na(df_colnames[row-1, col])\n           && df_colnames[row-1, col-1] != df_colnames[row-1, col])){ #1つ上の階層の列名が変わった場合\n        colname &lt;- NA #今の階層もクリア\n      }\n    }\n    if(is.na(df_colnames[row, col]))\n      df_colnames[row, col] &lt;- colname\n    colname &lt;- df_colnames[row, col]\n  }\n}\ndf_colnames\n\n# A tibble: 2 × 15\n  ...1  ...2   ...3  ...4  ...5  ...6  ...7  ...8  ...9  ...10 ...11 ...12 ...13\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 年齢  生保標準生… 生保標準… 第三分野… 第三分野… 生保標準… 生保標準… 生保標準… 生保標準… 第三分野… 第三分野… 生保標準… 生保標準…\n2 &lt;NA&gt;  男性   女性  男性  女性  男性  女性  男性  女性  男性  女性  男性  女性 \n# ℹ 2 more variables: ...14 &lt;chr&gt;, ...15 &lt;chr&gt;\n\n\nそして、NAとなっている箇所を読み飛ばしつつ、各列名を結合したリストを作成します。\n結合時の区切り文字は何でも指定可能ですが、今回は\"/\"にしてみます。\n\nls_colnames &lt;- sapply(df_colnames, function(x) paste0(x[!is.na(x)], collapse = \"/\"))\nnames(ls_colnames) &lt;- NULL\nls_colnames\n\n [1] \"年齢\"                                       \n [2] \"生保標準生命表２０１８（死亡保険用）/男性\"  \n [3] \"生保標準生命表２０１８（死亡保険用）/女性\"  \n [4] \"第三分野標準生命表２０１８/男性\"            \n [5] \"第三分野標準生命表２０１８/女性\"            \n [6] \"生保標準生命表２００７（死亡保険用）/男性\"  \n [7] \"生保標準生命表２００７（死亡保険用）/女性\"  \n [8] \"生保標準生命表２００７（年金開始後用）/男性\"\n [9] \"生保標準生命表２００７（年金開始後用）/女性\"\n[10] \"第三分野標準生命表２００７/男性\"            \n[11] \"第三分野標準生命表２００７/女性\"            \n[12] \"生保標準生命表１９９６（死亡保険用）/男性\"  \n[13] \"生保標準生命表１９９６（死亡保険用）/女性\"  \n[14] \"生保標準生命表１９９６（年金開始後用）/男性\"\n[15] \"生保標準生命表１９９６（年金開始後用）/女性\"\n\n\nこうしてできた列名リストを引数col_namesに与え、元のデータ(Excelシート上の5行目以降)を読み取ります。\n\ndf_all &lt;- read_excel(path_seimeihyo, sheet = \"標準生命表\", range = cell_rows(c(5, NA)), col_names = ls_colnames)\nstr(df_all)\n\ntibble [131 × 15] (S3: tbl_df/tbl/data.frame)\n $ 年齢                                       : num [1:131] 0 1 2 3 4 5 6 7 8 9 ...\n $ 生保標準生命表２０１８（死亡保険用）/男性  : num [1:131] 0.00081 0.00056 0.00036 0.00022 0.00014 0.0001 0.00009 0.00009 0.00009 0.00009 ...\n $ 生保標準生命表２０１８（死亡保険用）/女性  : num [1:131] 0.00078 0.00053 0.00033 0.00019 0.00011 0.00008 0.00008 0.00008 0.00007 0.00007 ...\n $ 第三分野標準生命表２０１８/男性            : num [1:131] 0.00053 0.00022 0.00015 0.00011 0.00008 0.00006 0.00006 0.00006 0.00005 0.00005 ...\n $ 第三分野標準生命表２０１８/女性            : num [1:131] 0.00052 0.0002 0.00014 0.00009 0.00007 0.00006 0.00005 0.00005 0.00004 0.00004 ...\n $ 生保標準生命表２００７（死亡保険用）/男性  : num [1:131] 0.00108 0.00075 0.00049 0.00031 0.00021 0.00017 0.00016 0.00016 0.00016 0.00015 ...\n $ 生保標準生命表２００７（死亡保険用）/女性  : num [1:131] 0.00096 0.00066 0.00042 0.00026 0.00016 0.00012 0.00012 0.00012 0.00011 0.0001 ...\n $ 生保標準生命表２００７（年金開始後用）/男性: num [1:131] 0.00058 0.00026 0.0002 0.00014 0.00011 0.00009 0.00009 0.00007 0.00006 0.00006 ...\n $ 生保標準生命表２００７（年金開始後用）/女性: num [1:131] 0.00047 0.00022 0.00015 0.0001 0.00007 0.00006 0.00006 0.00005 0.00004 0.00004 ...\n $ 第三分野標準生命表２００７/男性            : num [1:131] 0.00058 0.00041 0.00026 0.00017 0.00011 0.00009 0.00009 0.00008 0.00008 0.00007 ...\n $ 第三分野標準生命表２００７/女性            : num [1:131] 0.00051 0.00036 0.00023 0.00014 0.00009 0.00007 0.00007 0.00006 0.00006 0.00006 ...\n $ 生保標準生命表１９９６（死亡保険用）/男性  : num [1:131] 0.0011 0.00076 0.0005 0.00033 0.00024 0.00022 0.00022 0.00021 0.00019 0.00017 ...\n $ 生保標準生命表１９９６（死亡保険用）/女性  : num [1:131] 0.00094 0.00069 0.00048 0.00031 0.0002 0.00014 0.00013 0.00013 0.00013 0.00012 ...\n $ 生保標準生命表１９９６（年金開始後用）/男性: num [1:131] NA NA NA NA NA NA NA NA NA NA ...\n $ 生保標準生命表１９９６（年金開始後用）/女性: num [1:131] NA NA NA NA NA NA NA NA NA NA ...\n\n\n\n\n欠損値への対処\nread_excel関数の引数naに文字列を指定することで、その文字列があるセルを欠損値NAとみなして読み込むことが出来ます。 ベクトルを指定することで複数の文字列を指定することもできます。\n例えばchichwtsデータセットで文字列\"horsebean\"と\"soybean\"をNAとみなして読み込んでみましょう。\n\ndf_chick &lt;- read_excel(path_datasets, sheet = \"chickwts\")\ndf_chick$feed\n\n [1] \"horsebean\" \"horsebean\" \"horsebean\" \"horsebean\" \"horsebean\" \"horsebean\"\n [7] \"horsebean\" \"horsebean\" \"horsebean\" \"horsebean\" \"linseed\"   \"linseed\"  \n[13] \"linseed\"   \"linseed\"   \"linseed\"   \"linseed\"   \"linseed\"   \"linseed\"  \n[19] \"linseed\"   \"linseed\"   \"linseed\"   \"linseed\"   \"soybean\"   \"soybean\"  \n[25] \"soybean\"   \"soybean\"   \"soybean\"   \"soybean\"   \"soybean\"   \"soybean\"  \n[31] \"soybean\"   \"soybean\"   \"soybean\"   \"soybean\"   \"soybean\"   \"soybean\"  \n[37] \"sunflower\" \"sunflower\" \"sunflower\" \"sunflower\" \"sunflower\" \"sunflower\"\n[43] \"sunflower\" \"sunflower\" \"sunflower\" \"sunflower\" \"sunflower\" \"sunflower\"\n[49] \"meatmeal\"  \"meatmeal\"  \"meatmeal\"  \"meatmeal\"  \"meatmeal\"  \"meatmeal\" \n[55] \"meatmeal\"  \"meatmeal\"  \"meatmeal\"  \"meatmeal\"  \"meatmeal\"  \"casein\"   \n[61] \"casein\"    \"casein\"    \"casein\"    \"casein\"    \"casein\"    \"casein\"   \n[67] \"casein\"    \"casein\"    \"casein\"    \"casein\"    \"casein\"   \n\n\n\ndf_chick_na &lt;- read_excel(path_datasets, sheet = \"chickwts\", na = c(\"horsebean\", \"soybean\"))\ndf_chick_na$feed\n\n [1] NA          NA          NA          NA          NA          NA         \n [7] NA          NA          NA          NA          \"linseed\"   \"linseed\"  \n[13] \"linseed\"   \"linseed\"   \"linseed\"   \"linseed\"   \"linseed\"   \"linseed\"  \n[19] \"linseed\"   \"linseed\"   \"linseed\"   \"linseed\"   NA          NA         \n[25] NA          NA          NA          NA          NA          NA         \n[31] NA          NA          NA          NA          NA          NA         \n[37] \"sunflower\" \"sunflower\" \"sunflower\" \"sunflower\" \"sunflower\" \"sunflower\"\n[43] \"sunflower\" \"sunflower\" \"sunflower\" \"sunflower\" \"sunflower\" \"sunflower\"\n[49] \"meatmeal\"  \"meatmeal\"  \"meatmeal\"  \"meatmeal\"  \"meatmeal\"  \"meatmeal\" \n[55] \"meatmeal\"  \"meatmeal\"  \"meatmeal\"  \"meatmeal\"  \"meatmeal\"  \"casein\"   \n[61] \"casein\"    \"casein\"    \"casein\"    \"casein\"    \"casein\"    \"casein\"   \n[67] \"casein\"    \"casein\"    \"casein\"    \"casein\"    \"casein\"   \n\n\nこのように、一定の文字列をNAに変換するだけなら簡単に対処が可能です。",
    "crumbs": [
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>readxl</span>"
    ]
  },
  {
    "objectID": "articles/readxl.html#参考",
    "href": "articles/readxl.html#参考",
    "title": "readxl",
    "section": "参考",
    "text": "参考\n\n\nreadxl authors. 2023a. “GitHub - Tidyverse/Readxl: Read Excel Files (.xls and .xlsx) into R.” https://github.com/tidyverse/readxl.\n\n\n———. 2023b. “Sheet Geometry.” https://readxl.tidyverse.org/articles/sheet-geometry.html.\n\n\n———. 2023c. “Specify Cells for Reading.” https://readxl.tidyverse.org/reference/cell-specification.html.\n\n\nデータサイエンス関連基礎調査WG. 2020. “Rを用いたデータの可視化技術 解説書.” アクチュアリージャーナル 112 (September): 1–88.",
    "crumbs": [
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>readxl</span>"
    ]
  },
  {
    "objectID": "articles/rpart.html",
    "href": "articles/rpart.html",
    "title": "rpart",
    "section": "",
    "text": "パッケージの概要\nrpartは再帰的分割による回帰木・分類木の実装を与えます。また、rpart.plotで決定木の可視化が可能です。",
    "crumbs": [
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>rpart</span>"
    ]
  },
  {
    "objectID": "articles/rpart.html#使用例irisデータの分類",
    "href": "articles/rpart.html#使用例irisデータの分類",
    "title": "rpart",
    "section": "使用例：irisデータの分類",
    "text": "使用例：irisデータの分類\nirisデータを用いて、がく弁・花弁の長さ・幅の情報からアヤメの種類を特定する分類モデルを作成します。\n\nirisデータセットを読み込む\nirisデータを読み込み、データの先頭を表示します。\n\nSepal.Length：がく弁の長さ\nSepal.Width：がく弁の幅\nPetal.Length：花弁の長さ\nPetal.Width：花弁の幅\n\nアヤメの種類はsetosa(1)、versicolor(2)、virginica(3)の3種類です。\n\ndata(iris)\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\n\nirisデータの構造\nirisデータの各種構造を確認します。\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nまた、データを散布図にプロットして確認します。\n\nplot(iris, col=c(2, 3, 4)[iris$Species])\n\n\n\n\n\n\n\n\n\n\nモデル構築1（全体データ）\nまずは全てのデータを使って分類木モデルを構築してみます。\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\nWarning: package 'rpart.plot' was built under R version 4.5.1\n\n# シードを設定\nset.seed(123) \n(iris.rp &lt;- rpart(Species ~ ., data = iris))\n\nn= 150 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)  \n  2) Petal.Length&lt; 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *\n  3) Petal.Length&gt;=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)  \n    6) Petal.Width&lt; 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *\n    7) Petal.Width&gt;=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *\n\n\n\n可視化\nモデルを可視化します。plotで木の構造（分岐）を表示し、textで各ノードの分岐の基準や分類ラベルを表示します。デフォルトの設定だと図が見切れてしまうことがあります。\n\nplot(iris.rp)\ntext(iris.rp)\n\n\n\n\n\n\n\n\nrpart.plotを用いると、分類木をより分かりやすく表示させることができます。\n\nrpart.plot(iris.rp)\n\n\n\n\n\n\n\n\n\n\n\nモデル構築2（訓練データとテストデータに分割）\nirisデータをモデル生成のための訓練データと、モデル評価のためのテストデータに分割します。データ割合は訓練データを7割、テストデータを3割とします。確認のため、データサイズを出力します。\n\n# シードを設定\nset.seed(123) \n\n# データの分割\nsample_indices &lt;- sample(1:nrow(iris), 0.7 * nrow(iris))  \ndf.train &lt;- iris[sample_indices, ]\ndf.test &lt;- iris[-sample_indices, ]\n\n# データサイズの確認\nc(nrow(iris), nrow(df.train), nrow(df.test))\n\n[1] 150 105  45\n\n\n\n\nモデル生成\n訓練データを用いて分類木モデルを生成します。\n\n# シードを設定\nset.seed(123)\n(model.rp &lt;- rpart(Species ~ ., data = df.train))\n\nn= 105 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 105 68 virginica (0.34285714 0.30476190 0.35238095)  \n  2) Petal.Length&lt; 2.45 36  0 setosa (1.00000000 0.00000000 0.00000000) *\n  3) Petal.Length&gt;=2.45 69 32 virginica (0.00000000 0.46376812 0.53623188)  \n    6) Petal.Width&lt; 1.75 35  4 versicolor (0.00000000 0.88571429 0.11428571) *\n    7) Petal.Width&gt;=1.75 34  1 virginica (0.00000000 0.02941176 0.97058824) *\n\n\n\nモデル評価\nテストデータを使ってモデル評価を行います。まずはテストデータを元に生成したモデルを用いて予測結果を算出します。\n\nprediction &lt;- predict(model.rp, df.test, type = \"class\")\n\n予測結果とテストデータのもともとのアヤメの分類とを比較します。おおむね正しく分類できていることが分かります。\n\n(result &lt;- table(prediction, df.test$Species))\n\n            \nprediction   setosa versicolor virginica\n  setosa         14          0         0\n  versicolor      0         18         1\n  virginica       0          0        12\n\n(accuracy_prediction &lt;- sum(diag(result)) / sum(result))\n\n[1] 0.9777778\n\n\n\n\n\nハイパーパラメーターのチューニング\nrpartの主なハイパーパラメーターは以下の通りです。\n\n木の複雑度に関するパラメータ(cp)\nノード分割の最小サンプル数(minsplit)\n木の最大の深さ(maxdepth)\n\nこれらのハイパーパラメータの最適な設定を探す作業がハイパーパラメーターのチューニングとなります。\nまずはcpの最適な設定を確認します。これはprintcpを用いることができます。\n\nprintcp(model.rp)\n\n\nClassification tree:\nrpart(formula = Species ~ ., data = df.train)\n\nVariables actually used in tree construction:\n[1] Petal.Length Petal.Width \n\nRoot node error: 68/105 = 0.64762\n\nn= 105 \n\n       CP nsplit rel error  xerror     xstd\n1 0.52941      0  1.000000 1.10294 0.068075\n2 0.39706      1  0.470588 0.47059 0.069364\n3 0.01000      2  0.073529 0.11765 0.039979\n\n\nxerror（交差検証誤差）が最も低くなるcpは0.01でした。これはデフォルトの設定と一致します。\n次に、minsplitのチューニングを行います。簡便的にテストデータでの設定の差を確認します。なお、デフォルトの設定は20です。\n\n# シードを設定\nset.seed(123)\n\n# 候補となる minsplit の値\nminsplit_values &lt;- c(5, 20, 40)\n\n# minsplit ごとの精度を格納するデータフレーム\nresults &lt;- data.frame(minsplit = minsplit_values, Accuracy = NA)\n\n# 各 minsplit のモデルを作成し、精度を測定\nfor (i in seq_along(minsplit_values)) {\n  control &lt;- rpart.control(minsplit = minsplit_values[i])\n  model &lt;- rpart(Species ~ ., data = df.train, method = \"class\", control = control)\n  \n  # 予測\n  predictions &lt;- predict(model, df.test, type = \"class\")\n  accuracy &lt;- mean(predictions == df.test$Species)\n  \n  # 結果を保存\n  results$Accuracy[i] &lt;- accuracy\n}\n\n# 結果の確認\nprint(results)\n\n  minsplit  Accuracy\n1        5 0.9777778\n2       20 0.9777778\n3       40 0.9777778\n\n\nirisデータだと特段変化がないようです。\n最後にmaxdepthについても同様に試してみます。デフォルトの設定は5です。\n\n# シードを設定\nset.seed(123)\n\n# 候補となる maxdepth の値\nmaxdepth_values &lt;- c(3, 5, 7)\n\n# maxdepth ごとの精度を格納するデータフレーム\nresults &lt;- data.frame(maxdepth = maxdepth_values, Accuracy = NA)\n\n# 各 minsplit のモデルを作成し、精度を測定\nfor (i in seq_along(minsplit_values)) {\n  control &lt;- rpart.control(maxdepth = maxdepth_values[i])\n  model &lt;- rpart(Species ~ ., data = df.train, method = \"class\", control = control)\n  \n  # 予測\n  predictions &lt;- predict(model, df.test, type = \"class\")\n  accuracy &lt;- mean(predictions == df.test$Species)\n  \n  # 結果を保存\n  results$Accuracy[i] &lt;- accuracy\n}\n\n# 結果の確認\nprint(results)\n\n  maxdepth  Accuracy\n1        3 0.9777778\n2        5 0.9777778\n3        7 0.9777778\n\n\nこちらもirisデータだとと特段変化がないようです。",
    "crumbs": [
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>rpart</span>"
    ]
  },
  {
    "objectID": "articles/rsample.html",
    "href": "articles/rsample.html",
    "title": "rsample",
    "section": "",
    "text": "パッケージの概要\nrsampleはtidymodelsに含まれるパッケージのうちのひとつで、データ分割・リサンプリングに関する機能を提供します。 学習データと評価データの分割、k分割交差検証やブートストラップ法などが行えるほか、 層化抽出や時系列データの分割にも対応しています。\nlibrary(nycflights13) #今回使用するデータセット\nlibrary(rsample)\n#以下、tidyverse, tidymodelsから必要なパッケージを追加\nlibrary(tibble) #data.frame拡張版\nlibrary(recipes) #前処理\nlibrary(parsnip) #モデル構築\nlibrary(yardstick) #精度評価\nlibrary(workflows) #学習過程のオブジェクト化\nlibrary(ggplot2) #可視化",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>rsample</span>"
    ]
  },
  {
    "objectID": "articles/rsample.html#データセットの準備",
    "href": "articles/rsample.html#データセットの準備",
    "title": "rsample",
    "section": "データセットの準備",
    "text": "データセットの準備\n今回使用するデータセットnycflights13は2013年にニューヨークを出発した航空機に関するデータです。\n変数arr_delayは到着時の遅延時間（分）を表しており、 これを他の説明変数（出発日、離発着地点とその距離、航空会社コード等）から予測するモデルを構築することを考えます。\nデータセットの詳細は Hadley Wickham (2021) を参照してください。\nなお、以下ではmagrittrパッケージによるパイプ演算子%&gt;%1と、 dplyrパッケージによるデータ操作関数を使用しています。\n\ndf_all_raw &lt;- flights #flightsのほかにいくつかデータセットがあるが、それらは今回使用しない\n\n#文字列のfactor型への変換、目的変数がNAとなっているレコードの補完、予測に用いない説明変数の除去\ndf_all &lt;- mutate(df_all_raw, across(where(is.character), as.factor)) %&gt;%\n  mutate(arr_delay = if_else(is.na(arr_delay), mean(df_all_raw$arr_delay, na.rm = TRUE), arr_delay)) %&gt;%\n  select(-time_hour, -tailnum, -arr_time)\n#time_hourはPOSIXct型で表した予定出発日時で、sched_dep_timeと意味合いは同じ。他のデータセットの結合に用いるもの。\n#tailnumは機体番号。カテゴリ数が多すぎて予測に利用するのが難しく、例として取り扱うには向かないため取り除く。\n#arr_timeは実際の到着時刻。予定到着時刻と合わせると目的変数(遅延時間)が判明してしまうため取り除く。\nsummary(df_all)\n\n      year          month             day           dep_time    sched_dep_time\n Min.   :2013   Min.   : 1.000   Min.   : 1.00   Min.   :   1   Min.   : 106  \n 1st Qu.:2013   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.: 907   1st Qu.: 906  \n Median :2013   Median : 7.000   Median :16.00   Median :1401   Median :1359  \n Mean   :2013   Mean   : 6.549   Mean   :15.71   Mean   :1349   Mean   :1344  \n 3rd Qu.:2013   3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.:1744   3rd Qu.:1729  \n Max.   :2013   Max.   :12.000   Max.   :31.00   Max.   :2400   Max.   :2359  \n                                                 NA's   :8255                 \n   dep_delay       sched_arr_time   arr_delay           carrier     \n Min.   : -43.00   Min.   :   1   Min.   : -86.000   UA     :58665  \n 1st Qu.:  -5.00   1st Qu.:1124   1st Qu.: -16.000   B6     :54635  \n Median :  -2.00   Median :1556   Median :  -4.000   EV     :54173  \n Mean   :  12.64   Mean   :1536   Mean   :   6.895   DL     :48110  \n 3rd Qu.:  11.00   3rd Qu.:1945   3rd Qu.:  13.000   AA     :32729  \n Max.   :1301.00   Max.   :2359   Max.   :1272.000   MQ     :26397  \n NA's   :8255                                        (Other):62067  \n     flight     origin            dest           air_time        distance   \n Min.   :   1   EWR:120835   ORD    : 17283   Min.   : 20.0   Min.   :  17  \n 1st Qu.: 553   JFK:111279   ATL    : 17215   1st Qu.: 82.0   1st Qu.: 502  \n Median :1496   LGA:104662   LAX    : 16174   Median :129.0   Median : 872  \n Mean   :1972                BOS    : 15508   Mean   :150.7   Mean   :1040  \n 3rd Qu.:3465                MCO    : 14082   3rd Qu.:192.0   3rd Qu.:1389  \n Max.   :8500                CLT    : 14064   Max.   :695.0   Max.   :4983  \n                             (Other):242450   NA's   :9430                  \n      hour           minute     \n Min.   : 1.00   Min.   : 0.00  \n 1st Qu.: 9.00   1st Qu.: 8.00  \n Median :13.00   Median :29.00  \n Mean   :13.18   Mean   :26.23  \n 3rd Qu.:17.00   3rd Qu.:44.00  \n Max.   :23.00   Max.   :59.00",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>rsample</span>"
    ]
  },
  {
    "objectID": "articles/rsample.html#データの分割",
    "href": "articles/rsample.html#データの分割",
    "title": "rsample",
    "section": "データの分割",
    "text": "データの分割\nモデリングにおいては、構築したモデルの性能を評価するために、 モデルの構築に用いるデータとその性能評価を行うためのデータを切り分けておくことが必要です。 （以下、本稿では前者を「学習データ」、後者を「評価データ」といいます。）\ninitial_split関数を用いることで、この学習データと評価データの切り分けを行うことができます。\n以下のようなコードで、データ全体のうち80%を学習データに、残りを評価データに切り分けることができます。\n\nset.seed(2024)\nsplit_df_all &lt;- initial_split(df_all, prop = 0.8)\ndf_all_train &lt;- training(split_df_all) #学習データ\ndf_all_test &lt;- testing(split_df_all) #評価データ\n\ninitial_split関数で得られるオブジェクトに、 training関数やtesting関数を用いることでそれぞれのデータを得ることが出来ます。\n実際に80%と20%に切り分けられていることを確認してみます。\n\nsplit_df_all #これでそれぞれのデータの行数が確認できる\n\n&lt;Training/Testing/Total&gt;\n&lt;269420/67356/336776&gt;\n\n#実際に分割されたデータの行数がこれに一致することを確認する\ncat(\"全データの行数:\", nrow(df_all),\n    \", 学習データの行数:\", nrow(df_all_train), \", 評価データの行数:\", nrow(df_all_test), \"\\n\")\n\n全データの行数: 336776 , 学習データの行数: 269420 , 評価データの行数: 67356 \n\n#行数だけでなく、データそのものについても確認してみる\ncat(\"全データのarr_delay合計:\", sum(df_all$arr_delay),\n    \"\\n学習データのarr_delay合計:\", sum(df_all_train$arr_delay),\n    \", 評価データのarr_delay合計:\", sum(df_all_test$arr_delay),\n    \"\\n上記の合計:\", sum(df_all_train$arr_delay)+sum(df_all_test$arr_delay), \"\\n\")\n\n全データのarr_delay合計: 2322197 \n学習データのarr_delay合計: 1876791 , 評価データのarr_delay合計: 445406.2 \n上記の合計: 2322197 \n\n\n以下では実際にモデル構築を行っていきますが、 このままでは件数が多くモデル構築に時間がかかってしまいます。\nモデルの精度向上よりも使用例の実行時間短縮を優先するため、以下では件数を減らしておくこととします。\n\nset.seed(2024)\ndf_small &lt;- testing(initial_split(df_all, prop = 0.98)) #336776件から2%分の6736件を取り出す\n\nset.seed(2024)\nsplit_df_small &lt;- initial_split(df_small, prop = 0.9)\ndf_train &lt;- training(split_df_small)\ndf_test &lt;- testing(split_df_small)\n\nsplit_df_small\n\n&lt;Training/Testing/Total&gt;\n&lt;6062/674/6736&gt;",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>rsample</span>"
    ]
  },
  {
    "objectID": "articles/rsample.html#k分割交差検証法",
    "href": "articles/rsample.html#k分割交差検証法",
    "title": "rsample",
    "section": "k分割交差検証法",
    "text": "k分割交差検証法\n\nvfold_cv関数の使い方\nモデルの精度検証でよく用いられる手法としてk分割交差検証法(k-fold cross validation)と呼ばれるものがあります。\nこれは、データをk分割したうえで、1分割目以外のk-1個で学習→1分割目で精度検証、2分割目以外のk-1個で学習→2分割目で精度検証、…k分割目以外のk-1個で学習→k分割目で精度検証、のように学習～検証をk回反復するものです。\n前述の学習データと評価データの分割をさらに発展させたものといえますが、 このk分割交差検証法は主に学習データの中でモデルの精度を検証するために用いられます。 （このことから、以下ではk分割交差検証法で学習に用いるデータを「分析セット」、検証に用いるデータを「検証セット」と呼び分けることとします。）\nrsampleパッケージのvfold_cv2関数を用いることで、 k分割交差検証法のための分析セットや検証セットを簡単に用意することが出来ます。\n\nsplitscv_df_train &lt;- vfold_cv(df_train, v = 4) #4分割\nsplitscv_df_train #k回の学習～検証それぞれに使うsplitが格納されたdata.frame\n\n#  4-fold cross-validation \n# A tibble: 4 × 2\n  splits              id   \n  &lt;list&gt;              &lt;chr&gt;\n1 &lt;split [4546/1516]&gt; Fold1\n2 &lt;split [4546/1516]&gt; Fold2\n3 &lt;split [4547/1515]&gt; Fold3\n4 &lt;split [4547/1515]&gt; Fold4\n\n\n得られるものがデータフレームである以外はinitial_split関数とほぼ同様に使えますが、 initial_splitで用いた関数はtrainingとtestingであった一方、 こちらではanalysisとassessmentになっています。\n\napply(splitscv_df_train, 1, function(row) {\n  id &lt;- row$id\n  split &lt;- row$splits\n  df_analysis &lt;- analysis(split)\n  df_assessment &lt;- assessment(split)\n  paste0(\"id:\", id, \", df_analysisの行数:\", nrow(df_analysis), \", df_assessmentの行数:\", nrow(df_assessment))\n})\n\n[1] \"id:Fold1, df_analysisの行数:4546, df_assessmentの行数:1516\"\n[2] \"id:Fold2, df_analysisの行数:4546, df_assessmentの行数:1516\"\n[3] \"id:Fold3, df_analysisの行数:4547, df_assessmentの行数:1515\"\n[4] \"id:Fold4, df_analysisの行数:4547, df_assessmentの行数:1515\"\n\n\nなお、これによって学習データのk倍分のデータが用意されますが、 メモリの使用量は単にk倍になるわけではなく、うまく節約されます。 大規模なデータで交差検証を行う場合でも問題なく使用することが出来ます。\n\n\n使用例\nvfold_cv関数による交差検証を行う実例3を紹介します。\nまずは前処理やモデルの定義を行います。\n\n#前処理の手順を定義\n##k分割交差検証ではk回同じ前処理を繰り返していわゆる「リーク」を防止することがある\n##そこで前処理の手続き自体をオブジェクト化しておき、後で(for文の中で)実行するという方法をとる \nrec_init &lt;- recipe(df_small, formula = arr_delay ~ .)\napply_poststeps &lt;- function(rec){\n  rec &lt;- rec %&gt;%\n    step_impute_mean(all_numeric_predictors()) %&gt;% #数値型のNAをその平均値で置換\n    step_dummy(all_factor()) #XGBoostを使用する際はすべてのデータを数値型で保持する必要があるため、factor型はダミー変数に変換\n}\n\n#今回用いるモデル(XGBoost)の定義を用意\nmodel_engine_xgboost &lt;- boost_tree(mode = \"regression\", engine = \"xgboost\") %&gt;%\n  #ハイパーパラメータの指定\n  set_args(trees = 300, learn_rate = 0.15, tree_depth = 6, min_n = 1, sample_size = 1, mtry = 75)\n\n#前処理とモデルの定義を1オブジェクトにまとめたもの\nwf_xgboost &lt;- workflow() %&gt;%\n  add_recipe(rec_init %&gt;% apply_poststeps()) %&gt;%\n  add_model(model_engine_xgboost)\n\n最初に最低限の前処理だけを行ったモデルを構築し、 その後は前処理やモデル構築の過程に改良を施すことにより精度改善を目指すこととします。\nここで精度を測る指標として、5分割交差検証によるRMSE平均を採用することとします。 まずは最初のモデルに対して精度を測っておきます。\n\n#5分割で交差検証を行う\nset.seed(2024)\nsplitscv_df_train &lt;- vfold_cv(df_train, v = 5)\n\ndo_cv &lt;- function(splitscv_df_train, wf){#後で使いまわすため、交差検証の手続きを関数化\n  results_cv &lt;- tibble(id = character(), rmse = numeric())\n  for(i in 1:nrow(splitscv_df_train)){\n    t1 &lt;- proc.time()\n    #i番目の分析セットと検証セットを抽出\n    split &lt;- splitscv_df_train$splits[[i]]\n    df_analysis &lt;- analysis(split)\n    df_assessment &lt;- assessment(split)\n    #分析セットで学習\n    set.seed(2024)\n    wfres &lt;- wf %&gt;% fit(data = df_analysis)\n    #検証セットで予測\n    df_test_xypredy &lt;- wfres %&gt;% \n      predict(new_data = df_assessment) %&gt;%\n      bind_cols(df_assessment)\n    #RMSEで精度評価\n    res &lt;- df_test_xypredy %&gt;% \n      yardstick::rmse(truth = arr_delay, estimate = .pred)\n    #評価結果をデータフレームに記録\n    results_cv &lt;- bind_rows(results_cv, tibble(id = splitscv_df_train$id[[i]], rmse = res$.estimate[[1]]))\n    #評価結果と経過時間を出力\n    t2 &lt;- proc.time()\n    tm &lt;- (t2-t1)[3]\n    cat(splitscv_df_train$id[[i]], \"... rmse:\", res$.estimate[[1]], \", 経過時間:\", tm, \"\\n\")\n  }\n  results_cv\n}\n\nwf &lt;- wf_xgboost\nresults_cv &lt;- do_cv(splitscv_df_train, wf) \n\nFold1 ... rmse: 16.06052 , 経過時間: 1.4 \nFold2 ... rmse: 21.37279 , 経過時間: 1.15 \nFold3 ... rmse: 16.59439 , 経過時間: 1.14 \nFold4 ... rmse: 14.79878 , 経過時間: 1.14 \nFold5 ... rmse: 15.17561 , 経過時間: 1.1 \n\nrmse1 &lt;- mean(results_cv$rmse) #RMSEの平均値で精度を測ることにする\ncat(\"rmse平均値:\", rmse1, \"\\n\")\n\nrmse平均値: 16.80042 \n\n\nその後データを分析してみたところ、出発した日付（特徴量名year, month, day）が重要な特徴量であることがわかりました。 遅延を発生させる事象が起きたかどうかを判断するのに日付が重要ということでしょう。 （ホリデーシーズンで乗客が多かった、特定の日付で天候が悪化した、等…）\n今回のデータは年月日が別々の変数に格納されているので、上記のような事象を想定するならば年月日は一つの特徴量にまとめたほうがよいかもしれません。 そこで、月と日をまとめた特徴量monthdayを追加（2013年のデータしかないため年は無視）して精度が向上するか検証してみましょう。\n\n#1/1からの経過日数+1 (うるう年の日付で計算)\nget_monthday &lt;- function(month, day) {\n  as.numeric(difftime(as.Date(paste(2020, month, day, sep = \"-\")),\n                      as.Date(paste(2020, \"01-01\", sep = \"-\")), units = \"days\"))+1\n}\n#「月と日をまとめた特徴量を追加」というステップを前処理に追加\nwf &lt;- wf_xgboost %&gt;%\n  update_recipe(rec_init %&gt;% step_mutate(monthday = get_monthday(month,day)) %&gt;% apply_poststeps())\n\n#先ほどと同じ交差検証を行う\nresults_cv &lt;- do_cv(splitscv_df_train, wf) \n\nFold1 ... rmse: 16.20968 , 経過時間: 1.29 \nFold2 ... rmse: 21.4931 , 経過時間: 1.22 \nFold3 ... rmse: 16.56371 , 経過時間: 1.24 \nFold4 ... rmse: 14.42167 , 経過時間: 1.23 \nFold5 ... rmse: 15.65897 , 経過時間: 1.28 \n\nrmse2 &lt;- mean(results_cv$rmse)\ncat(\"rmse平均値:\", rmse2, \"\\n\")\n\nrmse平均値: 16.86943 \n\n\n特徴量追加前は16.8、追加後は16.87となりました。\nRMSEは小さいほど「精度が良い」という評価になるため、数値のうえでは若干の改善ということになりますが、 大きく結果が変わったわけではなく、今回の分割数では偶然の可能性も否定できません。 分割数を増やしたときに差が生まれるかどうかを確認してみてもよいでしょう。\nともかく、vfold_cv関数を用いることで交差検証のためのデータ分割が手軽に行えることがわかります。",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>rsample</span>"
    ]
  },
  {
    "objectID": "articles/rsample.html#ブートストラップ法",
    "href": "articles/rsample.html#ブートストラップ法",
    "title": "rsample",
    "section": "ブートストラップ法",
    "text": "ブートストラップ法\n\nbootstraps関数の使い方\nブートストラップ法も精度評価に用いられる手法のうちのひとつで、 こちらは単純にデータを分割するのではなく、元データと同じ回数の復元抽出を何度も繰り返すものです。\nrsampleパッケージのbootstraps関数を用いることで、 復元抽出されたデータを簡単に用意することが出来ます。\n\nboots_train &lt;- bootstraps(df_train, times = 4) #4回\nboots_train\n\n# Bootstrap sampling \n# A tibble: 4 × 2\n  splits              id        \n  &lt;list&gt;              &lt;chr&gt;     \n1 &lt;split [6062/2213]&gt; Bootstrap1\n2 &lt;split [6062/2223]&gt; Bootstrap2\n3 &lt;split [6062/2240]&gt; Bootstrap3\n4 &lt;split [6062/2243]&gt; Bootstrap4\n\n\n使用方法はvfold_cv関数と同様です。\nanalysisで得られるものはもとのデータと同じ件数になっています。 一方、assessmentで得られるものは復元抽出で一度も選ばれなかったレコードを集めたもので、 件数はそれぞれの試行で異なります。\n\napply(boots_train, 1, function(row) {\n  id &lt;- row$id\n  split &lt;- row$splits\n  df_analysis &lt;- analysis(split)\n  df_assessment &lt;- assessment(split)\n  paste0(\"id:\", id, \", df_analysisの行数:\", nrow(df_analysis), \", df_assessmentの行数:\", nrow(df_assessment))\n})\n\n[1] \"id:Bootstrap1, df_analysisの行数:6062, df_assessmentの行数:2213\"\n[2] \"id:Bootstrap2, df_analysisの行数:6062, df_assessmentの行数:2223\"\n[3] \"id:Bootstrap3, df_analysisの行数:6062, df_assessmentの行数:2240\"\n[4] \"id:Bootstrap4, df_analysisの行数:6062, df_assessmentの行数:2243\"\n\n\n\n\n使用例\nブートストラップ法は機械学習におけるパラメータや予測値等について、 その分布を図るのに用いられることがあります。\n以下ではブートストラップ法により10組の学習データを用意してみて、 それぞれによって評価データにおける予測値がどう変化するかを確認してみることにします。\n\nset.seed(2024)\nboots_train &lt;- bootstraps(df_train, times = 10) #試行回数10回のブートストラップ法\n\nmodels &lt;- tibble() #各試行での結果を保存するための変数\n#まずそれぞれのデータで学習してモデルを構築\nfor (i in 1:nrow(boots_train)){\n  id &lt;- boots_train$id[[i]]\n  split &lt;- boots_train$splits[[i]]\n  #学習データの前処理\n  recp &lt;- rec_init %&gt;% apply_poststeps() %&gt;% prep(training = analysis(split), fresh = TRUE)\n  df_train_baked_tmp &lt;- recp %&gt;% bake(new_data = NULL)\n  #学習\n  set.seed(2024)\n  model_fitted_xgboost_tmp &lt;- model_engine_xgboost %&gt;% fit(arr_delay ~ ., data = df_train_baked_tmp)\n  models &lt;- bind_rows(models, tibble_row(id = id, recp = recp, model = model_fitted_xgboost_tmp))\n}\n\n#それぞれの予測結果を集める\ndf_test_predy_tmp &lt;- list()\nfor (i in 1:nrow(models)){\n  id &lt;- models$id[[i]]\n  recp &lt;- models$recp[[i]]\n  model_fitted_xgboost_tmp &lt;- models$model[[i]]\n  #評価データの前処理\n  df_test_baked_tmp &lt;- recp %&gt;% bake(new_data = df_test)\n  #評価データで予測し、結果を格納\n  df_test_predy_tmp[[id]] &lt;- predict(model_fitted_xgboost_tmp, new_data = df_test_baked_tmp)\n}\n\nそれぞれの予測値をプロットしてみると次のとおり。\n\ndf_test_predy_tmp_tidy &lt;- tibble()\nfor(id in names(df_test_predy_tmp))\n  df_test_predy_tmp_tidy &lt;- bind_rows(df_test_predy_tmp_tidy,\n                                      tibble(id_split = id, \n                                             id_instance = 1:nrow(df_test_predy_tmp[[id]]), \n                                             .pred = df_test_predy_tmp[[id]][[\".pred\"]]))\ndf_test_predy_tmp_tidy$id_split &lt;- as.factor(df_test_predy_tmp_tidy$id_split)\n\nggplot() +\n  geom_point(aes(x = id_instance, y = .pred, colour = id_split),\n             data = df_test_predy_tmp_tidy %&gt;% filter(id_instance &lt;= 10),\n             position = position_jitter(width = 0.2, height = 0, seed = 2024))\n\n\n\n\n\n\n\n\n条件によるものの、10分程度は遅延時間の予測値がブレるようです。 ※実際の遅延時間がこの分布に従うというわけではなく、あくまで現在のモデル（XGBoost）を前提としたときの予測値のブレであることに注意してください。\nまた、ブートストラップ法によって構築したモデルの平均値等を使用することで過学習を防止する、 バギングという手法も存在しています。\n単体モデルと比較して精度が向上するかを確かめてみましょう。\n\n##まず単体モデルの場合\nset.seed(2024)\n#XGBoost向けの前処理\nrecp &lt;- rec_init %&gt;% apply_poststeps() %&gt;% prep(training = df_train, fresh = TRUE)\ndf_train_baked &lt;- recp %&gt;% bake(new_data = NULL)\ndf_test_baked &lt;- recp %&gt;% bake(new_data = df_test)\n#学習\nmodel_fitted_xgboost &lt;- model_engine_xgboost %&gt;% fit(arr_delay ~ ., data = df_train_baked)\n#予測\ndf_test_predy &lt;- predict(model_fitted_xgboost, new_data = df_test_baked)\ndf_test_xypredy &lt;- df_test_predy %&gt;% bind_cols(df_test)\nres &lt;- df_test_xypredy %&gt;% yardstick::rmse(truth = arr_delay, estimate = .pred)\ncat(\"単体モデルのRMSE:\", res$.estimate[[1]],\"\\n\")\n\n単体モデルのRMSE: 17.72646 \n\n##ブートストラップ法の10個のモデルの予測平均値の場合\ndf_test_predy_b &lt;- tibble(.pred = rowMeans(do.call(cbind, df_test_predy_tmp))) #予測値の平均値を計算\ndf_test_xypredy_b &lt;- df_test_predy_b %&gt;% bind_cols(df_test)\nres &lt;- df_test_xypredy_b %&gt;% yardstick::rmse(truth = arr_delay, estimate = .pred)\ncat(\"10モデル平均のRMSE:\", res$.estimate[[1]],\"\\n\")\n\n10モデル平均のRMSE: 17.30855 \n\n\n今回の例ではブートストラップ法の平均のほうが勝ったようです。",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>rsample</span>"
    ]
  },
  {
    "objectID": "articles/rsample.html#時系列データへの対応",
    "href": "articles/rsample.html#時系列データへの対応",
    "title": "rsample",
    "section": "時系列データへの対応",
    "text": "時系列データへの対応\n今回のデータにおいては出発した日付が重要な特徴量となっていました。\nホリデーシーズンのように日付からあらかじめ予測できる事象であればともかく、 天候の悪化のような事象は将来の日付だけで特定できるものではなく、 これを前提に学習してしまうと、将来予測するためのモデルとしては使用できないと考えられます。\nこのような時系列データをもとに将来予測を行う場合は時刻によってデータを分割し、 過去分を学習データ、将来分を評価データとするのが適切でしょう。\n\ninitial_time_split関数の使い方\ninitial_split関数の代わりにinitial_time_split関数を用いることで、このようなデータ分割を行うことができます。\n\nset.seed(2024)\ntime_split &lt;- initial_time_split(df_all, prop = 0.8)\ntime_split\n\n&lt;Training/Testing/Total&gt;\n&lt;269420/67356/336776&gt;\n\n\n\ntime_split %&gt;% training() %&gt;% head()\n\n# A tibble: 6 × 16\n   year month   day dep_time sched_dep_time dep_delay sched_arr_time arr_delay\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;          &lt;int&gt;     &lt;dbl&gt;\n1  2013     1     1      517            515         2            819        11\n2  2013     1     1      533            529         4            830        20\n3  2013     1     1      542            540         2            850        33\n4  2013     1     1      544            545        -1           1022       -18\n5  2013     1     1      554            600        -6            837       -25\n6  2013     1     1      554            558        -4            728        12\n# ℹ 8 more variables: carrier &lt;fct&gt;, flight &lt;int&gt;, origin &lt;fct&gt;, dest &lt;fct&gt;,\n#   air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;\n\ntime_split %&gt;% testing() %&gt;% head()\n\n# A tibble: 6 × 16\n   year month   day dep_time sched_dep_time dep_delay sched_arr_time arr_delay\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;          &lt;int&gt;     &lt;dbl&gt;\n1  2013     7    21      925            847        38           1145        30\n2  2013     7    21      926            930        -4           1039       -17\n3  2013     7    21      926            930        -4           1230       -26\n4  2013     7    21      927            930        -3           1218       -30\n5  2013     7    21      927            815        72            930        55\n6  2013     7    21      929            925         4           1220         3\n# ℹ 8 more variables: carrier &lt;fct&gt;, flight &lt;int&gt;, origin &lt;fct&gt;, dest &lt;fct&gt;,\n#   air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;\n\n\nなお、この関数はデータから時系列を自動的に判断しているわけではありません。 時系列順にデータをソートしてから分割するようにしてください。\n\n\nsliding_period関数の使い方\n時系列データ特有のリサンプリング手法としてスライディングウィンドウ法があります。 分析セット期間の長さ（例えば30日）と検証セット期間の長さ（例えば10日）を決めておいて、 長さを一定のままデータ抽出する範囲（ウィンドウ）をスライドしていくものです。\nrsampleパッケージのsliding_period関数等で実現することが出来ます。\n\n#sliding_period関数を使用する際は時刻のデータをDate型やPOSIXct型で用意しておく必要がある\ndf_small_date &lt;- df_small %&gt;%\n  mutate(date = as.Date(paste(year, month, day, sep = \"-\"))) %&gt;%\n  arrange(date) #時系列順にデータが並んでいる必要があるためソートしておく\n                                     \nsliding &lt;- sliding_period(\n  data = df_small_date,\n  index = date, #データを区切るのに用いる時刻が入っている列名\n  every = 10, #10日単位でデータを区切る\n  period = \"day\",\n  origin = min(df_small_date$date), #データを区切るときの基準点\n  lookback = 2, #10×(1+2)=30日分を分析セットとする\n  assess_start = 4,#分析データの末日から、31～50日後を検証セットとする\n  assess_stop = 5,\n  step = 2, #データ抽出範囲を20日ずつずらしていく\n)\n\nfor(i in 1:5){\n  cat(sliding$id[[i]], \"分析セットの日付範囲:\", as.character(min(analysis(sliding$splits[[i]])$date)), \n      \"～\", as.character(max(analysis(sliding$splits[[i]])$date)),\n      \"検証セットの日付範囲:\", as.character(min(assessment(sliding$splits[[i]])$date)), \n      \"～\", as.character(max(assessment(sliding$splits[[i]])$date)), \"\\n\")\n}\n\nSlice01 分析セットの日付範囲: 2013-01-01 ～ 2013-01-30 検証セットの日付範囲: 2013-03-02 ～ 2013-03-21 \nSlice02 分析セットの日付範囲: 2013-01-21 ～ 2013-02-19 検証セットの日付範囲: 2013-03-22 ～ 2013-04-10 \nSlice03 分析セットの日付範囲: 2013-02-10 ～ 2013-03-11 検証セットの日付範囲: 2013-04-11 ～ 2013-04-30 \nSlice04 分析セットの日付範囲: 2013-03-02 ～ 2013-03-31 検証セットの日付範囲: 2013-05-01 ～ 2013-05-20 \nSlice05 分析セットの日付範囲: 2013-03-22 ～ 2013-04-20 検証セットの日付範囲: 2013-05-21 ～ 2013-06-09 \n\n\n他に様々な関数や引数が用意されていますが、その詳細は Hannah Frick et al. (2022b) を参照してください。",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>rsample</span>"
    ]
  },
  {
    "objectID": "articles/rsample.html#引数strataによる層化抽出",
    "href": "articles/rsample.html#引数strataによる層化抽出",
    "title": "rsample",
    "section": "引数strataによる層化抽出",
    "text": "引数strataによる層化抽出\n学習データと評価データの分割の際に無作為抽出した場合、 小さなカテゴリがある場合等の偏りのあるデータでは、両者でその分布が異なってしまうことがあります。\n例えばfactor型変数carrierを確認してみると、 学習データと評価データで若干ながら分布が異なることがわかります。\n\ntb_train &lt;- table(df_train$carrier)\ntb_test &lt;- table(df_test$carrier)\ntibble(carrier := names(tb_train), dist_train := as.double(tb_train), dist_test := as.double(tb_test)) %&gt;% head()\n\n# A tibble: 6 × 3\n  carrier dist_train dist_test\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 9E             325        35\n2 AA             602        73\n3 AS              17         1\n4 B6             971        98\n5 DL             807       105\n6 EV             998       103\n\n\n引数strataに列名を指定することで、その列ごとにデータを分けてから（指定した割合で）データ分割を行うようになります。 このような抽出方法を層化抽出といいます。\n\nset.seed(2024)\nsplit_df_small_strata &lt;- initial_split(df_small, prop = 0.9, strata = carrier)\ndf_train_strata &lt;- training(split_df_small_strata)\ndf_test_strata &lt;- testing(split_df_small_strata)\n\ntb_train_strata &lt;- table(df_train_strata$carrier)\ntb_test_strata &lt;- table(df_test_strata$carrier)\ntibble(carrier := names(tb_train_strata), dist_train := as.double(tb_train_strata), dist_test := as.double(tb_test_strata)) %&gt;% head()\n\n# A tibble: 6 × 3\n  carrier dist_train dist_test\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 9E             326        34\n2 AA             607        68\n3 AS              17         1\n4 B6             959       110\n5 DL             822        90\n6 EV             987       114\n\n\nなお、非常に小さなカテゴリは他のカテゴリと統合して取り扱われます。 その判定は引数poolで指定でき、そのデフォルト値は0.1となっています。\nまた、今回はfactor型変数による層化抽出を行いましたが、数値型変数も指定可能です。 この場合、分位点でいくつかのビンに分割して行われます。その分割数は引数breaksで指定できます。",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>rsample</span>"
    ]
  },
  {
    "objectID": "articles/rsample.html#参考文献",
    "href": "articles/rsample.html#参考文献",
    "title": "rsample",
    "section": "参考文献",
    "text": "参考文献\n\n\nHadley Wickham. 2021. “Nycflights13: Flights That Departed NYC in 2013 / Flights Data.” https://nycflights13.tidyverse.org/reference/flights.html.\n\n\nHannah Frick, Fanny Chow, Max Kuhn, Michael Mahoney, Julia Silge, and Hadley Wickham. 2022a. “General Resampling Infrastructure • Rsample.” https://rsample.tidymodels.org/.\n\n\n———. 2022b. “Time-Based Resampling — Slide-Resampling • Rsample.” https://rsample.tidymodels.org/reference/slide-resampling.html.\n\n\n松村優哉, 瓜生真也, and 吉村広志. 2023. Rユーザのためのtidymodels[実践]入門〜モダンな統計・機械学習モデリングの世界. 技術評論社.",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>rsample</span>"
    ]
  },
  {
    "objectID": "articles/rsample.html#footnotes",
    "href": "articles/rsample.html#footnotes",
    "title": "rsample",
    "section": "",
    "text": "右辺の関数の第1引数に左辺を渡すという演算子で、たとえばa %&gt;% f %&gt;% g(b)という記述はg(f(a),b)と同等です。↩︎\nk-fold cross validationは v-fold cross validationと呼ばれることもあり、vfold_cvの関数名はそこから取られています。↩︎\n以下では同じtidymodelsに含まれるパッケージを使用していますが、 rsampleパッケージを使用する際に必須というわけではなく、あくまでコードを見やすくするために導入しているものです。\ntidymodelsに含まれる他のパッケージについても 合わせて学習されることをお勧めしますが、本稿では詳しい解説は割愛します。↩︎",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>rsample</span>"
    ]
  },
  {
    "objectID": "articles/shapviz.html",
    "href": "articles/shapviz.html",
    "title": "shapviz",
    "section": "",
    "text": "パッケージの概要\nshapvizは、予測モデルの解釈手法の一種であるSHAPの可視化に特化したパッケージです。 treeshapやkernelshap等のSHAPを計算するパッケージと組み合わせて使用します。",
    "crumbs": [
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>shapviz</span>"
    ]
  },
  {
    "objectID": "articles/shapviz.html#パッケージの概要",
    "href": "articles/shapviz.html#パッケージの概要",
    "title": "shapviz",
    "section": "",
    "text": "SHAPとは\nSHAPという手法については データサイエンス関連基礎調査WG　大江麗地 (2024) に解説があるため、こちらを参照することをお勧めします。 以下では詳細な説明は避け、概要のみを記載します。\nSHAP(SHapley Additive exPlanation)とは予測モデルの解釈に用いられる手法の一種で、 ある予測モデルの入力（説明変数）と出力（予測値）の組に対して、 どの説明変数の寄与によってその予測値となったのかを加法的に分解するものです。\n 個別の予測値 = その予測の説明変数1の寄与 + \\cdots + その予測の説明変数Nの寄与 + 予測値平均\nこのようにして分解された各サンプル・説明変数の寄与をSHAP値と呼びます。\n個別サンプルの予測に対する解釈を与える、いわゆるローカルな手法だと考えられますが、 多くのサンプルのSHAPを計算してそれをグラフにする、平均値で要約する等により、 モデル全体の解釈を与える、いわゆるグローバルな手法としても使用することが出来ます。",
    "crumbs": [
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>shapviz</span>"
    ]
  },
  {
    "objectID": "articles/shapviz.html#準備",
    "href": "articles/shapviz.html#準備",
    "title": "shapviz",
    "section": "準備",
    "text": "準備\n\nパッケージの読み込み\n\nlibrary(AER) #データセット\n\nlibrary(tibble) #data.frame拡張版\nlibrary(dplyr) #data.frameの操作\nlibrary(rsample) #データ分割\nlibrary(recipes) #前処理\n\nlibrary(xgboost) #今回使用するモデルのパッケージ\nlibrary(ROCR) #精度評価\nlibrary(treeshap) #SHAPの計算\n\nlibrary(ggplot2) #グラフの描画\nlibrary(patchwork) #複数のgpplotを組み合わせる\nlibrary(shapviz) #SHAPの可視化\n\n\n\nデータセットの読み込み\nChristian Kleiber and Achim Zeileis (2008) で使用されたデータセット等をまとめたパッケージAERに含まれる、 HealthInsuranceというデータセットを使用します。\n性別・年齢・学歴・家族構成・雇用状態（自営業か否か）健康保険の加入状況等に関する 約9,000個のサンプルが含まれています。 今回は、健康保険に加入しているかどうかを予測するモデルを作成することとします。\nデータセットの詳細については Achim Zeileis (2024) を参照してください1。\n\ndata(\"HealthInsurance\")\ndf_all &lt;- HealthInsurance\n\nsummary(df_all)\n\n health          age        limit         gender     insurance  married   \n no : 629   Min.   :18.00   no :7571   female:4169   no :1750   no :3369  \n yes:8173   1st Qu.:30.00   yes:1231   male  :4633   yes:7052   yes:5433  \n            Median :39.00                                                 \n            Mean   :38.94                                                 \n            3rd Qu.:48.00                                                 \n            Max.   :62.00                                                 \n                                                                          \n selfemp        family             region     ethnicity         education   \n no :7731   Min.   : 1.000   northeast:1682   other: 365   none      :1119  \n yes:1071   1st Qu.: 2.000   midwest  :2023   afam :1083   ged       : 374  \n            Median : 3.000   south    :3075   cauc :7354   highschool:4434  \n            Mean   : 3.094   west     :2022                bachelor  :1549  \n            3rd Qu.: 4.000                                 master    : 524  \n            Max.   :14.000                                 phd       : 135  \n                                                           other     : 667  \n\n\n\n\n前処理\n今回例として使用するモデルでは、説明変数が数値型である必要があるので、factor型変数を数値型に変換しておきます2。\n\nrec_init &lt;- df_all %&gt;% recipe(insurance ~ .) %&gt;% #前処理手順の定義\n  #ethinicityは最も多いカテゴリがcaucなので、これを基準カテゴリに変更\n  step_relevel(ethnicity, ref_level = \"cauc\") %&gt;% \n  #educationは学歴を表す説明変数で、大きいほど高学歴であるため、そのままダミー変数にするのではなく、数値に変換\n  step_mutate(education_main = as.numeric(education) - 1) %&gt;%\n  #ただし、最後のカテゴリだけは「その他」を表しているので、これだけは別のダミー変数に分離する\n  step_mutate(education_other = if_else(education_main == 6, 1, 0)) %&gt;%\n  step_mutate(education_main = if_else(education_main &lt; 6, education_main, 0)) %&gt;%\n  step_rm(education) %&gt;%\n  step_dummy(all_factor_predictors()) %&gt;% #他のfactor型変数は単純にダミー変数化\n  step_relevel(insurance, ref_level = \"yes\")\n  #目的変数は健康保険に加入しているかを表すinsurance\n\ndf_baked &lt;- rec_init %&gt;% prep() %&gt;% bake(new_data = NULL) #上記で定義した前処理手順を実際に実行\n\n上記前処理を施したうえで、学習データとテストデータに分割します。\n\nset.seed(2024)\nsplit_df &lt;- rsample::initial_split(df_baked, prop = 0.8) #80%を学習データ、20%をテストデータとする\ndf_train &lt;- rsample::training(split_df)\ndf_test &lt;- rsample::testing(split_df)\n\ndf_train_x &lt;- df_train %&gt;% dplyr::select(-insurance)\ndf_train_y &lt;- df_train$insurance\ndf_test_x &lt;- df_test %&gt;% dplyr::select(-insurance)\ndf_test_y &lt;- df_test$insurance\n\n\n\nモデル構築\n続いてXGBoostによる予測モデルを学習データをもとに構築します。3\n2値分類の問題ですが、予測モデルの出力としては加入しているか否かの2通りではなく、 加入している確率を出力するようにしています。\n\nset.seed(2024)\nmodel_xgboost &lt;- xgboost(data = as.matrix(df_train_x), label = as.matrix(2 - as.numeric(df_train_y)), nrounds = 100,\n                  params = list(eta = 0.3, max_depth = 2, gamma = 0, min_child_weight = 1, \n                             subsample = 1, colsample_bytree = 1, colsample_bynode = 2/14, objective = \"binary:logistic\"),\n                  verbose = 0)\n\n構築した予測モデルの精度をテストデータを用いて確認しておきます。\nここではAUC(ROC)を確認します。これは2値分類モデルで使用される評価指標で、高いほど精度が良いという評価になります。\n\ncalc_score &lt;- function(object, predfun, df_test_x, df_test_y){\n  yhat &lt;- object %&gt;% predfun(df_test_x)\n  pr &lt;- ROCR::prediction(yhat, df_test_y)\n  auc &lt;- pr %&gt;% ROCR::performance(\"auc\")\n  auc_plot &lt;- pr %&gt;% ROCR::performance(\"tpr\", \"fpr\")\n  list(\n    auc_plot = auc_plot,\n    auc = auc@y.values %&gt;% as.numeric()\n    )\n}\n\npredfun_xgboost &lt;- function(object, newdata){\n  dt &lt;- as.matrix(newdata)\n  object %&gt;% predict(newdata = dt)\n}\nscore &lt;- calc_score(model_xgboost, predfun_xgboost, df_test_x, df_test_y)\nscore$auc_plot %&gt;% plot()\n\n\n\n\n\n\n\nscore$auc\n\n[1] 0.7503933\n\n\n0.75は高くもなく低くもないといった程度ではあるものの、用途によってはこれでも十分でしょう。 （例えば True Positive Rate = 0.6, False Positive Rate = 0.2 あたりとなるしきい値をとれば、 　全体の8割程度を占める加入者のうち6割を削減しつつ、少数派の非加入者のうち8割を残した集団が作れる）",
    "crumbs": [
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>shapviz</span>"
    ]
  },
  {
    "objectID": "articles/shapviz.html#基本的な使用方法",
    "href": "articles/shapviz.html#基本的な使用方法",
    "title": "shapviz",
    "section": "基本的な使用方法",
    "text": "基本的な使用方法\nまずは別のパッケージを用いてSHAPを計算します。 ここでは計算が高速なtreeshapを使用します。\n\nset.seed(2024) #SHAPを計算したいサンプル\nnrow_shap &lt;- nrow(df_train) #SHAPは計算コストが高いことが多いが、treeshapであれば全サンプルでも問題ない\ndf_shap &lt;- df_train[sample(nrow(df_train), nrow_shap), ]\ndf_shap_x &lt;- df_shap %&gt;% dplyr::select(-insurance)\n\nt1 &lt;- proc.time()\n\nobj_uni &lt;- treeshap::unify(model_xgboost, df_shap_x)\nshap_ts &lt;- treeshap::treeshap(obj_uni, x = df_shap_x)\n\nt2 &lt;- proc.time()\nt0 &lt;- (t2-t1)[3]\nnames(t0) &lt;- NULL\n\ncat(\"処理時間:\", t0, \"秒\")\n\n処理時間: 0.44 秒\n\n\n次に、shapviz関数でshapvizパッケージで可視化できるオブジェクトに変換します。\n\nsv &lt;- shapviz::shapviz(shap_ts)\n\n最後に、このオブジェクトをshapvizパッケージの関数に入力することで可視化できます。 例えば、個別のサンプルに対する寄与の分解を表示するには次のようにします。\n\nshapviz::sv_waterfall(sv, row_id = 1) #1つ目のサンプルの予測結果に対してプロット\n\n\n\n\n\n\n\n\nこのサンプルでは、自営業であること（selfemp_yes=1）や独身である（married_yes=0）ことによって、 平均的な被験者よりも健康保険に加入しない傾向にあると判断されたようです。\n上記sv_waterfallの他にも可視化を行う関数が色々用意されていますが、基本的な使用方法は同様です。",
    "crumbs": [
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>shapviz</span>"
    ]
  },
  {
    "objectID": "articles/shapviz.html#可視化方法一覧",
    "href": "articles/shapviz.html#可視化方法一覧",
    "title": "shapviz",
    "section": "可視化方法一覧",
    "text": "可視化方法一覧\n\nSHAP Summary Plot\nグローバルな手法として全サンプルの結果を一覧に表示し、 説明変数ごとに全般的にどの程度寄与しているかをプロットするには次のようにします。\n\nshapviz::sv_importance(sv, kind = \"beeswarm\")\n\n\n\n\n\n\n\n\n横軸は寄与の大きさを、色付けは説明変数の値を示しており、 例えば明るい色の点が右側にある場合は、その説明変数が高いほど予測確率が高くなることを示します。\nデフォルトでは寄与が大きい説明変数から順に並べられるので、 最も予測確率への寄与が大きい説明変数は学歴（education_main）であることがわかります。 また、学歴が高いほど健康保険に加入する傾向があることがわかります。\n\n\nSHAP Feature Importance Plot\nSummary Plotは各サンプルの寄与をすべてプロットしていました。\n寄与の絶対値の大きいサンプルが多い説明変数は重要であると考えられるので、 その平均値を棒グラフにして描画することで、どれが重要な説明変数なのかが一目でわかるようになります。\nこのようなプロットをFeature Importance Plot（特徴量重要度プロット）といい、プロットするには次のようにします。\n\nshapviz::sv_importance(sv)\n\n\n\n\n\n\n\n\n\n\nSHAP Dependence Plot\nここまでは説明変数の寄与の大きさを比較することを主目的としていました。 各説明変数に着目して、それがどのように寄与しているかはSummary Plotでも確認可能ですが、 横軸に説明変数の値、縦軸に寄与としたグラフをプロットすることも考えられます。\nこれをDependence Plotといい、プロットするにはsv_dependence関数を使用します。\n\nshapviz::sv_dependence(sv, v = c(\"education_main\", \"age\"), color_var = NULL)\n\n\n\n\n\n\n\n\n学歴がおおむね線形に影響を及ぼすことはSummary Plotでも大まかには確認できましたが、このようなグラフにすることでより明確になりました。 また、年齢が与える影響は非常に複雑で、20代前半で一度加入率が落ち込み、以降は少しずつ上がっていくという推移になることがわかります。\nこのプロットでは横軸が同じでも、縦軸（寄与）が異なる点が多数描かれています。 これはサンプルによって寄与が異なるためで、交互作用がある場合にこのような現象が発生します。\n引数color_varを省略するか\"auto\"とすることで、交互作用の大きい説明変数を自動で選び、その値によってグラフが色分けされるようになります。\n\nshapviz::sv_dependence(sv, v = c(\"education_main\", \"age\"), color_var = \"auto\")\n\n\n\n\n\n\n\n\n\n学歴（education_main）と交互作用のある説明変数としては婚姻状況（married_yes）が選ばれました。 高卒・大卒（education_mainが1～3）では結婚している場合は若干SHAP値が上がる一方、それ以外の場合はその逆となるようです。 ややこしい話ですが、婚姻状況（married_yes）のSHAP値が、結婚している場合に正であるという状況はどのサンプルでも変わらないので、 結婚による加入率上昇はどの学歴層でもみられるものの、その効果は学歴によって差があるということになります。\n年齢（age）と交互作用のある説明変数としては、居住地域が西側であるかどうか（region_west）が選ばれました。 全体的な形はあまり変わらないように思われますが、居住地域が西側の場合、30歳前後での加入率が下がるようです。\n\n引数color_varに説明変数名を指定することで、好きな特徴量で色分けをすることができます。\n\nshapviz::sv_dependence(sv, v = \"age\", color_var = \"married_yes\")\n\n\n\n\n\n\n\n\n20代前半では結婚している（married_yes=1）と健康保険への加入率が低いように見える一方、それより上の年代では大きく差はないように見えます。\n\n\n2D SHAP Dependence Plot\n前述のSHAP Dependence Plotでは2つの説明変数の関係を色分けで示していましたが、 別の方法として縦軸と横軸に説明変数をとる方法もあります。使用する関数はsv_dependence2D関数です。 ここで色分けに使用されるSHAP値は2変数のSHAP値の合計になります。\n\nshapviz::sv_dependence2D(sv, x = \"education_main\", y = \"married_yes\")\n\n\n\n\n\n\n\n\n横軸（学歴）をどこにとっても、下側（独身）よりも上側（既婚）のほうが色が明るく、 前述した、「結婚による加入率上昇はどの学歴層でもみられる」という状況をうまく可視化することが出来ました。\nなお、説明変数が地理データである場合には、 例えば横軸に経度、縦軸に緯度を取るような使い方もあります(Michael Mayer and Adrian Stando 2024a)。\n\n\nSHAP Waterfall Plot\n最初に例として挙げた、個別のサンプルに対する寄与の分解4をWaterfall Plotといいます。\n\nshapviz::sv_waterfall(sv, row_id = 1) #1つ目のサンプルの予測結果に対してプロット\n\n\n\n\n\n\n\n\n\n\nSHAP Force Plot\n個別サンプルに対する寄与の分解についてはもう一つプロットが用意されており、 それが左側に正方向の寄与、右側に負方向の寄与を一次元的に並べるForce Plotです。\n\nshapviz::sv_force(sv, row_id = 1) #1つ目のサンプルの予測結果に対してプロット",
    "crumbs": [
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>shapviz</span>"
    ]
  },
  {
    "objectID": "articles/shapviz.html#発展的な話題",
    "href": "articles/shapviz.html#発展的な話題",
    "title": "shapviz",
    "section": "発展的な話題",
    "text": "発展的な話題\n\n対応パッケージ一覧\n今回はtreeshapパッケージで計算したSHAPを使用しましたが、 shapvizパッケージが取り扱えるパッケージは他にもあります。\n2024年8月時点で取り扱えるパッケージ・オブジェクトの一覧は次のとおり。\n\n\n\nパッケージ名\nクラス名\n\n\n\n\nxgboost5\nxgb.Booster\n\n\nlightgbm\nlgb.Booster\n\n\nh2o\nH2ORegressionModel\n\n\nh2o\nH2OBinomialModel\n\n\nh2o\nH2OModel\n\n\nfastshap\nexplain\n\n\ntreeshap\ntreeshap\n\n\nshapr\nexplain\n\n\nkernelshap\nkernelshap\n\n\nkernelshap\npermshap\n\n\nDALEX\npredict_parts\n\n\n\nいずれもshapviz::shapviz関数で取り扱うことが出来ますが、引数には多少の差異があります。 詳細は Michael Mayer and Adrian Stando (2024c) を参照してください。\nなお、Michael Mayer and Adrian Stando (2024c) にはSHAP値を格納した一般的なオブジェクトを shapvizパッケージに対応させる方法についても記載があります。\n\n\n見た目の調節\nshapvizパッケージの関数には見た目の調節を行うための引数がいくつか存在します。\nまた、内部的にggplot2パッケージが使用されていることから、 ggplot2の関数を用いた見た目の調節も可能です。\n以下、例を示します6。\n\nshapviz::sv_importance(sv, kind = \"beeswarm\",\n                       max_display = 6, #6変数まで表示\n                       show_numbers = TRUE, #SHAP Feature Importanceを印字\n                       viridis_args = list(begin = 0.1, end = 0.9, option = \"plasma\")) + #色の設定\n  ggplot2::theme_light()#ggplotのテーマの設定\n\n\n\n\n\n\n\n\n\nshapviz::sv_waterfall(sv, row_id = 1,\n                      max_display = 14, #14変数まで表示\n                      order_fun = function(s) 1:length(s), #並べ方を決める関数…ここでは元の並びを維持\n                      fill_colors = c(\"black\", \"red\"),#正側、負側の色指定\n                      annotation_size = 5) + #下端のE[f(x)]と上端のf(x)の大きさ\n  labs(title = \"SHAP Waterfall Plot\", subtitle = \"row_id = 1\") + #タイトル指定\n  ggplot2::theme(plot.title = element_text(size=16)) #タイトルの大きさ指定\n\n\n\n\n\n\n\n\n詳しくは各関数のドキュメンテーションを参照してください。\n\n\n複数のSHAP値の取り扱い（mshapbizオブジェクト）\nshapviz::shapviz関数で作られるオブジェクトはshapvizオブジェクトと呼ばれますが、 複数のshapvizオブジェクトを内包したmshapvizオブジェクトも存在し、 これもshapvizパッケージの関数でプロットすることが可能です。\nmshapvizオブジェクトを得る方法はいくつかありますが、 たとえばsplit関数でshapvizオブジェクトを分割する方法があります。\n\nsvs &lt;- split(sv, f = df_shap_x$education_main) #education_mainの値ごとに分割\nshapviz::sv_dependence(svs, v = \"age\", color_var = NULL)\n\n\n\n\n\n\n\n\n他には、shapvizオブジェクトをc関数で結合する方法もあります。\nなお、shapvizオブジェクトの分割の際には、 データフレーム等と同じような変数[行, 列]記法を用いることが出来ます。\n\nsvs &lt;- c(northeast = sv[df_shap_x$region_midwest+df_shap_x$region_south+df_shap_x$region_west==0, ],\n         midwest = sv[df_shap_x$region_midwest==1, ],\n         south = sv[df_shap_x$region_south==1, ],\n         west = sv[df_shap_x$region_west==1, ])\nshapviz::sv_dependence(svs, v = \"age\", color_var = NULL)\n\n\n\n\n\n\n\n\n詳しくは Michael Mayer and Adrian Stando (2024b) を参照してください。\n\n\nSHAP交互作用値（SHAP Interaction Plot）\nここまでに挙げたグラフは、すべて予測値の加法的な分解としてのSHAP値を様々な切り口でプロットしたものでした。\nSHAP値は予測値への寄与を各説明変数に割り振ったものでしたが、 これとは別に、（2次の）交互作用への寄与を各2つの説明変数の組に割り振った、SHAP交互作用値(SHAP Interaction Values)というものもあります7。\nSHAP交互作用値が計算できるパッケージは限られており、2024年8月時点ではtreeshapのみです。\n\nt1 &lt;- proc.time()\n#interactionsをTRUEにしておく\nshap_ts &lt;- treeshap::treeshap(obj_uni, x = df_shap_x, interactions = TRUE)\nt2 &lt;- proc.time()\nt0 &lt;- (t2-t1)[3]\nnames(t0) &lt;- NULL\n\ncat(\"処理時間:\", t0, \"秒\")\n\n処理時間: 3.44 秒\n\nsv_i &lt;- shapviz::shapviz(shap_ts)\n\nSummary Plotと同じように、説明変数の組ごとにSHAP交互作用値をプロットするには次のようにします。\n\n#重要なものから3つを選び、3x3のプロットを作成\nshapviz::sv_interaction(sv_i, max_display = 3, kind = \"beeswarm\")\n\n\n\n\n\n\n\n\n（左上から右下に至る）対角線のプロットはSummary Plotと同様に、予測値への寄与が大きさが横軸に示されています。 残るプロットが説明変数の組に対するSHAP交互作用値を横軸にプロットしたものです。\nSHAP値のSummary Plotの場合は、交互作用が「SHAP値＝横軸方向のぶれ」となって表現されていたのに対し、 このプロットではSHAP交互作用値へ分解されているため、「ぶれ」が無くなっていることがわかります。 （たとえばeducation_mainのプロットでは、同じ学歴におけるSHAP値が同一となっている）\nまた、対角線を挟んで右上側にある3つと左下側にある3つについて、対称な位置にあるもの同士のグラフの形は同じですが、色分けが異なります。 例えば1行目・2列目のグラフ（education_main:married_yes）では学歴（education_main）で色分けされており、 最も低い層と高い層で交互作用が大きくなっていることが読み取れます。\nなお、引数kindに\"no\"を与えた場合、Feature Importanceの類似物として、 SHAP交互作用値の絶対値の平均値を、行列の形で得ることが出来ます。 可視化には別の関数が必要です。ここではheatmap関数を用いてみます。\n\nmatrix_i &lt;- shapviz::sv_interaction(sv_i, kind = \"no\")\nmatrix_i %&gt;% heatmap(Rowv = NA, Colv = NA, revC = TRUE, symm = TRUE, margins = c(8,4))\n\n\n\n\n\n\n\n\n今回の例では交互作用があまり無い（XGBoostの予測値は、説明変数ごとに加法的に分解できるモデルとあまり変わらない）ということしか把握できませんでした。 敢えて対角線要素を除いてプロットするには次のようにします。\n\nmatrix_i2 &lt;- matrix_i\ndiag(matrix_i2) &lt;- 0\nmatrix_i2 %&gt;% heatmap(Rowv = NA, Colv = NA, revC = TRUE, symm = TRUE, margins = c(8,4))\n\n\n\n\n\n\n\n\n特定の説明変数の組に着目した分析を行う場合は、 前述のSHAP Dependence Plotで引数interactionsにTRUEを与えてSHAP交互作用値をプロットします。\n\nshapviz::sv_dependence(sv_i, v = \"age\", color_var = \"region_west\", interactions = TRUE)\n\n\n\n\n\n\n\n\n前述したような、居住地域が西側の場合、30歳前後での加入率が下がることが表現されています。 加えて、18歳や60歳以上では逆に加入率が上がるということをも可視化することができました。\nなお、今回のモデルは木の構造が比較的単純なため、2次の交互作用ともなるとグラフもかなり単純なものになっています。\nsv_dependence2D関数版もあります。 縦軸、横軸に説明変数をとり、SHAP交互作用値で色分けをするというもので、 先ほどのグラフから単に縦軸と色分けを逆転させただけです。\n\nshapviz::sv_dependence2D(sv_i, x = \"age\", y = \"region_west\", interactions = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>shapviz</span>"
    ]
  },
  {
    "objectID": "articles/shapviz.html#参考文献",
    "href": "articles/shapviz.html#参考文献",
    "title": "shapviz",
    "section": "参考文献",
    "text": "参考文献\n\n\nAchim Zeileis. 2024. “R: Medical Expenditure Panel Survey Data.” https://search.r-project.org/CRAN/refmans/AER/html/HealthInsurance.html.\n\n\nAgency for Healthcare Research and Quality. n.d. “Medical Expenditure Panel Survey (MEPS) | Agency for Healthcare Research and Quality.” https://www.ahrq.gov/data/meps.html.\n\n\nBob Rudis, Noam Ross, and Simon Garnier. 2024. “Introduction to the Viridis Color Maps • Viridis.” https://sjmgarnier.github.io/viridis/articles/intro-to-viridis.html.\n\n\nChristian Kleiber, and Achim Zeileis. 2008. Applied Econometrics with R. Springer.\n\n\nH. Wickham. 2024. “Viridis Colour Scales from viridisLite — Scale_colour_viridis_d • Ggplot2.” https://ggplot2.tidyverse.org/reference/scale_viridis.html.\n\n\nMichael Mayer, and Adrian Stando. 2024a. “Geographic Components.” https://cran.r-project.org/web/packages/shapviz/vignettes/geographic.html.\n\n\n———. 2024b. “Multiple ‘Shapviz’ Objects.” https://cran.r-project.org/web/packages/shapviz/vignettes/multiple_output.html.\n\n\n———. 2024c. “Using ‘Shapviz’.” https://cran.r-project.org/web/packages/shapviz/vignettes/basic_use.html.\n\n\nデータサイエンス関連基礎調査WG　大江麗地. 2024. “Interpretable Machine Learning.” アクチュアリージャーナル 127 (June): 78–117.",
    "crumbs": [
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>shapviz</span>"
    ]
  },
  {
    "objectID": "articles/shapviz.html#footnotes",
    "href": "articles/shapviz.html#footnotes",
    "title": "shapviz",
    "section": "",
    "text": "元の研究は、自営業者の健康保険加入率の低さと健康状態の関係性を調べたものです。この研究で用いられた、1996年の米国医療費パネル調査(MEPS(Agency for Healthcare Research and Quality, n.d.))から抽出されたものがこのデータセットです。↩︎\nなお、ここではデータ前処理にrecipesパッケージを使用しています。 また、%&gt;%はmagrittrパッケージによるパイプ演算子で、右辺の関数の第1引数に左辺を渡すという働きがあります。 たとえばa %&gt;% f %&gt;% g(b)という記述はg(f(a),b)と同等です。↩︎\nハイパーパラメータは事前にチューニングしたものを入力しています。 チューニングの過程については本稿の主題を外れるので、割愛します。↩︎\nこのプロットは下端に予測値の平均値がまず現れ、SHAP値（寄与）を順番に足していくと上端の予測値になるという構成になっています。 しかし、下端の予測値の平均値は、treeshapパッケージを用いる場合はデフォルトでは必ずゼロが表示される仕様です。 何かしらの数値を表示したい場合は、treeshap関数の引数baselineにその平均値を与える必要があります。 加えて、今回は2値分類モデルを構築した（学習時にobjective = \"binary:logistic\"と指定した）都合上、 SHAP値は確率予測値を分解したものではなく、確率予測値をロジット変換したものの分解になっています。 このため、上端に表示された予測値は確率予測値そのものにはなっていません。↩︎\n実はXGBoostのモデルは、treeshapパッケージを明示的に用いずとも直接shapviz::shapviz関数を使用することが出来ます。 本稿では一般的な用法と同じ流れとなるよう、明示的にtreeshapパッケージを用いる形としました。↩︎\n引数viridis_argsは、ggplot2::scale_colour_viridis_cに引き渡される引数をリストで指定するものです。 使用方法の詳細は H. Wickham (2024) や Bob Rudis, Noam Ross, and Simon Garnier (2024) を参照してください。↩︎\nSHAP値は「その説明変数が入力されていない場合とされた場合の予測値の差」の加重平均として計算します。これを、説明変数X, Yの組に対して「X,Y両方入力された場合 - Xが入力されてYが入力されない場合 - Yが入力されてXが入力されない場合 + X,Y両方入力されない場合」の加重平均と置き換えたものがSHAP交互作用値です。↩︎",
    "crumbs": [
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>shapviz</span>"
    ]
  },
  {
    "objectID": "articles/skimr.html",
    "href": "articles/skimr.html",
    "title": "skimr",
    "section": "",
    "text": "パッケージの概要\nskimr パッケージはデータフレームの要約に特化したパッケージです。skimr() 関数を用いることで、R の標準関数である summary() よりも詳細かつ視認性の高い要約を得ることができます。",
    "crumbs": [
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>skimr</span>"
    ]
  },
  {
    "objectID": "articles/skimr.html#データフレームを要約する",
    "href": "articles/skimr.html#データフレームを要約する",
    "title": "skimr",
    "section": "データフレームを要約する",
    "text": "データフレームを要約する\nskim() 関数をデータフレームに適用すると、特徴量ごとに欠損値の数（n_missing）、非欠損値の割合（complete_rate）、数値型（numeric）の特徴量の平均および標準偏差、因子型（factor）の特徴量の最頻水準や水準数などの要約が出力されます。特に、数値変数について簡易的なヒストグラムが出力される点が特徴的です。\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.5.1\n\nskim(iris)\n\n\nData summary\n\n\nName\niris\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSpecies\n0\n1\nFALSE\n3\nset: 50, ver: 50, vir: 50\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSepal.Length\n0\n1\n5.84\n0.83\n4.3\n5.1\n5.80\n6.4\n7.9\n▆▇▇▅▂\n\n\nSepal.Width\n0\n1\n3.06\n0.44\n2.0\n2.8\n3.00\n3.3\n4.4\n▁▆▇▂▁\n\n\nPetal.Length\n0\n1\n3.76\n1.77\n1.0\n1.6\n4.35\n5.1\n6.9\n▇▁▆▇▂\n\n\nPetal.Width\n0\n1\n1.20\n0.76\n0.1\n0.3\n1.30\n1.8\n2.5\n▇▁▇▅▃",
    "crumbs": [
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>skimr</span>"
    ]
  },
  {
    "objectID": "articles/skimr.html#要約結果を加工抽出する",
    "href": "articles/skimr.html#要約結果を加工抽出する",
    "title": "skimr",
    "section": "要約結果を加工・抽出する",
    "text": "要約結果を加工・抽出する\nskim() 関数の返り値は、“skim_df” という特別なクラスを持つデータフレームです。\n\nclass(iris_skim &lt;- skim(iris))\n\n[1] \"skim_df\"    \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nskimr には “skim_df” オブジェクトを加工するための関数が用意されています。たとえば、yank() 関数を用いることで特定のデータ型（skim_type）に関する情報を取り出すことができます。\n\n# 数値型特徴量を取り出す\niris_skim |&gt; yank(\"numeric\")\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSepal.Length\n0\n1\n5.84\n0.83\n4.3\n5.1\n5.80\n6.4\n7.9\n▆▇▇▅▂\n\n\nSepal.Width\n0\n1\n3.06\n0.44\n2.0\n2.8\n3.00\n3.3\n4.4\n▁▆▇▂▁\n\n\nPetal.Length\n0\n1\n3.76\n1.77\n1.0\n1.6\n4.35\n5.1\n6.9\n▇▁▆▇▂\n\n\nPetal.Width\n0\n1\n1.20\n0.76\n0.1\n0.3\n1.30\n1.8\n2.5\n▇▁▇▅▃\n\n\n\n\n\nまた、focus() 関数を用いることで、特定の列を取り出すことができます。\n\n# 一部の要約を選択したのち因子型特徴量を取り出す\niris_skim |&gt; focus(factor.n_unique, factor.ordered) |&gt; yank(\"factor\")\n\nVariable type: factor\n\n\n\nskim_variable\nn_unique\nordered\n\n\n\n\nSpecies\n3\nFALSE",
    "crumbs": [
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>skimr</span>"
    ]
  },
  {
    "objectID": "articles/skimr.html#グループ化されたデータフレームを要約する",
    "href": "articles/skimr.html#グループ化されたデータフレームを要約する",
    "title": "skimr",
    "section": "グループ化されたデータフレームを要約する",
    "text": "グループ化されたデータフレームを要約する\nskim() 関数は group_by() でグループ化されたデータフレームにも対応しており、各特徴量に関するグループごとの集計結果を出力することができます。\n\niris |&gt; dplyr::group_by(Species) |&gt; skim()\n\n\nData summary\n\n\nName\ndplyr::group_by(iris, Spe…\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nSpecies\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nSpecies\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSepal.Length\nsetosa\n0\n1\n5.01\n0.35\n4.3\n4.80\n5.00\n5.20\n5.8\n▃▃▇▅▁\n\n\nSepal.Length\nversicolor\n0\n1\n5.94\n0.52\n4.9\n5.60\n5.90\n6.30\n7.0\n▂▇▆▃▃\n\n\nSepal.Length\nvirginica\n0\n1\n6.59\n0.64\n4.9\n6.23\n6.50\n6.90\n7.9\n▁▃▇▃▂\n\n\nSepal.Width\nsetosa\n0\n1\n3.43\n0.38\n2.3\n3.20\n3.40\n3.68\n4.4\n▁▃▇▅▂\n\n\nSepal.Width\nversicolor\n0\n1\n2.77\n0.31\n2.0\n2.52\n2.80\n3.00\n3.4\n▁▅▆▇▂\n\n\nSepal.Width\nvirginica\n0\n1\n2.97\n0.32\n2.2\n2.80\n3.00\n3.18\n3.8\n▂▆▇▅▁\n\n\nPetal.Length\nsetosa\n0\n1\n1.46\n0.17\n1.0\n1.40\n1.50\n1.58\n1.9\n▁▃▇▃▁\n\n\nPetal.Length\nversicolor\n0\n1\n4.26\n0.47\n3.0\n4.00\n4.35\n4.60\n5.1\n▂▂▇▇▆\n\n\nPetal.Length\nvirginica\n0\n1\n5.55\n0.55\n4.5\n5.10\n5.55\n5.88\n6.9\n▃▇▇▃▂\n\n\nPetal.Width\nsetosa\n0\n1\n0.25\n0.11\n0.1\n0.20\n0.20\n0.30\n0.6\n▇▂▂▁▁\n\n\nPetal.Width\nversicolor\n0\n1\n1.33\n0.20\n1.0\n1.20\n1.30\n1.50\n1.8\n▅▇▃▆▁\n\n\nPetal.Width\nvirginica\n0\n1\n2.03\n0.27\n1.4\n1.80\n2.00\n2.30\n2.5\n▂▇▆▅▇",
    "crumbs": [
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>skimr</span>"
    ]
  },
  {
    "objectID": "articles/skimr.html#データフレーム以外のオブジェクトを要約する",
    "href": "articles/skimr.html#データフレーム以外のオブジェクトを要約する",
    "title": "skimr",
    "section": "データフレーム以外のオブジェクトを要約する",
    "text": "データフレーム以外のオブジェクトを要約する\nskimr はデータフレームの要約を効率的に行うことを目的として設計されていますが、ベクトル、行列、時系列データなど、データフレームに変換することが可能な他のデータ型のオブジェクトに対しても使うことができます。\n\n# integer型ベクトルのスキミング\nskim(1:100)\n\n\nData summary\n\n\nName\n1:100\n\n\nNumber of rows\n100\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n0\n1\n50.5\n29.01\n1\n25.75\n50.5\n75.25\n100\n▇▇▇▇▇\n\n\n\n\n# 行列のスキミング\nskim(matrix(1:9, 3, 3))\n\n\nData summary\n\n\nName\nmatrix(1:9, 3, 3)\n\n\nNumber of rows\n3\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nV1\n0\n1\n2\n1\n1\n1.5\n2\n2.5\n3\n▇▁▇▁▇\n\n\nV2\n0\n1\n5\n1\n4\n4.5\n5\n5.5\n6\n▇▁▇▁▇\n\n\nV3\n0\n1\n8\n1\n7\n7.5\n8\n8.5\n9\n▇▁▇▁▇\n\n\n\n\n# 時系列データのスキミング\nclass(Nile) # ts\n\n[1] \"ts\"\n\nskim(Nile)\n\n\nData summary\n\n\nName\nNile\n\n\nNumber of rows\n100\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nts\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: ts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nstart\nend\nfrequency\ndeltat\nmean\nsd\nmin\nmax\nmedian\nline_graph\n\n\n\n\nx\n0\n1\n1871\n1970\n1\n1\n919.35\n169.23\n456\n1370\n893.5\n⢁⠊⢂⠊⢄⣀⠔⢄",
    "crumbs": [
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>skimr</span>"
    ]
  },
  {
    "objectID": "articles/skimr.html#要約関数を自作する",
    "href": "articles/skimr.html#要約関数を自作する",
    "title": "skimr",
    "section": "要約関数を自作する",
    "text": "要約関数を自作する\nskim_with() 関数を用いることで、要約関数を自作することも可能です。詳しい使い方はパッケージの Vignette をご参照ください。\n\nmy_skim &lt;- skim_with(numeric = sfl(n = length, sum, var))\niris |&gt;\n  dplyr::group_by(Species) |&gt;\n  my_skim() |&gt;\n  yank(\"numeric\") |&gt;\n  dplyr::select(skim_variable, Species, hist, n, sum, var)\n\nVariable type: numeric\n\n\n\nskim_variable\nSpecies\nhist\nn\nsum\nvar\n\n\n\n\nSepal.Length\nsetosa\n▃▃▇▅▁\n50\n250.3\n0.12\n\n\nSepal.Length\nversicolor\n▂▇▆▃▃\n50\n296.8\n0.27\n\n\nSepal.Length\nvirginica\n▁▃▇▃▂\n50\n329.4\n0.40\n\n\nSepal.Width\nsetosa\n▁▃▇▅▂\n50\n171.4\n0.14\n\n\nSepal.Width\nversicolor\n▁▅▆▇▂\n50\n138.5\n0.10\n\n\nSepal.Width\nvirginica\n▂▆▇▅▁\n50\n148.7\n0.10\n\n\nPetal.Length\nsetosa\n▁▃▇▃▁\n50\n73.1\n0.03\n\n\nPetal.Length\nversicolor\n▂▂▇▇▆\n50\n213.0\n0.22\n\n\nPetal.Length\nvirginica\n▃▇▇▃▂\n50\n277.6\n0.30\n\n\nPetal.Width\nsetosa\n▇▂▂▁▁\n50\n12.3\n0.01\n\n\nPetal.Width\nversicolor\n▅▇▃▆▁\n50\n66.3\n0.04\n\n\nPetal.Width\nvirginica\n▂▇▆▅▇\n50\n101.3\n0.08",
    "crumbs": [
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>skimr</span>"
    ]
  },
  {
    "objectID": "articles/stringr.html",
    "href": "articles/stringr.html",
    "title": "stringr",
    "section": "",
    "text": "パッケージの概要\nstringrは、文字列を操作するための関数群を提供するパッケージです。また、このパッケージは、Rでモダンな分析環境を構築するためのパッケージ群である「tidyverse」に含まれています。\nlibrary(stringr)\nlibrary(dplyr) #パイプ処理の例に利用\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>stringr</span>"
    ]
  },
  {
    "objectID": "articles/stringr.html#r標準の関数との比較",
    "href": "articles/stringr.html#r標準の関数との比較",
    "title": "stringr",
    "section": "R標準の関数との比較",
    "text": "R標準の関数との比較\nR標準の関数と比較すると、stringrパッケージの関数は、基本的には関数の名称がstrから始まり、最初の引数は文字列になるなど、命名規則・引数の取扱いに一貫性があります。\n\n# 比較するベクトルを作成\nx &lt;- c(\"apple\", \"banana\", NA, \"\", \"あいう\") \n\n# strから始まる関数がstringrパッケージの関数\n# 文字数の確認\nstr_length(x)\n\n[1]  5  6 NA  0  3\n\nnchar(x)\n\n[1]  5  6 NA  0  3\n\n# 小文字（a）を大文字（A）に変換\nstr_replace_all(x, \"a\", \"A\")\n\n[1] \"Apple\"  \"bAnAnA\" NA       \"\"       \"あいう\"\n\ngsub(\"a\", \"A\", x)\n\n[1] \"Apple\"  \"bAnAnA\" NA       \"\"       \"あいう\"\n\n\nまた、stringrパッケージはtidyverseに含まれるパッケージであるため、パイプ処理（%&gt;%）と一緒に利用しやすくなっています。\n\n# 比較するベクトルを作成\nx &lt;- c(\"apple\", \"banana\", NA, \"\", \"あいう\") \n\n# パイプ処理の例（R標準）\nx %&gt;%\n  { gsub(\"a\", \"A\", .) }\n\n[1] \"Apple\"  \"bAnAnA\" NA       \"\"       \"あいう\"\n\n# パイプ処理の例(stringr)\n# 最初の引数が文字列となるため、自然な記載が可能\nx %&gt;%\n  str_replace_all(\"a\", \"A\")\n\n[1] \"Apple\"  \"bAnAnA\" NA       \"\"       \"あいう\"",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>stringr</span>"
    ]
  },
  {
    "objectID": "articles/stringr.html#関数の紹介",
    "href": "articles/stringr.html#関数の紹介",
    "title": "stringr",
    "section": "関数の紹介",
    "text": "関数の紹介\nstringrパッケージの関数の実行例をいくつか紹介します。\n\n# 文字列の一部を抽出\nstr_sub(\"abcdef\", 2, 4)\n\n[1] \"bcd\"\n\n# 文字列の連結\nstr_c(\"a\", \"b\")\n\n[1] \"ab\"\n\n# 指定したパターンに合致するかを判定。正規表現も利用可能\nX &lt;- c(\"apple\", \"banana\")\nstr_detect(X, \"a\")  # 文字列にaが含まれるかどうかを判定\n\n[1] TRUE TRUE\n\nstr_detect(X, \"^a\") # 文字列がaから始まるかどうかを判定\n\n[1]  TRUE FALSE\n\n# 指定したパターンに合致する最初の文字を置換\nstr_replace(\"apple\", \"p\", \"P\")\n\n[1] \"aPple\"\n\n# 指定した区切り文字で分割\nstr_split(\"a,b,c\", \",\")\n\n[[1]]\n[1] \"a\" \"b\" \"c\"\n\n# 文字列の先頭・末尾の空白を削除\nstr_trim(\" hello world \")\n\n[1] \"hello world\"\n\n# 大文字に変換\nstr_to_upper(\"abc\")\n\n[1] \"ABC\"",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>stringr</span>"
    ]
  },
  {
    "objectID": "articles/survival.html",
    "href": "articles/survival.html",
    "title": "survival",
    "section": "",
    "text": "パッケージの概要\nsurvival パッケージはRで生存時間解析を行うためのパッケージで、生存関数やハザード関数に関する推定・検定などが実装されています。",
    "crumbs": [
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>survival</span>"
    ]
  },
  {
    "objectID": "articles/survival.html#生存時間解析用データの作成",
    "href": "articles/survival.html#生存時間解析用データの作成",
    "title": "survival",
    "section": "生存時間解析用データの作成",
    "text": "生存時間解析用データの作成\nsurvivalパッケージでは、Surv() 関数を用いて生存時間解析用のデータセット（Survオブジェクト）を作成することができます。Survオブジェクトは、生存時間（観察時間）と打ち切り指標を表すベクトルの組をもとにして作成されます。\n個人 i=1,2,... について、死亡や要介護状態への移行などの注目するイベント（死亡等）による集団からの脱退と、注目しないイベントによる脱退や観察の終了（打ち切り）が発生するとします。死亡等の発生時間を X_i 、打ち切りの発生時間を C_i とすると、いずれか早い方の発生時間 T_i = min(X_i, C_i) だけが観測されます。生存時間解析においては、死亡等が観察されたか打ち切られたかを表す変数を D_i として、\\{(T_i, D_i)\\}_{i=1,2,...} を収集したデータセットが分析対象とされます。\n\nlibrary(survival)\nx &lt;- c(1, 2, 3, 3, 4, 5, 5)\nc &lt;- c(4, 1, 2, 4, 4, 6, 2)\nt &lt;- pmin(x, c)\nd &lt;- t == x\nSurv(time = t, event = d)\n\n[1] 1  1+ 2+ 3  4  5  2+\n\n# + は打ち切られたことを示す。",
    "crumbs": [
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>survival</span>"
    ]
  },
  {
    "objectID": "articles/survival.html#ノンパラメトリック推定",
    "href": "articles/survival.html#ノンパラメトリック推定",
    "title": "survival",
    "section": "ノンパラメトリック推定",
    "text": "ノンパラメトリック推定\nsurvfit() 関数を用いると、Surv オブジェクトに基づいて生存関数 S(t) に関するKaplan-Meier推定を行うことができます。Kaplan-Meier推定法では、S(t) を下式で推定します。\n\\hat{S}(t)=\\Pi_{i:\\ t_i{\\lt}t}(1-n.event_i/n.risk_i)\n\nsummary(sample.sf &lt;- survfit(Surv(t, d) ~ 1))\n\nCall: survfit(formula = Surv(t, d) ~ 1)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1      7       1    0.857   0.132       0.6334            1\n    3      3       1    0.571   0.249       0.2429            1\n    4      2       1    0.286   0.237       0.0561            1\n    5      1       1    0.000     NaN           NA           NA\n\nplot(sample.sf)\n\n\n\n\n\n\n\n# 累積ハザード関数の Nelson-Aalen 推定量\ndata.frame(sample.sf$time,\n           cumsum(sample.sf$n.event / sample.sf$n.risk))\n\n  sample.sf.time cumsum.sample.sf.n.event.sample.sf.n.risk.\n1              1                                  0.1428571\n2              2                                  0.1428571\n3              3                                  0.4761905\n4              4                                  0.9761905\n5              5                                  1.9761905\n\n\n処置の有無などを表す特徴量によって2群に分けられるときは、formula の ~ の右側にその特徴量を指定することで、生存関数の推定を群ごとに行うことができます。\n\ndata(gehan, package = \"MASS\")\nstr(gehan)\n\n'data.frame':   42 obs. of  4 variables:\n $ pair : int  1 1 2 2 3 3 4 4 5 5 ...\n $ time : int  1 10 22 7 3 32 12 23 8 22 ...\n $ cens : int  1 1 1 1 1 0 1 1 1 1 ...\n $ treat: Factor w/ 2 levels \"6-MP\",\"control\": 2 1 2 1 2 1 2 1 2 1 ...\n\nplot(survfit(Surv(time, cens) ~ treat, data = gehan),\n     lty = 1:2, xlab = \"weeks\", ylab = \"survival\")\nlegend(26, 1, levels(gehan$treat), lty = 1:2)\n\n\n\n\n\n\n\n\n2群の差についての統計的検定として、survdiff() 関数を用いてログランク検定を行うことができます。ログランク検定では、生存関数が同じであるという帰無仮説がデータによってテストされます。\n\nsurvdiff(Surv(time, cens) ~ treat, data = gehan)\n\nCall:\nsurvdiff(formula = Surv(time, cens) ~ treat, data = gehan)\n\n               N Observed Expected (O-E)^2/E (O-E)^2/V\ntreat=6-MP    21        9     19.3      5.46      16.8\ntreat=control 21       21     10.7      9.77      16.8\n\n Chisq= 16.8  on 1 degrees of freedom, p= 4e-05",
    "crumbs": [
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>survival</span>"
    ]
  },
  {
    "objectID": "articles/survival.html#パラメトリック推定",
    "href": "articles/survival.html#パラメトリック推定",
    "title": "survival",
    "section": "パラメトリック推定",
    "text": "パラメトリック推定\nsurvreg() 関数を用いることで、生存関数 S(x) の形状として指数分布、ワイブル分布、対数ロジスティック分布などを仮定したパラメトリック推定を行うことができます。\n\nstr(kidney)\n\n'data.frame':   76 obs. of  7 variables:\n $ id     : num  1 1 2 2 3 3 4 4 5 5 ...\n $ time   : num  8 16 23 13 22 28 447 318 30 12 ...\n $ status : num  1 1 1 0 1 1 1 1 1 1 ...\n $ age    : num  28 28 48 48 32 32 31 32 10 10 ...\n $ sex    : num  1 1 2 2 1 1 2 2 1 1 ...\n $ disease: Factor w/ 4 levels \"Other\",\"GN\",\"AN\",..: 1 1 2 2 1 1 1 1 1 1 ...\n $ frail  : num  2.3 2.3 1.9 1.9 1.2 1.2 0.5 0.5 1.5 1.5 ...\n\nsreg &lt;- survreg(Surv(time, status) ~ as.factor(sex),\n                data = kidney, dist = \"weibull\")\n\n# Kaplan-Meier 推定値\nplot(survfit(Surv(time, status) ~ as.factor(sex), data = kidney), lty = 1:2)\n\n# Weibull分布モデルのグラフ\nshape &lt;- 1 / sreg$scale # 形状パラメータ\nscaleM &lt;- exp(coef(sreg)[1]) # 尺度パラメータ\nscaleF &lt;- exp(coef(sreg)[1]  + coef(sreg)[2])\ncurve(1 - pweibull(x, shape, scaleM), add = TRUE, col = \"darkcyan\")\ncurve(1 - pweibull(x, shape, scaleF), add = TRUE, col = \"darkred\", lty = 2)\nlegend(200, 1, c(\"male\", \"female\"), lty = 1:2)\n\n\n\n\n\n\n\n# パラメータの確認\ncat(paste0(\"weibull params\\n  shape: \", round(shape, 4),\n           \"\\n  scale(male): \", round(scaleM, 4),\n           \"\\n  scale(female): \", round(scaleF, 4)))\n\nweibull params\n  shape: 0.9041\n  scale(male): 60.0208\n  scale(female): 160.9822",
    "crumbs": [
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>survival</span>"
    ]
  },
  {
    "objectID": "articles/survival.html#セミパラメトリック推定",
    "href": "articles/survival.html#セミパラメトリック推定",
    "title": "survival",
    "section": "セミパラメトリック推定",
    "text": "セミパラメトリック推定\ncoxph() 関数を用いることで、Cox比例ハザード回帰モデル（Cox proportional hazards regression model）を構築することができます。\nKaplan-Meier推定では集団ごとにデータを分けることで生存関数を推定しましたが、Cox比例ハザードモデルでは、個人ごとのハザード関数を説明変数（共変量）Z に基づく線形予測子を用いて以下の式のようにモデル化し、回帰係数を一種の最尤法で推定します。\n\\lambda_i(t; Z)=\\lambda_0(t)\\ exp({\\beta}^{T}Z)\n比例ハザードモデルは、ベースラインハザード関数（潜在基礎ハザード関数） \\lambda_0(t) の部分にノンパラメトリックな仮定を残しつつ、相対ハザード exp(\\beta^TZ) の部分をパラメトリックにモデル化し、推定することから、セミパラメトリックモデルに分類されます。\n\ngehan データセットでの実行例\n\n\nsummary(gehan.cox &lt;- coxph(Surv(time, cens) ~ treat, gehan))\n\nCall:\ncoxph(formula = Surv(time, cens) ~ treat, data = gehan)\n\n  n= 42, number of events= 30 \n\n               coef exp(coef) se(coef)     z Pr(&gt;|z|)    \ntreatcontrol 1.5721    4.8169   0.4124 3.812 0.000138 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n             exp(coef) exp(-coef) lower .95 upper .95\ntreatcontrol     4.817     0.2076     2.147     10.81\n\nConcordance= 0.69  (se = 0.041 )\nLikelihood ratio test= 16.35  on 1 df,   p=5e-05\nWald test            = 14.53  on 1 df,   p=1e-04\nScore (logrank) test = 17.25  on 1 df,   p=3e-05\n\n# 対照群\"control\"のハザード関数は、処置群\"6MP\"の4.82倍\n\nZ &lt;- data.frame(treat = levels(gehan$treat))\nplot(survfit(gehan.cox, Z), lty = 1:2, xlab = \"weeks\", ylab = \"survival\")\nlines(survfit(Surv(time, cens) ~ treat, gehan), lty = 1:2, col = \"gray\")\nlegend(21, 1, c(\"Cox\", \"Kaplan-Meier\"), lty = 1, col = c(\"black\", \"gray\"))\n\n\n\n\n\n\n\n\n\nkidney データセットでの実行例\n\n\nkidney.cox &lt;- coxph(Surv(time, status) ~ age + as.factor(sex) + disease,\n                    data = kidney)\nsummary(kidney.cox)\n\nCall:\ncoxph(formula = Surv(time, status) ~ age + as.factor(sex) + disease, \n    data = kidney)\n\n  n= 76, number of events= 58 \n\n                     coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nage              0.003181  1.003186  0.011146  0.285   0.7754    \nas.factor(sex)2 -1.483137  0.226925  0.358230 -4.140 3.47e-05 ***\ndiseaseGN        0.087957  1.091941  0.406369  0.216   0.8286    \ndiseaseAN        0.350794  1.420195  0.399717  0.878   0.3802    \ndiseasePKD      -1.431108  0.239044  0.631109 -2.268   0.0234 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                exp(coef) exp(-coef) lower .95 upper .95\nage                1.0032     0.9968   0.98151    1.0253\nas.factor(sex)2    0.2269     4.4067   0.11245    0.4579\ndiseaseGN          1.0919     0.9158   0.49238    2.4216\ndiseaseAN          1.4202     0.7041   0.64880    3.1088\ndiseasePKD         0.2390     4.1833   0.06939    0.8235\n\nConcordance= 0.697  (se = 0.041 )\nLikelihood ratio test= 17.65  on 5 df,   p=0.003\nWald test            = 19.9  on 5 df,   p=0.001\nScore (logrank) test = 20.13  on 5 df,   p=0.001\n\nZ &lt;- data.frame(age = mean(kidney$age), sex = 1,\n                disease = levels(kidney$disease))\n# 連続変数は平均値がベースラインになるように変換されている。\npredict(kidney.cox, Z)\n\n          1           2           3           4 \n 0.00000000  0.08795655  0.35079420 -1.43110776 \n\nplot(survfit(kidney.cox, Z), lty = 1:4, xlab = \"age = 43.7, sex = 1\")\nlegend(260, 1, levels(kidney$disease), lty = 1:4)",
    "crumbs": [
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>survival</span>"
    ]
  },
  {
    "objectID": "articles/tibble.html",
    "href": "articles/tibble.html",
    "title": "tibble",
    "section": "",
    "text": "パッケージの概要\ntibbleは、従来のdata.frameを洗練させたデータ構造tibbleを作成することのできるパッケージです。また、このパッケージは、Rでモダンな分析環境を構築するためのパッケージ群である「tidyberse」に含まれています。\n\nsuppressMessages(require(tidyverse))\nrequire(tibble)\nrequire(microbenchmark)\n\nLoading required package: microbenchmark\n\n\nWarning: package 'microbenchmark' was built under R version 4.5.1\n\n\n\n\ntibbleの作成方法\nas_tibble関数を利用することで、listや、data.frameをtibbleに変換することができます。\n\nclass(iris)\n\n[1] \"data.frame\"\n\nas_tibble(iris)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n\nまた、tibble関数を用いると、個々のベクトルをまとめて、tibbleを作成することができます。\n\ntibble(x = 1:3, y = 1)\n\n# A tibble: 3 × 2\n      x     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1     1\n2     2     1\n3     3     1\n\n\n\n\ndata.frameとの違い①：表示\nあるデータをConsoleに表示する場合、tibbleはデフォルトでデータの表示数を制限し、Consoleがデータで埋め尽くされるのを防いでくれます。また、データの型がカラム名の下に表示されます。\n例えば、irisには150個のデータが含まれており、data.frameのまま表示すると、150個のデータが表示されます。一方、tibbleとして表示すると、Consoleに表示されるデータの数は10個だけです。 （表示される数は、オプションで変更することが可能です）\n\nas_tibble(iris)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n#head関数を利用すれば、data.frameでも表示数をコントロール可能\nhead(iris, n=10)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n\n\n\n\ndata.frameとの違い②：列名の変換有無\nベクトルからdata.frameを作成する場合、自動で列名が変換されてしまうケースがあるが、tibbleを作成する場合、デフォルトの設定では列名が自動変換されません。 また、name_repairsをuniversalと指定すると、列名が変換されるようになります。\n\ntibble(`a + b` = 1:5)\n\n# A tibble: 5 × 1\n  `a + b`\n    &lt;int&gt;\n1       1\n2       2\n3       3\n4       4\n5       5\n\ndata.frame(`a + b` = 1:5)\n\n  a...b\n1     1\n2     2\n3     3\n4     4\n5     5\n\ntibble(`a 1` = 1, `a 2` = 2, .name_repair = \"universal\")\n\nNew names:\n• `a 1` -&gt; `a.1`\n• `a 2` -&gt; `a.2`\n\n\n# A tibble: 1 × 2\n    a.1   a.2\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     2\n\n\n\n\ndata.frameとの違い③：サブセット操作の厳密さ\n特定の列を指定して、データを表示する場合、tibbleでは厳密に列名を指定する必要があります。 また、data.frameはサブセットを指定する条件次第で、データフレームではなく、ベクトルを返すこともありますが、tibbleは常にtibbleを返します。\n\ndf &lt;- data.frame(one = 1, two = \"a\", three = \"v\")\ndf2 &lt;- tibble(df)\n\ndf$o\n\n[1] 1\n\ndf2$o\n\nWarning: Unknown or uninitialised column: `o`.\n\n\nNULL\n\ndf[, c(\"one\", \"two\")]\n\n  one two\n1   1   a\n\ndf2[, c(\"one\", \"two\")]\n\n# A tibble: 1 × 2\n    one two  \n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n\ndf[, \"one\"]\n\n[1] 1\n\ndf2[, \"one\"]\n\n# A tibble: 1 × 1\n    one\n  &lt;dbl&gt;\n1     1\n\n\n\n\ndata.frameとの違い④：パフォーマンス\nas.tibble()は、as.data.frame()よりも高速です\n\nX &lt;- list(1:10000, 1:10000, 1:10000)\nnames(X) &lt;- c(\"test1\", \"test2\", \"test3\")\nmicrobenchmark(\n  as_tibble(X),\n  as.data.frame(X)\n)\n\nUnit: microseconds\n             expr   min    lq    mean median    uq   max neval\n     as_tibble(X)  78.9  85.8 100.425  90.85  94.5 368.8   100\n as.data.frame(X) 150.5 158.9 172.055 164.75 170.6 307.0   100",
    "crumbs": [
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>tibble</span>"
    ]
  },
  {
    "objectID": "articles/tuneRanger.html",
    "href": "articles/tuneRanger.html",
    "title": "tuneRanger",
    "section": "",
    "text": "パッケージの概要\n機械学習におけるRandomForestモデルの構築を行えます。また、tuneRangerではハイパーパラメータのチューニングを行う機能も提供されます。",
    "crumbs": [
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>tuneRanger</span>"
    ]
  },
  {
    "objectID": "articles/tuneRanger.html#参考url",
    "href": "articles/tuneRanger.html#参考url",
    "title": "tuneRanger",
    "section": "参考URL",
    "text": "参考URL\n公式ドキュメント https://cran.r-project.org/web/packages/tuneRanger/tuneRanger.pdf\nRの機械学習パッケージmlrのチュートリアル（タスクの作成から予測まで） https://qiita.com/nozma/items/bedcb35cba925764247a",
    "crumbs": [
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>tuneRanger</span>"
    ]
  },
  {
    "objectID": "articles/tuneRanger.html#使用例irisデータの分類",
    "href": "articles/tuneRanger.html#使用例irisデータの分類",
    "title": "tuneRanger",
    "section": "使用例：irisデータの分類",
    "text": "使用例：irisデータの分類\nirisデータを用いて、がく弁・花弁の長さ・幅の情報からアヤメの種類を特定するRandomForestモデルをtuneRangerパッケージを用いて構築します。\n\nirisデータセットを読み込む\nirisデータを読み込み、データの先頭を表示します。\n\nSepal.Length：がく弁の長さ\nSepal.Width：がく弁の幅\nPetal.Length：花弁の長さ\nPetal.Width：花弁の幅\n\nアヤメの種類はsetosa(1)、versicolor(2)、virginica(3)の3種類です\n\ndata(iris)\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\n\nirisデータの構造\nirisデータの各種構造を確認します。\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nまた、データを散布図にプロットして確認します。\n\nplot(iris, col=c(2, 3, 4)[iris$Species])\n\n\n\n\n\n\n\n\n\n\nデータセットの準備\nirisデータをモデル生成のための訓練データと、モデル評価のためのテストデータに分割します。データ割合は訓練データを7割、テストデータを3割とします\n\n# 再現性のためにシードを設定\nset.seed(100)\n\n# データの分割\nn &lt;- nrow(iris)\ntrain.rate &lt;- 0.7\n\n# データそのものではなく、データ番号を吐き出している\n(train.set &lt;- sample(n, n * train.rate))\n\n  [1] 102 112   4  55  70  98 135   7  43 140  51  25   2  68 137  48  32  85\n [19]  91 121  16 116  66 146  93  45  30 124 126  87  95  97 120  29  92  31\n [37]  54  41 105 113  24 142 143  63  65   9 150  20  14  78  88   3  36  27\n [55]  46  59  96  69  47 147 129 136  12 141 130  56  22  82  53  99   5  44\n [73]  28  52 139  42  15  57  75  37  26 110 100 149 132 107  35  58 127 111\n [91] 144  86 114  71 123 119  18   8 128  83 138  19 115  23  89\n\n(test.set &lt;- setdiff(1:n, train.set))\n\n [1]   1   6  10  11  13  17  21  33  34  38  39  40  49  50  60  61  62  64  67\n[20]  72  73  74  76  77  79  80  81  84  90  94 101 103 104 106 108 109 117 118\n[39] 122 125 131 133 134 145 148\n\n\n\n\nモデル生成\nmakeClassifTaskにてタスクの定義を行い、makeLeanerにて適用するアルゴリズムの選択を行います。\n\n(task &lt;- makeClassifTask(data = iris, target = \"Species\"))\n\nSupervised task: iris\nType: classif\nTarget: Species\nObservations: 150\nFeatures:\n   numerics     factors     ordered functionals \n          4           0           0           0 \nMissings: FALSE\nHas weights: FALSE\nHas blocking: FALSE\nHas coordinates: FALSE\nClasses: 3\n    setosa versicolor  virginica \n        50         50         50 \nPositive class: NA\n\nlrn &lt;- makeLearner(\"classif.lda\")\n\n分割したデータセットを基にモデルの訓練を行います。また、訓練時の誤分類率(mmce)や精度(ace)を把握することが出来ます。\n\n(model &lt;- train(lrn, task, subset = train.set))\n\nModel for learner.id=classif.lda; learner.class=classif.lda\nTrained on: task.id = iris; obs = 105; features = 4\nHyperparameters: \n\n\n\n\nパラメータチューニング\nパラメータチューニングのための実行時間を事前に知ることが出来ます。\n\nestimateTimeTuneRanger(task, num.trees = 500, num.threads = 3, iters = 30)\n\nApproximated time for tuning: 50S\n\n\ntuneRangerを使用して、実際にパラメータチューニングをやってみます。\nチューニング後のモデルの精度を確認してみます。精度が向上していることが分かります。\n\npred.res &lt;- predict(res$model, task = task, subset = test.set)\nperformance(pred.res, measures = list(mmce, acc))\n\n      mmce        acc \n0.02222222 0.97777778",
    "crumbs": [
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>tuneRanger</span>"
    ]
  },
  {
    "objectID": "articles/yardstick.html",
    "href": "articles/yardstick.html",
    "title": "yardstick",
    "section": "",
    "text": "パッケージの概要\nyardstickは機械学習や予測モデリングを行うためのパッケージ群tidymodelsに含まれており、構築したモデルの性能を評価するための指標(評価指標)を計算する機能を提供するパッケージです。 様々な種類の評価指標に対応する関数が用意されていますので、構築したモデルやデータの性質、ビジネス上の目的を踏まえて、適切な指標を選択し使用していく必要があります。",
    "crumbs": [
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>yardstick</span>"
    ]
  },
  {
    "objectID": "articles/yardstick.html#基本的な使用方法",
    "href": "articles/yardstick.html#基本的な使用方法",
    "title": "yardstick",
    "section": "基本的な使用方法",
    "text": "基本的な使用方法\nyardstickの基本的な使用方法を紹介します。 テストデータとして、yardstickに含まれている二値分類用のテストデータtwo_class_exampleを使用します。 two_class_exampleは真のクラスtruthと予測クラスpredicted、及び両クラスに対するクラス確率の予測値からなるデータセットです。 クラス確率の閾値が0.5に設定されており、クラス1のクラス確率を示すカラムClass1が0.5を超えているレコードに対しては予測クラスを示すpredictedカラムが”Class1”、Class2クラスが0.5を超えているレコードではpredictedが”Class2”となっています。\n\nlibrary(yardstick)\nlibrary(dplyr)  \n\ndata(two_class_example, package = \"yardstick\")\nstr(two_class_example)\n\n'data.frame':   500 obs. of  4 variables:\n $ truth    : Factor w/ 2 levels \"Class1\",\"Class2\": 2 1 2 1 2 1 1 1 2 2 ...\n $ Class1   : num  0.00359 0.67862 0.11089 0.73516 0.01624 ...\n $ Class2   : num  0.996 0.321 0.889 0.265 0.984 ...\n $ predicted: Factor w/ 2 levels \"Class1\",\"Class2\": 2 1 2 1 2 1 1 1 2 2 ...\n\n\nyardstickでは、評価指標ごとに対応する関数が用意されています。 算出したい評価指標に対応する関数にデータセットを渡し、真値および予測値を示すカラムを指定するのが基本的な使用方法です。\nここでは例として、正しいクラスに分類できた件数の割合を表す指標Accuracyを計算します。 評価指標Accuracyに対してはaccuracy関数が用意されており、次のとおり使用します。\n\ntwo_class_example %&gt;%\n  accuracy(truth = truth, estimate = predicted)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.838\n\n\n結果はtibble形式で、.metricは評価指標名、.estimatorは評価指標の計算方法、算出された評価指標の数値は.estimateというカラムで返されます。 Accuracyの値を見ると、two_class_exampleの予測クラスは約84%が真のクラスと一致していることがわかります。\nなお各評価指標の計算関数は、上の例のように、計算したい予測値と真値をデータフレームで渡す関数のほかに、ベクトルでデータを渡すバージョンも用意されています。\n\naccuracy_vec(truth = two_class_example$truth, estimate = two_class_example$predicted)\n\n[1] 0.838\n\n\n評価指標Accuracyの例を示しましたが、基本的な使用方法は他の評価指標の関数も同様です。 以降では、回帰問題および分類問題にわけて、代表的な評価指標と対応するyardstickの関数の使用例を紹介していきます。",
    "crumbs": [
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>yardstick</span>"
    ]
  },
  {
    "objectID": "articles/yardstick.html#回帰問題の評価指標",
    "href": "articles/yardstick.html#回帰問題の評価指標",
    "title": "yardstick",
    "section": "回帰問題の評価指標",
    "text": "回帰問題の評価指標\n数値データを予測する回帰問題における、代表的な評価指標とyardstickによる使用例を紹介します。 サンプルデータとして、yardstickで用意されているsolubility_testデータを用います。 solubility_testデータは、数値型の真値solubilityと予測値predictionの二つのカラムを持つデータセットです。\n\ndata(\"solubility_test\", package = \"yardstick\")\nstr(solubility_test)\n\n'data.frame':   316 obs. of  2 variables:\n $ solubility: num  0.93 0.85 0.81 0.74 0.61 0.58 0.57 0.56 0.52 0.45 ...\n $ prediction: num  0.368 -0.15 -0.505 0.54 -0.479 ...\n\n\n\nplot(x=solubility_test$prediction, y=solubility_test$solubility)\n\n\n\n\n\n\n\n\nMAE\nMAE(Mean Absolute Error)は次式のとおり、真の値と予測値との差の絶対値について平均を取った指標です。 すなわち、真の値に対して予測値が平均的にどの程度ずれているのかを評価する指標となっています。 ここで、Nはデータの件数、y_iと\\hat{y_i}はそれぞれデータiに対する真の値および予測値を表します。\n\n\\frac{1}{N}\\sum_{i=1}^{N}|y_i-\\hat{y_i}|\n\n\nsolubility_test %&gt;%\n  mae(truth = solubility, estimate = prediction)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard       0.545\n\n\nRMSE\nRMSE(Root Mean Squared Error)は真値と予測値との差の二乗の平均を取った指標(MSE)の平方根です。 誤差の二乗をもとに計算した指標なので、予測値のずれが大きなデータに対してはMAEよりも大きな値が加算されることになります。 したがって、外れ値や誤差が大きくなることに対して、より大きなペナルティを課したい場合に適切な指標指標と考えられます。\n\n\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\hat{y_i})^2}\n\n\nsolubility_test %&gt;%\n  rmse(truth = solubility, estimate = prediction)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.722\n\n\n決定係数\n決定係数は、目的変数のばらつき(総変動)のうち説明変数により説明される割合で、モデルの当てはまりの良さを示す指標として使用されます。\n\nsolubility_test %&gt;%\n  rsq(truth = solubility, estimate = prediction)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.879\n\n\nyardstickのrsq関数は相関係数の二乗として実装されており、(0, 1)の範囲を取ります。 一方、rsq_tradは1-残差変動/総変動として計算されているバージョンです。 両者は線形回帰モデルでない場合一致せず、また線形回帰モデル以外にrsq_tradを適用すると値が負になる可能性もある等、特に線形モデル以外への決定係数の適用とその解釈には注意が必要です。\n\nsolubility_test %&gt;%\n  rsq_trad(truth = solubility, estimate = prediction)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 rsq_trad standard       0.879",
    "crumbs": [
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>yardstick</span>"
    ]
  },
  {
    "objectID": "articles/yardstick.html#分類問題の評価指標",
    "href": "articles/yardstick.html#分類問題の評価指標",
    "title": "yardstick",
    "section": "分類問題の評価指標",
    "text": "分類問題の評価指標\n続いて、分類問題に関する評価指標とyardstickによる使用例を紹介します。 ここでもサンプルデータとして、冒頭で紹介したtwo_class_exampleを使用します。 なおtwo_class_exampleデータがそうなっているように、分類問題の評価指標の関数に渡す真値および予測値はfactor型でなければなりません。\n混同行列\n最初に、分類問題における代表的な評価指標の計算に使用される、混同行列(Confusion Matrix)を紹介します。 混同行列は、真のクラスと予測クラスの組み合わせでデータ件数を分割表にまとめたもので、yardstickではconf_mat関数で出力することができます。\n\ntwo_class_example %&gt;%\n  conf_mat(truth = truth, estimate = predicted)\n\n          Truth\nPrediction Class1 Class2\n    Class1    227     50\n    Class2     31    192\n\n\n陽性(positive)と陰性(negative)とで判別する二値分類の混同行列は２×２の分割表となりますが、一般的に以下の名称でまとめられます。\n\n2 \\times 2 confusion matrix.\n\n\n\n\n\n\n\n\n\nTruth\n\n\nPositive\nNegative\n\n\n\n\nPrediction\nPositive\nTP(true positive)\nFP(false positive)\n\n\nNegative\nFN(false negative)\nTN(true negative)\n\n\n\n以降では、2×2の混同行列を前提として、上表の表記で評価指標を紹介していきます。\nAccuracy(正解率)\n冒頭でも紹介した評価指標Accuracyは、真のクラスがpositiveのデータを正しくpositiveと予測できた、または真のクラスがnegativeのデータを正しくnegativeと予測できた件数の割合ですので、混同行列からは以下の通り計算されます。\n\\frac{TP+TN}{TP+TN+FN+FP} yardstickでの実行例は次のとおりです。(冒頭で紹介したものと同じです。)\n\ntwo_class_example %&gt;%\n  accuracy(truth = truth, estimate = predicted)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.838\n\n\nAccuracyはわかりやすい指標ですが、クラスごとの件数に偏りがある不均衡データの場合には注意が必要です。 例えば、Class1が全体の95%を占めるようなデータの場合、「全てのデータを一律Class1と予測する」モデルのAccuracyは95%という一見高い数値となります。 これがもし、少数のクラス(Class2)が重要であり、Class2を正しく識別したいような問題の場合、Accuracyは適切な評価指標とは言えないでしょう。\nPrecision(適合率)\nPrecisionは、positiveと予測したデータのうち真のクラスがpositiveであった割合を示す指標で、具体的には以下の通りです。\n\\frac{TP}{TP+FP} yardstickではprecision関数で計算します。 真値及び予測値のfactor型変数において、2クラスのうちどちらをpositiveとして計算するかは、event引数に”first”または”second”を渡して指定することができ、省略した場合のデフォルトは”first”です。このevent引数によるpositiveクラスの設定方法に関しては、以降で紹介する分類用の評価指標関数において同様です。\n\ntwo_class_example %&gt;%\n  precision(truth = truth, estimate = predicted)\n\n# A tibble: 1 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 precision binary         0.819\n\n\nPrecisionは予測値がpositiveであるデータに着目した評価指標なので、偽陽性(FP)を減らしたい場合に有効です。 スパムメールの分類においてスパムをpositiveとして判別したい場合や、あるサービスについて解約が見込まれる顧客をpositiveと予測し何らかのコストのかかる施策を打って対応する場合等、positiveと予測するからには高い的中率でありたい状況での適用が考えられます。\nRecall(再現率)\nRecallは真の値がpositiveのデータのうち、正しくpositiveと予測したデータの割合です。 算式とyardstickによる実行例は以下の通りです。\n\\frac{TP}{TP+FN}\n\ntwo_class_example %&gt;%\n  recall(truth = truth, estimate = predicted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 recall  binary         0.880\n\n\nRecallは真の値がpositiveのデータに着目した評価指標なので、例えばpositiveデータの割合が小さい不均衡データにおいて、真の値がpositiveであるデータを見逃さないようにしたい、といった状況で有効です。\nF-β score\nPrecisionとRecallは着目する観点が異なっていましたが、どちらも考慮した評価指標としてF-β scoreがあります。 算式は次の通りで、\\betaはRecallとPrecisionのどちらを重視するかを調整するパラメータです。 両者に差をつけない\\beta=1のとき、F-1 score呼ばれます。\n\\frac{(1+\\beta^2)\\times Recall \\times Precision}{\\beta^2 \\times Precision + Recall} yardstickではf_meas関数で算出しますが、パラメータbetaを引数として渡すことができます。 以下はF-1 scoreの算出例です。(betaのデフォルトは1なので、以下の場合省略も可能です。)\n\ntwo_class_example %&gt;%\n  f_meas(truth = truth, estimate = predicted, beta = 1)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 f_meas  binary         0.849\n\n\nSpecificity\nRecallは真値がpositiveのデータに着目して、正しくpositiveと予測できた割合を示す指標でしたが、同様の考え方で真値がnegativeのデータに着目した指標がSpecificityです。\n\\frac{TN}{TN+FP}\n\ntwo_class_example %&gt;%\n  spec(truth = truth, estimate = predicted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 spec    binary         0.793\n\n\nROC AUC\nここまでの指標は、予測クラスをpositiveクラスとnegativeクラスのどちらかに分けた状態を前提としていました。 実際には、多くのモデルは両クラスに属する確率(クラス確率)を算出し、例えばpositiveクラスのクラス確率が一定の閾値を上回ったデータについてpositiveと判定する、といった処理が行われることも多いでしょう。 クラス確率に対する閾値を変えると、予測クラスが変わり混同行列も変化しますので、モデルの全体としての適切性にも影響を与えることになるでしょう。\nこのような、閾値の変化に対するモデルの予測結果の変化を可視化する方法として、ROC曲線(Receiver Operatorating Characteristic curve)という方法があります。 ROC曲線は、positiveクラスのクラス確率に対する閾値を1(全てのデータをnegativeと予測)から0(全てのデータをpositiveと予測)へと徐々に変化させたときの、各閾値における真陽性率(=Recall=Sensitivity)を縦軸、偽陽性率(=1-Specificity)を横軸としてプロットしたものです。 yardstickでは、ggplot2パッケージのautoplot関数を利用してROC曲線を描画するための、roc_curve関数が用意されています。\n\nlibrary(ggplot2)\n\ntwo_class_example %&gt;%\n  roc_curve(truth, Class1) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\n分類の精度が良いモデルでは、多くの閾値において真陽性率(真のクラスがpositiveのときに正しくpositiveと分類できる割合)が高く、偽陽性率(真のクラスがnegativeのときに誤ってpositiveと分類する割合)が低いこと、すなわちROC曲線が左上に広がっている状態が期待されるでしょう。 逆に完全にランダムな分類器では、真のクラスがpositiveであろうとnegativeであろうと、等確率でpositiveに分類することになりますので、ROC曲線は傾き1の直線(上図の点線)となります。\nROC曲線の下部の面積(AUC; Area Under Curve)を測ることで、上の議論を定量的に評価することができます。 AUC=0.5はランダムな分類器に対応し、AUCが1に近づくほど、多くの閾値でよい分類性能を示していることになります。\n\ntwo_class_example %&gt;%\n    roc_auc(truth, Class1)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.939\n\n\nLog loss\nクラス確率を使用して評価する他の指標として、Log lossを紹介します。 Log lossは以下の式で計算され、各データにおいて正解クラスに対して高いクラス確率を予測できていると小さい値が、誤ったクラスに対して高いクラス確率を予測していると大きな値が加算されます。 そのため、数値が小さいほど精度の高いモデルということになります。\n\n-\\frac{1}{N}\\sum^{N}_{i=1}[y_i\\log(p_i)+(1-y_i)\\log(1-p_i)]\n ここでNはデータ数、y_iはデータiに対する正解ラベル(1 or 0)、p_iはクラスy=1に対するクラス確率です。\nyardstickでは以下のとおり計算できます。\n\ntwo_class_example %&gt;%\n  mn_log_loss(truth = truth, Class1)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 mn_log_loss binary         0.328\n\n\nLog lossは予測したクラス確率を直接評価する指標なので、モデルで予測した確率を用いて期待値を計算するような場合等で有効な指標と考えられます。 ただし、確率に着目しているが故に、閾値を設けて分類まで行うような問題では、以下のようなケースが発生し得ることに注意が必要です。\n\n# 2データの2クラス分類問題\ny &lt;- factor(c(1, 1), levels = c(1, 2)) # 正解クラスは両データとも1\np1 &lt;- c(0.99, 0.49)  # モデル1のクラス1確率の予測値\np2 &lt;- c(0.51, 0.51)  # モデル2のクラス1確率の予測値\n\nlogloss_1 &lt;- mn_log_loss_vec(y, p1)\nlogloss_2 &lt;- mn_log_loss_vec(y, p2)\ncat(\"model 1: \", logloss_1, \"\\nmodel 2: \", logloss_2)\n\nmodel 1:  0.3617001 \nmodel 2:  0.6733446\n\n\nこの例では、高い確信度をもって1データ目をクラス1と予測している、モデル1のLog lossの数値が小さくなっています。 一方で、仮に閾値を0.5として分類まで行うような場合では、モデル2は両データとも正解ですが、モデル1ではデータ2は不正解となり、Log lossによる判断とは逆の結果となります。 このように、閾値を設けて分類まで行うような状況では、クラス確率の予測値そのものに着目するLog lossよりも、クラス確率の順序関係を重視しているROC AUCを使用することが考えられます。\n多クラス分類\nここまでは二値分類を前提として評価指標を紹介してきましたが、多クラス分類についても簡単に紹介します。 次の例ではyardstickの多クラス分類用のテストデータhpc_cvを用います。 このデータセットは4クラスの分類を行うためのもので、真のクラスであるobs、予測クラスpred、各クラスのクラス確率の予測値のカラムを持ちます。 更に、データを分割しての検証(クロスバリデーション)を意図した、グループを示すカラムResampleが用意されています。\n\ndata(hpc_cv, package = \"yardstick\")\nstr(hpc_cv)\n\n'data.frame':   3467 obs. of  7 variables:\n $ obs     : Factor w/ 4 levels \"VF\",\"F\",\"M\",\"L\": 1 1 1 1 1 1 1 1 1 1 ...\n $ pred    : Factor w/ 4 levels \"VF\",\"F\",\"M\",\"L\": 1 1 1 1 1 1 1 1 1 1 ...\n $ VF      : num  0.914 0.938 0.947 0.929 0.942 ...\n $ F       : num  0.0779 0.0571 0.0495 0.0653 0.0543 ...\n $ M       : num  0.00848 0.00482 0.00316 0.00579 0.00381 ...\n $ L       : num  1.99e-05 1.01e-05 5.00e-06 1.56e-05 7.29e-06 ...\n $ Resample: chr  \"Fold01\" \"Fold01\" \"Fold01\" \"Fold01\" ...\n\n\n多クラス分類のデータに対しても、一部の関数は二値分類と同じように適用することができます。 以下では、hpc_cvデータのFold01に対して、正しく真のクラスを予測できている割合Accuracyを計算しています。\n\nhpc_cv %&gt;%\n  filter(Resample == \"Fold01\") %&gt;%\n  accuracy(obs, pred)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.726\n\n\nPrecisionやRecall等の多クラス分類への拡張方法はいくつかありますが、ここでは、これら関数のデフォルトに設定されているMacro averagingをいう方法を紹介します。 Macro averagingでは、複数あるクラスごとに「当該クラス」対「それ以外のクラス」の二値分類として評価指標を計算し、計算した指標について全クラスの平均を取る方法です。 すなわち、kクラスの分類では、Pr_{1}を「クラス1」と「それ以外のクラス」の二値分類に関して計算したPrecision、Pr_{2}を「クラス2」と「それ以外のクラス」に対するPrecision、というようにkクラスの計算を行った上で以下のとおり平均を取った指標です。 \n\\frac{Pr_1+Pr_2+\\dots+Pr_k}{k}\n yardstickのprecision関数は、多クラス分類用のデータを渡すと自動的に多クラス分類用の計算を行ってくれます。 引数estimatorに”macro”を設定することでMacro averagingを明示的に指定することができますが、多クラス分類用のデータを与えるとデフォルトでMacro averagingが計算されるため省略可能です。\n\nhpc_cv %&gt;%\n  filter(Resample == \"Fold01\") %&gt;%\n  precision(obs, pred)\n\n# A tibble: 1 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 precision macro          0.637\n\n\nここまでいくつかの評価指標を紹介してきましたが、yardstickには他にも様々な評価指標が用意されていますので、必要に応じてyardstickの公式サイトをご参照ください。",
    "crumbs": [
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>yardstick</span>"
    ]
  },
  {
    "objectID": "articles/yardstick.html#その他便利な機能",
    "href": "articles/yardstick.html#その他便利な機能",
    "title": "yardstick",
    "section": "その他便利な機能",
    "text": "その他便利な機能\n\nグループ別の評価指標の計算\nhpc_cvデータはクロスバリデーション用のグループを示すResampleカラムが用意されていました。 yardstickの各評価指標の関数は、dplyerパッケージのgroup_by関数によりグループ化することで、グループごとの評価指標を計算することができます。 以下ではResampleカラムでグループ化し、グループごとのAccuracyを計算しています。\n\nhpc_cv %&gt;%\n  group_by(Resample) %&gt;%\n  accuracy(obs, pred)\n\n# A tibble: 10 × 4\n   Resample .metric  .estimator .estimate\n   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n 1 Fold01   accuracy multiclass     0.726\n 2 Fold02   accuracy multiclass     0.712\n 3 Fold03   accuracy multiclass     0.758\n 4 Fold04   accuracy multiclass     0.712\n 5 Fold05   accuracy multiclass     0.712\n 6 Fold06   accuracy multiclass     0.697\n 7 Fold07   accuracy multiclass     0.675\n 8 Fold08   accuracy multiclass     0.721\n 9 Fold09   accuracy multiclass     0.673\n10 Fold10   accuracy multiclass     0.699\n\n\n\n\n複数評価指標のセット化\nyardstickのmetric_set関数を使用すると、複数の評価指標をまとめて1セットとした評価指標の関数を作成することができます。 以下では、二値分類問題に対する三つの評価指標をまとめた新たな評価指標関数を作成し、two_class_exampleデータに適用しています。\n\nclass_metrics &lt;- metric_set(accuracy, precision, recall)\ntwo_class_example %&gt;%\n  class_metrics(truth = truth, estimate = predicted)\n\n# A tibble: 3 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.838\n2 precision binary         0.819\n3 recall    binary         0.880\n\n\nmetric_setは、データに対して複数の評価指標を一度に適用したいときに便利ですが、セットとしてまとめる評価指標は同じ問題に適用できるものでなければなりません。 例えば回帰問題用のデータに使用するMAEの関数と、分類問題用のデータに使用するLog lossの関数とをmetric_setでまとめようとすると、エラーになるためご注意ください。\n\n# 以下は実行時エラー\n# error_metric_set &lt;- metric_set(mn_log_loss, mae)\n\nなお、yardstickには基本的な評価指標をいくつかまとめたセットがmetrics関数として既に用意されています。 metrics関数は、データの種類ごとにあらかじめ設定されている指標を計算して返してくれます。 例えば、二値分類と回帰については、それぞれ以下に示す指標が返されます。\n\n# 二値分類\ntwo_class_example %&gt;%\n  metrics(truth = truth, estimate = predicted, Class1)\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.838\n2 kap         binary         0.675\n3 mn_log_loss binary         0.328\n4 roc_auc     binary         0.939\n\n\n\n# 回帰\nsolubility_test %&gt;%\n  metrics(truth = solubility, estimate = prediction)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.722\n2 rsq     standard       0.879\n3 mae     standard       0.545\n\n\n\n\nパラメータ調整済の評価指標関数の作成\n先に紹介したように、F-β scoreはパラメータ\\betaを調整することでRecallとPrecisionのどちらを重視するかをコントロールできる指標で、対応するf_meas関数には引数betaとして指定できるものでした。 f_measのように、実行時にパラメータを渡す評価指標は、metric_tweak関数を使用することで、あらかじめ所定のパラメータをセットした状態の関数を作ることができます。\n\n# beta = 0.5の関数F-β関数を作成\nf_05_score &lt;- metric_tweak(\"F-0.5 score\", f_meas, beta=0.5)\ntwo_class_example %&gt;%\n  f_05_score(truth = truth, estimate = predicted)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 F-0.5 score binary         0.831\n\n\nこれは複数のパラメータの組み合わせでmetric_setを作成したい場合等に便利です。\n\n# betaの値ごとのF-β scoreを計算\nf_1_score &lt;- metric_tweak(\"F-1 score\", f_meas, beta=1)\nf_2_score &lt;- metric_tweak(\"F-2 score\", f_meas, beta=2)\nf_beta_metric &lt;- metric_set(f_05_score, f_1_score, f_2_score)\ntwo_class_example %&gt;%\n  f_beta_metric(truth = truth, estimate = predicted)\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 F-0.5 score binary         0.831\n2 F-1 score   binary         0.849\n3 F-2 score   binary         0.867",
    "crumbs": [
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>yardstick</span>"
    ]
  },
  {
    "objectID": "articles/yardstick.html#参考資料",
    "href": "articles/yardstick.html#参考資料",
    "title": "yardstick",
    "section": "参考資料",
    "text": "参考資料\n[1] Kuhn et al., (2020). Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org.\n[2] Kuhn M, Vaughan D, Hvitfeldt E (2025). yardstick: Tidy Characterizations of Model Performance. R package version 1.3.2, https://yardstick.tidymodels.org, https://github.com/tidymodels/yardstick.",
    "crumbs": [
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>yardstick</span>"
    ]
  }
]